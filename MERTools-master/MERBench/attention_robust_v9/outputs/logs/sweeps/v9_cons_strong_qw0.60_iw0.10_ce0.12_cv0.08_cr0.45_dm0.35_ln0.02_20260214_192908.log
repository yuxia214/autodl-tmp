====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, consistency_emo_weight=0.12, consistency_val_weight=0.08, contrastive_temperature=0.07, contrastive_weight=0.1, corruption_max_rate=0.45, corruption_warmup_epochs=25, cross_kl_weight=0.01, dataset='MER2023', debug=False, double_mask_ratio=0.35, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, emo_loss_weight=1.0, epochs=100, feat_scale=1, feat_type='utt', feature_noise_prob=0.35, feature_noise_std=0.02, feature_noise_warmup=5, focal_gamma=2.0, fusion_residual_scale=0.4, fusion_temperature=1.0, gate_alpha=0.55, gpu=0, grad_clip=1.0, hidden_dim=128, huber_beta=0.8, hyper_path=None, impute_loss_weight=0.1, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, latent_noise_std=0.02, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_agreement_weight=0.008, modality_dropout=0.18, modality_dropout_warmup=15, model='attention_robust_v9', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, quality_weight=0.6, recon_weight=0.1, reg_loss_type='smoothl1', reliability_temperature=0.9, save_iters=100000000.0, save_root='/root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_cons_strong_qw0.60_iw0.10_ce0.12_cv0.08_cr0.45_dm0.35_ln0.02_20260214_192908-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_gated_uncertainty=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, use_valence_prior=True, val_loss_weight=1.4, valence_center_reg_weight=0.005, valence_consistency_weight=0.1, video_feature='clip-vit-large-patch14-UTT', weight_consistency_weight=0.02)
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 42%|████▏     | 1433/3373 [00:00<00:00, 14318.99it/s] 85%|████████▍ | 2865/3373 [00:00<00:00, 13075.03it/s]100%|██████████| 3373/3373 [00:00<00:00, 13804.03it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s] 27%|██▋       | 902/3373 [00:00<00:00, 9006.65it/s] 58%|█████▊    | 1962/3373 [00:00<00:00, 9942.05it/s] 91%|█████████ | 3077/3373 [00:00<00:00, 10479.78it/s]100%|██████████| 3373/3373 [00:00<00:00, 10345.03it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s] 43%|████▎     | 1459/3373 [00:00<00:00, 14457.31it/s] 87%|████████▋ | 2941/3373 [00:00<00:00, 14605.52it/s]100%|██████████| 3373/3373 [00:00<00:00, 14756.69it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 9482.69it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 12684.11it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 16665.30it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 14105.75it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 9680.27it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 19838.51it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 15963.28it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 11855.26it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 17130.00it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3622; eval:-0.0266; lr:0.000500
epoch:2; metric:emoval; train:0.0654; eval:0.3299; lr:0.000500
epoch:3; metric:emoval; train:0.2484; eval:0.1535; lr:0.000500
epoch:4; metric:emoval; train:0.2988; eval:0.3563; lr:0.000500
epoch:5; metric:emoval; train:0.3755; eval:0.2404; lr:0.000500
epoch:6; metric:emoval; train:0.4089; eval:0.4439; lr:0.000500
epoch:7; metric:emoval; train:0.4795; eval:0.4406; lr:0.000500
epoch:8; metric:emoval; train:0.5232; eval:0.5450; lr:0.000500
epoch:9; metric:emoval; train:0.5543; eval:0.4898; lr:0.000500
epoch:10; metric:emoval; train:0.5663; eval:0.5660; lr:0.000500
epoch:11; metric:emoval; train:0.5789; eval:0.5473; lr:0.000500
epoch:12; metric:emoval; train:0.5748; eval:0.5108; lr:0.000500
epoch:13; metric:emoval; train:0.5662; eval:0.5285; lr:0.000500
epoch:14; metric:emoval; train:0.5784; eval:0.5259; lr:0.000500
epoch:15; metric:emoval; train:0.6076; eval:0.5564; lr:0.000500
epoch:16; metric:emoval; train:0.5868; eval:0.5230; lr:0.000500
epoch:17; metric:emoval; train:0.5859; eval:0.5594; lr:0.000500
epoch:18; metric:emoval; train:0.5811; eval:0.5347; lr:0.000500
epoch:19; metric:emoval; train:0.5627; eval:0.5256; lr:0.000500
epoch:20; metric:emoval; train:0.5604; eval:0.5437; lr:0.000500
epoch:21; metric:emoval; train:0.5273; eval:0.5232; lr:0.000250
epoch:22; metric:emoval; train:0.5927; eval:0.5502; lr:0.000250
epoch:23; metric:emoval; train:0.6100; eval:0.5063; lr:0.000250
epoch:24; metric:emoval; train:0.6204; eval:0.5320; lr:0.000250
epoch:25; metric:emoval; train:0.5995; eval:0.4919; lr:0.000250
epoch:26; metric:emoval; train:0.5729; eval:0.5766; lr:0.000250
epoch:27; metric:emoval; train:0.5868; eval:0.5521; lr:0.000250
epoch:28; metric:emoval; train:0.5781; eval:0.5630; lr:0.000250
epoch:29; metric:emoval; train:0.5833; eval:0.5507; lr:0.000250
epoch:30; metric:emoval; train:0.5819; eval:0.5055; lr:0.000250
epoch:31; metric:emoval; train:0.5688; eval:0.5265; lr:0.000250
epoch:32; metric:emoval; train:0.5989; eval:0.5492; lr:0.000250
epoch:33; metric:emoval; train:0.5942; eval:0.5358; lr:0.000250
epoch:34; metric:emoval; train:0.5964; eval:0.5528; lr:0.000250
epoch:35; metric:emoval; train:0.5862; eval:0.5651; lr:0.000250
epoch:36; metric:emoval; train:0.6252; eval:0.5207; lr:0.000250
epoch:37; metric:emoval; train:0.5899; eval:0.4831; lr:0.000125
epoch:38; metric:emoval; train:0.6086; eval:0.5579; lr:0.000125
epoch:39; metric:emoval; train:0.6243; eval:0.5580; lr:0.000125
epoch:40; metric:emoval; train:0.6414; eval:0.5334; lr:0.000125
epoch:41; metric:emoval; train:0.6279; eval:0.5492; lr:0.000125
epoch:42; metric:emoval; train:0.6261; eval:0.5328; lr:0.000125
epoch:43; metric:emoval; train:0.6548; eval:0.5312; lr:0.000125
epoch:44; metric:emoval; train:0.6664; eval:0.5621; lr:0.000125
epoch:45; metric:emoval; train:0.6226; eval:0.5612; lr:0.000125
epoch:46; metric:emoval; train:0.6451; eval:0.5466; lr:0.000125
epoch:47; metric:emoval; train:0.6536; eval:0.5761; lr:0.000125
epoch:48; metric:emoval; train:0.6598; eval:0.5520; lr:0.000063
epoch:49; metric:emoval; train:0.6668; eval:0.5672; lr:0.000063
epoch:50; metric:emoval; train:0.6688; eval:0.5719; lr:0.000063
epoch:51; metric:emoval; train:0.6690; eval:0.5588; lr:0.000063
epoch:52; metric:emoval; train:0.6782; eval:0.5378; lr:0.000063
epoch:53; metric:emoval; train:0.6611; eval:0.5882; lr:0.000063
epoch:54; metric:emoval; train:0.6696; eval:0.5572; lr:0.000063
epoch:55; metric:emoval; train:0.6919; eval:0.5544; lr:0.000063
epoch:56; metric:emoval; train:0.6888; eval:0.5734; lr:0.000063
epoch:57; metric:emoval; train:0.6799; eval:0.5655; lr:0.000063
epoch:58; metric:emoval; train:0.6909; eval:0.5307; lr:0.000063
epoch:59; metric:emoval; train:0.6784; eval:0.5168; lr:0.000063
epoch:60; metric:emoval; train:0.6896; eval:0.5475; lr:0.000063
epoch:61; metric:emoval; train:0.6835; eval:0.5470; lr:0.000063
epoch:62; metric:emoval; train:0.6970; eval:0.5630; lr:0.000063
epoch:63; metric:emoval; train:0.6954; eval:0.5577; lr:0.000063
epoch:64; metric:emoval; train:0.6857; eval:0.5609; lr:0.000031
epoch:65; metric:emoval; train:0.6940; eval:0.5683; lr:0.000031
epoch:66; metric:emoval; train:0.6952; eval:0.5672; lr:0.000031
epoch:67; metric:emoval; train:0.6928; eval:0.5650; lr:0.000031
epoch:68; metric:emoval; train:0.6894; eval:0.5538; lr:0.000031
epoch:69; metric:emoval; train:0.7001; eval:0.5653; lr:0.000031
epoch:70; metric:emoval; train:0.6729; eval:0.5426; lr:0.000031
epoch:71; metric:emoval; train:0.7151; eval:0.5827; lr:0.000031
epoch:72; metric:emoval; train:0.7060; eval:0.5697; lr:0.000031
epoch:73; metric:emoval; train:0.7095; eval:0.5474; lr:0.000031
epoch:74; metric:emoval; train:0.6985; eval:0.5525; lr:0.000031
epoch:75; metric:emoval; train:0.7324; eval:0.5520; lr:0.000016
epoch:76; metric:emoval; train:0.7110; eval:0.5348; lr:0.000016
epoch:77; metric:emoval; train:0.6926; eval:0.5477; lr:0.000016
epoch:78; metric:emoval; train:0.7123; eval:0.5462; lr:0.000016
epoch:79; metric:emoval; train:0.7069; eval:0.5615; lr:0.000016
epoch:80; metric:emoval; train:0.7261; eval:0.5589; lr:0.000016
epoch:81; metric:emoval; train:0.7117; eval:0.5316; lr:0.000016
epoch:82; metric:emoval; train:0.7239; eval:0.5400; lr:0.000016
epoch:83; metric:emoval; train:0.7277; eval:0.5540; lr:0.000016
Early stopping at epoch 83, best epoch: 53
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 52, duration: 340.1172194480896 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2235; eval:0.0890; lr:0.000500
epoch:2; metric:emoval; train:0.0836; eval:0.1488; lr:0.000500
epoch:3; metric:emoval; train:0.1523; eval:0.2064; lr:0.000500
epoch:4; metric:emoval; train:0.2923; eval:0.4129; lr:0.000500
epoch:5; metric:emoval; train:0.3819; eval:0.4427; lr:0.000500
epoch:6; metric:emoval; train:0.3631; eval:0.4763; lr:0.000500
epoch:7; metric:emoval; train:0.4192; eval:0.4851; lr:0.000500
epoch:8; metric:emoval; train:0.4504; eval:0.4572; lr:0.000500
epoch:9; metric:emoval; train:0.4132; eval:0.5364; lr:0.000500
epoch:10; metric:emoval; train:0.4414; eval:0.5277; lr:0.000500
epoch:11; metric:emoval; train:0.4758; eval:0.5285; lr:0.000500
epoch:12; metric:emoval; train:0.4924; eval:0.4974; lr:0.000500
epoch:13; metric:emoval; train:0.4944; eval:0.5641; lr:0.000500
epoch:14; metric:emoval; train:0.4807; eval:0.5139; lr:0.000500
epoch:15; metric:emoval; train:0.4685; eval:0.5243; lr:0.000500
epoch:16; metric:emoval; train:0.5058; eval:0.4974; lr:0.000500
epoch:17; metric:emoval; train:0.4659; eval:0.5822; lr:0.000500
epoch:18; metric:emoval; train:0.4618; eval:0.5369; lr:0.000500
epoch:19; metric:emoval; train:0.5024; eval:0.5379; lr:0.000500
epoch:20; metric:emoval; train:0.4902; eval:0.5799; lr:0.000500
epoch:21; metric:emoval; train:0.4670; eval:0.4590; lr:0.000500
epoch:22; metric:emoval; train:0.4818; eval:0.5919; lr:0.000500
epoch:23; metric:emoval; train:0.4620; eval:0.5600; lr:0.000500
epoch:24; metric:emoval; train:0.4335; eval:0.5639; lr:0.000500
epoch:25; metric:emoval; train:0.4576; eval:0.5702; lr:0.000500
epoch:26; metric:emoval; train:0.4610; eval:0.5477; lr:0.000500
epoch:27; metric:emoval; train:0.4766; eval:0.5666; lr:0.000500
epoch:28; metric:emoval; train:0.4418; eval:0.5424; lr:0.000500
epoch:29; metric:emoval; train:0.4801; eval:0.5548; lr:0.000500
epoch:30; metric:emoval; train:0.4601; eval:0.5432; lr:0.000500
epoch:31; metric:emoval; train:0.4643; eval:0.4932; lr:0.000500
epoch:32; metric:emoval; train:0.4662; eval:0.5183; lr:0.000500
epoch:33; metric:emoval; train:0.4723; eval:0.5558; lr:0.000250
epoch:34; metric:emoval; train:0.5013; eval:0.6109; lr:0.000250
epoch:35; metric:emoval; train:0.5289; eval:0.5484; lr:0.000250
epoch:36; metric:emoval; train:0.5646; eval:0.5680; lr:0.000250
epoch:37; metric:emoval; train:0.5486; eval:0.5546; lr:0.000250
epoch:38; metric:emoval; train:0.5435; eval:0.5890; lr:0.000250
epoch:39; metric:emoval; train:0.5566; eval:0.6024; lr:0.000250
epoch:40; metric:emoval; train:0.5724; eval:0.5382; lr:0.000250
epoch:41; metric:emoval; train:0.5481; eval:0.6007; lr:0.000250
epoch:42; metric:emoval; train:0.5428; eval:0.6001; lr:0.000250
epoch:43; metric:emoval; train:0.5586; eval:0.5531; lr:0.000250
epoch:44; metric:emoval; train:0.5553; eval:0.5949; lr:0.000250
epoch:45; metric:emoval; train:0.5838; eval:0.5698; lr:0.000125
epoch:46; metric:emoval; train:0.6088; eval:0.5872; lr:0.000125
epoch:47; metric:emoval; train:0.6271; eval:0.5899; lr:0.000125
epoch:48; metric:emoval; train:0.6061; eval:0.5783; lr:0.000125
epoch:49; metric:emoval; train:0.6209; eval:0.5945; lr:0.000125
epoch:50; metric:emoval; train:0.6544; eval:0.5970; lr:0.000125
epoch:51; metric:emoval; train:0.6309; eval:0.6038; lr:0.000125
epoch:52; metric:emoval; train:0.6312; eval:0.6052; lr:0.000125
epoch:53; metric:emoval; train:0.6354; eval:0.5734; lr:0.000125
epoch:54; metric:emoval; train:0.6167; eval:0.5842; lr:0.000125
epoch:55; metric:emoval; train:0.6270; eval:0.5772; lr:0.000125
epoch:56; metric:emoval; train:0.6526; eval:0.5939; lr:0.000063
epoch:57; metric:emoval; train:0.6487; eval:0.5886; lr:0.000063
epoch:58; metric:emoval; train:0.6649; eval:0.6029; lr:0.000063
epoch:59; metric:emoval; train:0.6469; eval:0.5763; lr:0.000063
epoch:60; metric:emoval; train:0.6646; eval:0.5931; lr:0.000063
epoch:61; metric:emoval; train:0.6625; eval:0.5874; lr:0.000063
epoch:62; metric:emoval; train:0.6498; eval:0.5858; lr:0.000063
epoch:63; metric:emoval; train:0.6830; eval:0.5872; lr:0.000063
epoch:64; metric:emoval; train:0.6760; eval:0.5699; lr:0.000063
Early stopping at epoch 64, best epoch: 34
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 33, duration: 254.90131092071533 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2250; eval:-0.0255; lr:0.000500
epoch:2; metric:emoval; train:0.0279; eval:-0.0537; lr:0.000500
epoch:3; metric:emoval; train:0.1136; eval:0.0025; lr:0.000500
epoch:4; metric:emoval; train:0.2214; eval:0.0025; lr:0.000500
epoch:5; metric:emoval; train:0.3737; eval:0.2905; lr:0.000500
epoch:6; metric:emoval; train:0.4490; eval:0.3309; lr:0.000500
epoch:7; metric:emoval; train:0.4521; eval:0.4250; lr:0.000500
epoch:8; metric:emoval; train:0.5464; eval:0.5264; lr:0.000500
epoch:9; metric:emoval; train:0.5585; eval:0.4638; lr:0.000500
epoch:10; metric:emoval; train:0.5924; eval:0.5049; lr:0.000500
epoch:11; metric:emoval; train:0.5660; eval:0.5347; lr:0.000500
epoch:12; metric:emoval; train:0.5927; eval:0.5685; lr:0.000500
epoch:13; metric:emoval; train:0.5789; eval:0.5275; lr:0.000500
epoch:14; metric:emoval; train:0.5830; eval:0.5505; lr:0.000500
epoch:15; metric:emoval; train:0.5876; eval:0.4099; lr:0.000500
epoch:16; metric:emoval; train:0.5829; eval:0.5074; lr:0.000500
epoch:17; metric:emoval; train:0.6097; eval:0.5347; lr:0.000500
epoch:18; metric:emoval; train:0.5927; eval:0.4925; lr:0.000500
epoch:19; metric:emoval; train:0.5797; eval:0.5264; lr:0.000500
epoch:20; metric:emoval; train:0.5666; eval:0.5702; lr:0.000500
epoch:21; metric:emoval; train:0.5642; eval:0.5128; lr:0.000500
epoch:22; metric:emoval; train:0.5435; eval:0.5044; lr:0.000500
epoch:23; metric:emoval; train:0.5701; eval:0.5325; lr:0.000500
epoch:24; metric:emoval; train:0.5565; eval:0.5683; lr:0.000500
epoch:25; metric:emoval; train:0.5327; eval:0.5168; lr:0.000500
epoch:26; metric:emoval; train:0.5271; eval:0.4810; lr:0.000500
epoch:27; metric:emoval; train:0.5360; eval:0.5029; lr:0.000500
epoch:28; metric:emoval; train:0.5156; eval:0.5474; lr:0.000500
epoch:29; metric:emoval; train:0.5104; eval:0.5492; lr:0.000500
epoch:30; metric:emoval; train:0.5111; eval:0.5779; lr:0.000500
epoch:31; metric:emoval; train:0.5099; eval:0.5582; lr:0.000500
epoch:32; metric:emoval; train:0.5219; eval:0.5390; lr:0.000500
epoch:33; metric:emoval; train:0.5133; eval:0.5366; lr:0.000500
epoch:34; metric:emoval; train:0.5024; eval:0.4604; lr:0.000500
epoch:35; metric:emoval; train:0.5169; eval:0.5224; lr:0.000500
epoch:36; metric:emoval; train:0.5188; eval:0.5127; lr:0.000500
epoch:37; metric:emoval; train:0.5332; eval:0.5750; lr:0.000500
epoch:38; metric:emoval; train:0.5298; eval:0.5788; lr:0.000500
epoch:39; metric:emoval; train:0.5501; eval:0.5288; lr:0.000500
epoch:40; metric:emoval; train:0.5359; eval:0.5587; lr:0.000500
epoch:41; metric:emoval; train:0.5544; eval:0.5900; lr:0.000500
epoch:42; metric:emoval; train:0.5216; eval:0.5801; lr:0.000500
epoch:43; metric:emoval; train:0.5221; eval:0.5788; lr:0.000500
epoch:44; metric:emoval; train:0.5449; eval:0.5741; lr:0.000500
epoch:45; metric:emoval; train:0.5492; eval:0.5541; lr:0.000500
epoch:46; metric:emoval; train:0.5568; eval:0.5673; lr:0.000500
epoch:47; metric:emoval; train:0.5400; eval:0.5622; lr:0.000500
epoch:48; metric:emoval; train:0.5377; eval:0.5635; lr:0.000500
epoch:49; metric:emoval; train:0.5338; eval:0.5979; lr:0.000500
epoch:50; metric:emoval; train:0.5158; eval:0.5974; lr:0.000500
epoch:51; metric:emoval; train:0.5430; eval:0.5568; lr:0.000500
epoch:52; metric:emoval; train:0.5505; eval:0.5846; lr:0.000500
epoch:53; metric:emoval; train:0.5358; eval:0.5118; lr:0.000500
epoch:54; metric:emoval; train:0.5433; eval:0.5569; lr:0.000500
epoch:55; metric:emoval; train:0.2461; eval:0.4798; lr:0.000500
epoch:56; metric:emoval; train:0.3706; eval:0.5948; lr:0.000500
epoch:57; metric:emoval; train:0.4936; eval:0.5549; lr:0.000500
epoch:58; metric:emoval; train:0.5361; eval:0.5518; lr:0.000500
epoch:59; metric:emoval; train:0.5379; eval:0.5439; lr:0.000500
epoch:60; metric:emoval; train:0.5756; eval:0.5679; lr:0.000250
epoch:61; metric:emoval; train:0.5948; eval:0.5732; lr:0.000250
epoch:62; metric:emoval; train:0.5909; eval:0.6234; lr:0.000250
epoch:63; metric:emoval; train:0.6105; eval:0.5746; lr:0.000250
epoch:64; metric:emoval; train:0.5898; eval:0.6157; lr:0.000250
epoch:65; metric:emoval; train:0.6275; eval:0.5812; lr:0.000250
epoch:66; metric:emoval; train:0.6224; eval:0.6062; lr:0.000250
epoch:67; metric:emoval; train:0.6141; eval:0.5844; lr:0.000250
epoch:68; metric:emoval; train:0.6049; eval:0.5789; lr:0.000250
epoch:69; metric:emoval; train:0.6100; eval:0.5933; lr:0.000250
epoch:70; metric:emoval; train:0.6325; eval:0.5581; lr:0.000250
epoch:71; metric:emoval; train:0.5994; eval:0.5991; lr:0.000250
epoch:72; metric:emoval; train:0.6000; eval:0.5985; lr:0.000250
epoch:73; metric:emoval; train:0.6181; eval:0.5928; lr:0.000125
epoch:74; metric:emoval; train:0.6393; eval:0.6202; lr:0.000125
epoch:75; metric:emoval; train:0.6655; eval:0.5951; lr:0.000125
epoch:76; metric:emoval; train:0.6423; eval:0.6068; lr:0.000125
epoch:77; metric:emoval; train:0.6656; eval:0.5870; lr:0.000125
epoch:78; metric:emoval; train:0.6639; eval:0.6341; lr:0.000125
epoch:79; metric:emoval; train:0.6537; eval:0.6157; lr:0.000125
epoch:80; metric:emoval; train:0.6467; eval:0.6099; lr:0.000125
epoch:81; metric:emoval; train:0.6491; eval:0.5986; lr:0.000125
epoch:82; metric:emoval; train:0.6689; eval:0.6281; lr:0.000125
epoch:83; metric:emoval; train:0.6739; eval:0.5916; lr:0.000125
epoch:84; metric:emoval; train:0.6767; eval:0.6109; lr:0.000125
epoch:85; metric:emoval; train:0.6599; eval:0.6085; lr:0.000125
epoch:86; metric:emoval; train:0.6611; eval:0.5754; lr:0.000125
epoch:87; metric:emoval; train:0.6717; eval:0.6073; lr:0.000125
epoch:88; metric:emoval; train:0.6436; eval:0.6213; lr:0.000125
epoch:89; metric:emoval; train:0.6691; eval:0.6179; lr:0.000063
epoch:90; metric:emoval; train:0.6820; eval:0.6035; lr:0.000063
epoch:91; metric:emoval; train:0.6837; eval:0.6053; lr:0.000063
epoch:92; metric:emoval; train:0.6797; eval:0.5964; lr:0.000063
epoch:93; metric:emoval; train:0.6965; eval:0.6105; lr:0.000063
epoch:94; metric:emoval; train:0.6880; eval:0.6150; lr:0.000063
epoch:95; metric:emoval; train:0.7105; eval:0.6208; lr:0.000063
epoch:96; metric:emoval; train:0.6709; eval:0.6107; lr:0.000063
epoch:97; metric:emoval; train:0.6874; eval:0.6163; lr:0.000063
epoch:98; metric:emoval; train:0.6989; eval:0.6160; lr:0.000063
epoch:99; metric:emoval; train:0.6831; eval:0.6225; lr:0.000063
epoch:100; metric:emoval; train:0.6769; eval:0.6246; lr:0.000031
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 77, duration: 414.2603950500488 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2436; eval:-0.0906; lr:0.000500
epoch:2; metric:emoval; train:0.0005; eval:-0.0030; lr:0.000500
epoch:3; metric:emoval; train:0.1737; eval:0.0549; lr:0.000500
epoch:4; metric:emoval; train:0.2845; eval:0.2795; lr:0.000500
epoch:5; metric:emoval; train:0.4047; eval:0.2892; lr:0.000500
epoch:6; metric:emoval; train:0.4625; eval:0.4591; lr:0.000500
epoch:7; metric:emoval; train:0.5030; eval:0.4979; lr:0.000500
epoch:8; metric:emoval; train:0.5464; eval:0.5141; lr:0.000500
epoch:9; metric:emoval; train:0.5589; eval:0.4765; lr:0.000500
epoch:10; metric:emoval; train:0.5656; eval:0.5216; lr:0.000500
epoch:11; metric:emoval; train:0.5850; eval:0.5292; lr:0.000500
epoch:12; metric:emoval; train:0.5815; eval:0.4962; lr:0.000500
epoch:13; metric:emoval; train:0.5899; eval:0.5259; lr:0.000500
epoch:14; metric:emoval; train:0.5892; eval:0.5310; lr:0.000500
epoch:15; metric:emoval; train:0.5887; eval:0.5565; lr:0.000500
epoch:16; metric:emoval; train:0.5902; eval:0.5267; lr:0.000500
epoch:17; metric:emoval; train:0.6022; eval:0.5202; lr:0.000500
epoch:18; metric:emoval; train:0.5359; eval:0.5164; lr:0.000500
epoch:19; metric:emoval; train:0.5571; eval:0.5709; lr:0.000500
epoch:20; metric:emoval; train:0.5432; eval:0.5789; lr:0.000500
epoch:21; metric:emoval; train:0.5403; eval:0.5521; lr:0.000500
epoch:22; metric:emoval; train:0.5222; eval:0.5461; lr:0.000500
epoch:23; metric:emoval; train:0.5580; eval:0.5382; lr:0.000500
epoch:24; metric:emoval; train:0.5378; eval:0.5620; lr:0.000500
epoch:25; metric:emoval; train:0.5396; eval:0.4707; lr:0.000500
epoch:26; metric:emoval; train:0.5163; eval:0.5452; lr:0.000500
epoch:27; metric:emoval; train:0.5294; eval:0.5621; lr:0.000500
epoch:28; metric:emoval; train:0.5222; eval:0.4447; lr:0.000500
epoch:29; metric:emoval; train:0.5231; eval:0.5188; lr:0.000500
epoch:30; metric:emoval; train:0.4931; eval:0.5535; lr:0.000500
epoch:31; metric:emoval; train:0.5209; eval:0.5330; lr:0.000250
epoch:32; metric:emoval; train:0.5502; eval:0.5776; lr:0.000250
epoch:33; metric:emoval; train:0.5747; eval:0.5920; lr:0.000250
epoch:34; metric:emoval; train:0.5930; eval:0.5767; lr:0.000250
epoch:35; metric:emoval; train:0.5948; eval:0.5920; lr:0.000250
epoch:36; metric:emoval; train:0.5702; eval:0.5961; lr:0.000250
epoch:37; metric:emoval; train:0.6052; eval:0.5634; lr:0.000250
epoch:38; metric:emoval; train:0.5981; eval:0.5683; lr:0.000250
epoch:39; metric:emoval; train:0.6081; eval:0.5781; lr:0.000250
epoch:40; metric:emoval; train:0.6365; eval:0.5455; lr:0.000250
epoch:41; metric:emoval; train:0.5948; eval:0.5231; lr:0.000250
epoch:42; metric:emoval; train:0.6227; eval:0.5704; lr:0.000250
epoch:43; metric:emoval; train:0.6098; eval:0.5125; lr:0.000250
epoch:44; metric:emoval; train:0.6237; eval:0.5253; lr:0.000250
epoch:45; metric:emoval; train:0.6105; eval:0.5137; lr:0.000250
epoch:46; metric:emoval; train:0.6196; eval:0.5363; lr:0.000250
epoch:47; metric:emoval; train:0.6162; eval:0.5942; lr:0.000125
epoch:48; metric:emoval; train:0.6469; eval:0.5883; lr:0.000125
epoch:49; metric:emoval; train:0.6602; eval:0.5831; lr:0.000125
epoch:50; metric:emoval; train:0.6794; eval:0.5943; lr:0.000125
epoch:51; metric:emoval; train:0.6600; eval:0.5823; lr:0.000125
epoch:52; metric:emoval; train:0.6614; eval:0.5957; lr:0.000125
epoch:53; metric:emoval; train:0.6672; eval:0.5974; lr:0.000125
epoch:54; metric:emoval; train:0.6547; eval:0.5849; lr:0.000125
epoch:55; metric:emoval; train:0.6537; eval:0.5751; lr:0.000125
epoch:56; metric:emoval; train:0.6660; eval:0.5491; lr:0.000125
epoch:57; metric:emoval; train:0.6521; eval:0.5844; lr:0.000125
epoch:58; metric:emoval; train:0.6830; eval:0.5722; lr:0.000125
epoch:59; metric:emoval; train:0.6583; eval:0.5818; lr:0.000125
epoch:60; metric:emoval; train:0.6845; eval:0.5809; lr:0.000125
epoch:61; metric:emoval; train:0.6759; eval:0.5975; lr:0.000125
epoch:62; metric:emoval; train:0.6909; eval:0.5531; lr:0.000125
epoch:63; metric:emoval; train:0.6722; eval:0.5525; lr:0.000125
epoch:64; metric:emoval; train:0.6734; eval:0.5668; lr:0.000063
epoch:65; metric:emoval; train:0.6869; eval:0.5739; lr:0.000063
epoch:66; metric:emoval; train:0.6873; eval:0.5541; lr:0.000063
epoch:67; metric:emoval; train:0.6841; eval:0.6138; lr:0.000063
epoch:68; metric:emoval; train:0.6938; eval:0.5648; lr:0.000063
epoch:69; metric:emoval; train:0.7113; eval:0.5624; lr:0.000063
epoch:70; metric:emoval; train:0.6857; eval:0.6060; lr:0.000063
epoch:71; metric:emoval; train:0.6935; eval:0.5781; lr:0.000063
epoch:72; metric:emoval; train:0.6868; eval:0.5760; lr:0.000063
epoch:73; metric:emoval; train:0.7129; eval:0.5963; lr:0.000063
epoch:74; metric:emoval; train:0.7028; eval:0.5721; lr:0.000063
epoch:75; metric:emoval; train:0.6990; eval:0.5525; lr:0.000063
epoch:76; metric:emoval; train:0.6888; eval:0.5548; lr:0.000063
epoch:77; metric:emoval; train:0.7011; eval:0.5324; lr:0.000063
epoch:78; metric:emoval; train:0.7086; eval:0.5746; lr:0.000031
epoch:79; metric:emoval; train:0.7176; eval:0.5792; lr:0.000031
epoch:80; metric:emoval; train:0.7131; eval:0.5928; lr:0.000031
epoch:81; metric:emoval; train:0.7221; eval:0.5657; lr:0.000031
epoch:82; metric:emoval; train:0.7240; eval:0.5657; lr:0.000031
epoch:83; metric:emoval; train:0.7359; eval:0.5846; lr:0.000031
epoch:84; metric:emoval; train:0.7210; eval:0.5594; lr:0.000031
epoch:85; metric:emoval; train:0.7173; eval:0.5616; lr:0.000031
epoch:86; metric:emoval; train:0.7278; eval:0.5794; lr:0.000031
epoch:87; metric:emoval; train:0.7069; eval:0.5577; lr:0.000031
epoch:88; metric:emoval; train:0.7194; eval:0.5613; lr:0.000031
epoch:89; metric:emoval; train:0.7055; eval:0.5633; lr:0.000016
epoch:90; metric:emoval; train:0.7284; eval:0.5758; lr:0.000016
epoch:91; metric:emoval; train:0.7200; eval:0.5692; lr:0.000016
epoch:92; metric:emoval; train:0.7301; eval:0.5675; lr:0.000016
epoch:93; metric:emoval; train:0.7376; eval:0.5758; lr:0.000016
epoch:94; metric:emoval; train:0.7273; eval:0.5767; lr:0.000016
epoch:95; metric:emoval; train:0.7222; eval:0.5729; lr:0.000016
epoch:96; metric:emoval; train:0.7404; eval:0.5579; lr:0.000016
epoch:97; metric:emoval; train:0.7508; eval:0.5721; lr:0.000016
Early stopping at epoch 97, best epoch: 67
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 66, duration: 393.1657438278198 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2981; eval:0.0541; lr:0.000500
epoch:2; metric:emoval; train:-0.0423; eval:0.0584; lr:0.000500
epoch:3; metric:emoval; train:0.0532; eval:0.0679; lr:0.000500
epoch:4; metric:emoval; train:0.1583; eval:0.0090; lr:0.000500
epoch:5; metric:emoval; train:0.1981; eval:0.1483; lr:0.000500
epoch:6; metric:emoval; train:0.2588; eval:0.0851; lr:0.000500
epoch:7; metric:emoval; train:0.3807; eval:0.4530; lr:0.000500
epoch:8; metric:emoval; train:0.4694; eval:0.5234; lr:0.000500
epoch:9; metric:emoval; train:0.5443; eval:0.5066; lr:0.000500
epoch:10; metric:emoval; train:0.5741; eval:0.5670; lr:0.000500
epoch:11; metric:emoval; train:0.5745; eval:0.5383; lr:0.000500
epoch:12; metric:emoval; train:0.5779; eval:0.5252; lr:0.000500
epoch:13; metric:emoval; train:0.5892; eval:0.5715; lr:0.000500
epoch:14; metric:emoval; train:0.6011; eval:0.5510; lr:0.000500
epoch:15; metric:emoval; train:0.5851; eval:0.5584; lr:0.000500
epoch:16; metric:emoval; train:0.5946; eval:0.5605; lr:0.000500
epoch:17; metric:emoval; train:0.5986; eval:0.6028; lr:0.000500
epoch:18; metric:emoval; train:0.5738; eval:0.5247; lr:0.000500
epoch:19; metric:emoval; train:0.5871; eval:0.5501; lr:0.000500
epoch:20; metric:emoval; train:0.5775; eval:0.4587; lr:0.000500
epoch:21; metric:emoval; train:0.5661; eval:0.5634; lr:0.000500
epoch:22; metric:emoval; train:0.5579; eval:0.5565; lr:0.000500
epoch:23; metric:emoval; train:0.5562; eval:0.5848; lr:0.000500
epoch:24; metric:emoval; train:0.5437; eval:0.5619; lr:0.000500
epoch:25; metric:emoval; train:0.5395; eval:0.5592; lr:0.000500
epoch:26; metric:emoval; train:0.5305; eval:0.5515; lr:0.000500
epoch:27; metric:emoval; train:0.4910; eval:0.6179; lr:0.000500
epoch:28; metric:emoval; train:0.5202; eval:0.5560; lr:0.000500
epoch:29; metric:emoval; train:0.4899; eval:0.5402; lr:0.000500
epoch:30; metric:emoval; train:0.5146; eval:0.5848; lr:0.000500
epoch:31; metric:emoval; train:0.5157; eval:0.5777; lr:0.000500
epoch:32; metric:emoval; train:0.4425; eval:0.5769; lr:0.000500
epoch:33; metric:emoval; train:0.4414; eval:0.5939; lr:0.000500
epoch:34; metric:emoval; train:0.5019; eval:0.5526; lr:0.000500
epoch:35; metric:emoval; train:0.5010; eval:0.5434; lr:0.000500
epoch:36; metric:emoval; train:0.5042; eval:0.5784; lr:0.000500
epoch:37; metric:emoval; train:0.5173; eval:0.5302; lr:0.000500
epoch:38; metric:emoval; train:0.5014; eval:0.5901; lr:0.000250
epoch:39; metric:emoval; train:0.5693; eval:0.6078; lr:0.000250
epoch:40; metric:emoval; train:0.5737; eval:0.5794; lr:0.000250
epoch:41; metric:emoval; train:0.5789; eval:0.5671; lr:0.000250
epoch:42; metric:emoval; train:0.5765; eval:0.5734; lr:0.000250
epoch:43; metric:emoval; train:0.5953; eval:0.6109; lr:0.000250
epoch:44; metric:emoval; train:0.5754; eval:0.6007; lr:0.000250
epoch:45; metric:emoval; train:0.5847; eval:0.6276; lr:0.000250
epoch:46; metric:emoval; train:0.5881; eval:0.5786; lr:0.000250
epoch:47; metric:emoval; train:0.5908; eval:0.6112; lr:0.000250
epoch:48; metric:emoval; train:0.5967; eval:0.6154; lr:0.000250
epoch:49; metric:emoval; train:0.5938; eval:0.5861; lr:0.000250
epoch:50; metric:emoval; train:0.6088; eval:0.6146; lr:0.000250
epoch:51; metric:emoval; train:0.6079; eval:0.5663; lr:0.000250
epoch:52; metric:emoval; train:0.5987; eval:0.5769; lr:0.000250
epoch:53; metric:emoval; train:0.5959; eval:0.6030; lr:0.000250
epoch:54; metric:emoval; train:0.6106; eval:0.5389; lr:0.000250
epoch:55; metric:emoval; train:0.6103; eval:0.5813; lr:0.000250
epoch:56; metric:emoval; train:0.6109; eval:0.5651; lr:0.000125
epoch:57; metric:emoval; train:0.6275; eval:0.5888; lr:0.000125
epoch:58; metric:emoval; train:0.6459; eval:0.6230; lr:0.000125
epoch:59; metric:emoval; train:0.6422; eval:0.6305; lr:0.000125
epoch:60; metric:emoval; train:0.6511; eval:0.6076; lr:0.000125
epoch:61; metric:emoval; train:0.6474; eval:0.5776; lr:0.000125
epoch:62; metric:emoval; train:0.6511; eval:0.6047; lr:0.000125
epoch:63; metric:emoval; train:0.6637; eval:0.6196; lr:0.000125
epoch:64; metric:emoval; train:0.6548; eval:0.6018; lr:0.000125
epoch:65; metric:emoval; train:0.6446; eval:0.5995; lr:0.000125
epoch:66; metric:emoval; train:0.6711; eval:0.6194; lr:0.000125
epoch:67; metric:emoval; train:0.6593; eval:0.6056; lr:0.000125
epoch:68; metric:emoval; train:0.6553; eval:0.6202; lr:0.000125
epoch:69; metric:emoval; train:0.6607; eval:0.6007; lr:0.000125
epoch:70; metric:emoval; train:0.6425; eval:0.5934; lr:0.000063
epoch:71; metric:emoval; train:0.6877; eval:0.6220; lr:0.000063
epoch:72; metric:emoval; train:0.6773; eval:0.6268; lr:0.000063
epoch:73; metric:emoval; train:0.6629; eval:0.6225; lr:0.000063
epoch:74; metric:emoval; train:0.6869; eval:0.6245; lr:0.000063
epoch:75; metric:emoval; train:0.6639; eval:0.6020; lr:0.000063
epoch:76; metric:emoval; train:0.6958; eval:0.6220; lr:0.000063
epoch:77; metric:emoval; train:0.6898; eval:0.6252; lr:0.000063
epoch:78; metric:emoval; train:0.6895; eval:0.6230; lr:0.000063
epoch:79; metric:emoval; train:0.7031; eval:0.6221; lr:0.000063
epoch:80; metric:emoval; train:0.7011; eval:0.6090; lr:0.000063
epoch:81; metric:emoval; train:0.6991; eval:0.6105; lr:0.000031
epoch:82; metric:emoval; train:0.6899; eval:0.6141; lr:0.000031
epoch:83; metric:emoval; train:0.6982; eval:0.6158; lr:0.000031
epoch:84; metric:emoval; train:0.7043; eval:0.6104; lr:0.000031
epoch:85; metric:emoval; train:0.6973; eval:0.6212; lr:0.000031
epoch:86; metric:emoval; train:0.7233; eval:0.6088; lr:0.000031
epoch:87; metric:emoval; train:0.6962; eval:0.5912; lr:0.000031
epoch:88; metric:emoval; train:0.6830; eval:0.5955; lr:0.000031
epoch:89; metric:emoval; train:0.6970; eval:0.6212; lr:0.000031
Early stopping at epoch 89, best epoch: 59
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 58, duration: 363.92533135414124 >>>>>
====== Prediction and Saving =======
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_cons_strong_qw0.60_iw0.10_ce0.12_cv0.08_cr0.45_dm0.35_ln0.02_20260214_192908-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v9+utt+None_f1:0.7669_acc:0.7667_val:0.6058_1771069957.5552948.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_cons_strong_qw0.60_iw0.10_ce0.12_cv0.08_cr0.45_dm0.35_ln0.02_20260214_192908-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v9+utt+None_f1:0.8284_acc:0.8273_val:0.6201_1771069957.5552948.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_cons_strong_qw0.60_iw0.10_ce0.12_cv0.08_cr0.45_dm0.35_ln0.02_20260214_192908-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v9+utt+None_f1:0.7551_acc:0.7597_val:0.6563_1771069957.5552948.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_cons_strong_qw0.60_iw0.10_ce0.12_cv0.08_cr0.45_dm0.35_ln0.02_20260214_192908-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v9+utt+None_f1:0.8870_acc:0.8885_val:81.1785_1771069957.5552948.npz
