====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, consistency_emo_weight=0.1, consistency_val_weight=0.06, contrastive_temperature=0.07, contrastive_weight=0.1, corruption_max_rate=0.55, corruption_warmup_epochs=25, cross_kl_weight=0.01, dataset='MER2023', debug=False, double_mask_ratio=0.45, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, emo_loss_weight=1.0, epochs=100, feat_scale=1, feat_type='utt', feature_noise_prob=0.35, feature_noise_std=0.02, feature_noise_warmup=5, focal_gamma=2.0, fusion_residual_scale=0.4, fusion_temperature=1.0, gate_alpha=0.55, gpu=0, grad_clip=1.0, hidden_dim=128, huber_beta=0.8, hyper_path=None, impute_loss_weight=0.12, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, latent_noise_std=0.03, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_agreement_weight=0.008, modality_dropout=0.18, modality_dropout_warmup=15, model='attention_robust_v9', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, quality_weight=0.7, recon_weight=0.1, reg_loss_type='smoothl1', reliability_temperature=0.9, save_iters=100000000.0, save_root='/root/autodl-tmp/MERTools-master/MERBench/attention_robust_v9/outputs/sweep_results/v9_high_corrupt_qw0.70_iw0.12_ce0.10_cv0.06_cr0.55_dm0.45_ln0.03_20260214_195842-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_gated_uncertainty=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, use_valence_prior=True, val_loss_weight=1.4, valence_center_reg_weight=0.005, valence_consistency_weight=0.1, video_feature='clip-vit-large-patch14-UTT', weight_consistency_weight=0.02)
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 41%|████      | 1371/3373 [00:00<00:00, 13669.34it/s] 81%|████████  | 2738/3373 [00:00<00:00, 13072.48it/s]100%|██████████| 3373/3373 [00:00<00:00, 13324.96it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s] 30%|██▉       | 997/3373 [00:00<00:00, 9942.02it/s] 59%|█████▉    | 1992/3373 [00:00<00:00, 8569.60it/s] 85%|████████▍ | 2861/3373 [00:00<00:00, 7851.45it/s]100%|██████████| 3373/3373 [00:00<00:00, 7816.87it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s] 47%|████▋     | 1590/3373 [00:00<00:00, 15898.16it/s] 98%|█████████▊| 3310/3373 [00:00<00:00, 16659.85it/s]100%|██████████| 3373/3373 [00:00<00:00, 16724.02it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 10252.22it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 12387.60it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 17873.83it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 16049.38it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 12432.84it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 15765.04it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 13691.85it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 14035.09it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 15440.79it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2434; eval:-0.0612; lr:0.000500
epoch:2; metric:emoval; train:-0.0248; eval:-0.0074; lr:0.000500
epoch:3; metric:emoval; train:0.1586; eval:0.0899; lr:0.000500
epoch:4; metric:emoval; train:0.2811; eval:0.2395; lr:0.000500
epoch:5; metric:emoval; train:0.3912; eval:0.4228; lr:0.000500
epoch:6; metric:emoval; train:0.4956; eval:0.4891; lr:0.000500
epoch:7; metric:emoval; train:0.5150; eval:0.5238; lr:0.000500
epoch:8; metric:emoval; train:0.5535; eval:0.4740; lr:0.000500
epoch:9; metric:emoval; train:0.5667; eval:0.5577; lr:0.000500
epoch:10; metric:emoval; train:0.5812; eval:0.4735; lr:0.000500
epoch:11; metric:emoval; train:0.5624; eval:0.5211; lr:0.000500
epoch:12; metric:emoval; train:0.5782; eval:0.5341; lr:0.000500
epoch:13; metric:emoval; train:0.5685; eval:0.5035; lr:0.000500
epoch:14; metric:emoval; train:0.5638; eval:0.5268; lr:0.000500
epoch:15; metric:emoval; train:0.5575; eval:0.5660; lr:0.000500
epoch:16; metric:emoval; train:0.5788; eval:0.4311; lr:0.000500
epoch:17; metric:emoval; train:0.5461; eval:0.4862; lr:0.000500
epoch:18; metric:emoval; train:0.5557; eval:0.4571; lr:0.000500
epoch:19; metric:emoval; train:0.5387; eval:0.5588; lr:0.000500
epoch:20; metric:emoval; train:0.5545; eval:0.5370; lr:0.000500
epoch:21; metric:emoval; train:0.5233; eval:0.4664; lr:0.000500
epoch:22; metric:emoval; train:0.5210; eval:0.5201; lr:0.000500
epoch:23; metric:emoval; train:0.5000; eval:0.5217; lr:0.000500
epoch:24; metric:emoval; train:0.5134; eval:0.5240; lr:0.000500
epoch:25; metric:emoval; train:0.4690; eval:0.5477; lr:0.000500
epoch:26; metric:emoval; train:0.4825; eval:0.5217; lr:0.000250
epoch:27; metric:emoval; train:0.5445; eval:0.5536; lr:0.000250
epoch:28; metric:emoval; train:0.5503; eval:0.5657; lr:0.000250
epoch:29; metric:emoval; train:0.5177; eval:0.5781; lr:0.000250
epoch:30; metric:emoval; train:0.5314; eval:0.5091; lr:0.000250
epoch:31; metric:emoval; train:0.5280; eval:0.5172; lr:0.000250
epoch:32; metric:emoval; train:0.5557; eval:0.5670; lr:0.000250
epoch:33; metric:emoval; train:0.5470; eval:0.5402; lr:0.000250
epoch:34; metric:emoval; train:0.5249; eval:0.5209; lr:0.000250
epoch:35; metric:emoval; train:0.5405; eval:0.5424; lr:0.000250
epoch:36; metric:emoval; train:0.5412; eval:0.5223; lr:0.000250
epoch:37; metric:emoval; train:0.5591; eval:0.5618; lr:0.000250
epoch:38; metric:emoval; train:0.5470; eval:0.4999; lr:0.000250
epoch:39; metric:emoval; train:0.5804; eval:0.5527; lr:0.000250
epoch:40; metric:emoval; train:0.5602; eval:0.5298; lr:0.000125
epoch:41; metric:emoval; train:0.5790; eval:0.5555; lr:0.000125
epoch:42; metric:emoval; train:0.6040; eval:0.5591; lr:0.000125
epoch:43; metric:emoval; train:0.5998; eval:0.5603; lr:0.000125
epoch:44; metric:emoval; train:0.5905; eval:0.5746; lr:0.000125
epoch:45; metric:emoval; train:0.6162; eval:0.5607; lr:0.000125
epoch:46; metric:emoval; train:0.6138; eval:0.5610; lr:0.000125
epoch:47; metric:emoval; train:0.6055; eval:0.5889; lr:0.000125
epoch:48; metric:emoval; train:0.5964; eval:0.5751; lr:0.000125
epoch:49; metric:emoval; train:0.5983; eval:0.5576; lr:0.000125
epoch:50; metric:emoval; train:0.6230; eval:0.5820; lr:0.000125
epoch:51; metric:emoval; train:0.6155; eval:0.5339; lr:0.000125
epoch:52; metric:emoval; train:0.5685; eval:0.5498; lr:0.000125
epoch:53; metric:emoval; train:0.6035; eval:0.5532; lr:0.000125
epoch:54; metric:emoval; train:0.5971; eval:0.5323; lr:0.000125
epoch:55; metric:emoval; train:0.6239; eval:0.5303; lr:0.000125
epoch:56; metric:emoval; train:0.6336; eval:0.4966; lr:0.000125
epoch:57; metric:emoval; train:0.6207; eval:0.5268; lr:0.000125
epoch:58; metric:emoval; train:0.6217; eval:0.5405; lr:0.000063
epoch:59; metric:emoval; train:0.6438; eval:0.5645; lr:0.000063
epoch:60; metric:emoval; train:0.6449; eval:0.5584; lr:0.000063
epoch:61; metric:emoval; train:0.6509; eval:0.5570; lr:0.000063
epoch:62; metric:emoval; train:0.6491; eval:0.5634; lr:0.000063
epoch:63; metric:emoval; train:0.6487; eval:0.5384; lr:0.000063
epoch:64; metric:emoval; train:0.6439; eval:0.5564; lr:0.000063
epoch:65; metric:emoval; train:0.6485; eval:0.5247; lr:0.000063
epoch:66; metric:emoval; train:0.6463; eval:0.5351; lr:0.000063
epoch:67; metric:emoval; train:0.6599; eval:0.5388; lr:0.000063
epoch:68; metric:emoval; train:0.6569; eval:0.5248; lr:0.000063
epoch:69; metric:emoval; train:0.6277; eval:0.5437; lr:0.000031
epoch:70; metric:emoval; train:0.6729; eval:0.5491; lr:0.000031
epoch:71; metric:emoval; train:0.6635; eval:0.5650; lr:0.000031
epoch:72; metric:emoval; train:0.6448; eval:0.5372; lr:0.000031
epoch:73; metric:emoval; train:0.6424; eval:0.5602; lr:0.000031
epoch:74; metric:emoval; train:0.6656; eval:0.5408; lr:0.000031
epoch:75; metric:emoval; train:0.6666; eval:0.5389; lr:0.000031
epoch:76; metric:emoval; train:0.6431; eval:0.5442; lr:0.000031
epoch:77; metric:emoval; train:0.6591; eval:0.5314; lr:0.000031
Early stopping at epoch 77, best epoch: 47
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 46, duration: 320.2030818462372 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3058; eval:-0.0699; lr:0.000500
epoch:2; metric:emoval; train:0.0284; eval:0.1519; lr:0.000500
epoch:3; metric:emoval; train:0.1093; eval:0.2255; lr:0.000500
epoch:4; metric:emoval; train:0.2277; eval:0.3543; lr:0.000500
epoch:5; metric:emoval; train:0.3636; eval:0.4382; lr:0.000500
epoch:6; metric:emoval; train:0.3757; eval:0.5098; lr:0.000500
epoch:7; metric:emoval; train:0.4289; eval:0.5148; lr:0.000500
epoch:8; metric:emoval; train:0.4661; eval:0.5487; lr:0.000500
epoch:9; metric:emoval; train:0.4895; eval:0.5074; lr:0.000500
epoch:10; metric:emoval; train:0.5073; eval:0.4672; lr:0.000500
epoch:11; metric:emoval; train:0.5054; eval:0.5315; lr:0.000500
epoch:12; metric:emoval; train:0.5280; eval:0.5568; lr:0.000500
epoch:13; metric:emoval; train:0.5359; eval:0.5549; lr:0.000500
epoch:14; metric:emoval; train:0.5273; eval:0.5613; lr:0.000500
epoch:15; metric:emoval; train:0.5369; eval:0.5901; lr:0.000500
epoch:16; metric:emoval; train:0.5238; eval:0.5234; lr:0.000500
epoch:17; metric:emoval; train:0.5259; eval:0.5693; lr:0.000500
epoch:18; metric:emoval; train:0.5228; eval:0.5631; lr:0.000500
epoch:19; metric:emoval; train:0.5230; eval:0.4998; lr:0.000500
epoch:20; metric:emoval; train:0.4789; eval:0.5504; lr:0.000500
epoch:21; metric:emoval; train:0.5042; eval:0.5828; lr:0.000500
epoch:22; metric:emoval; train:0.4752; eval:0.5834; lr:0.000500
epoch:23; metric:emoval; train:0.4905; eval:0.5757; lr:0.000500
epoch:24; metric:emoval; train:0.4964; eval:0.5602; lr:0.000500
epoch:25; metric:emoval; train:0.4819; eval:0.5724; lr:0.000500
epoch:26; metric:emoval; train:0.4860; eval:0.5640; lr:0.000250
epoch:27; metric:emoval; train:0.4974; eval:0.5877; lr:0.000250
epoch:28; metric:emoval; train:0.5308; eval:0.6070; lr:0.000250
epoch:29; metric:emoval; train:0.5478; eval:0.5972; lr:0.000250
epoch:30; metric:emoval; train:0.5243; eval:0.6025; lr:0.000250
epoch:31; metric:emoval; train:0.5372; eval:0.5785; lr:0.000250
epoch:32; metric:emoval; train:0.5503; eval:0.5685; lr:0.000250
epoch:33; metric:emoval; train:0.5103; eval:0.5984; lr:0.000250
epoch:34; metric:emoval; train:0.5263; eval:0.5668; lr:0.000250
epoch:35; metric:emoval; train:0.5400; eval:0.5881; lr:0.000250
epoch:36; metric:emoval; train:0.5201; eval:0.5718; lr:0.000250
epoch:37; metric:emoval; train:0.5301; eval:0.5764; lr:0.000250
epoch:38; metric:emoval; train:0.5200; eval:0.5586; lr:0.000250
epoch:39; metric:emoval; train:0.5545; eval:0.5953; lr:0.000125
epoch:40; metric:emoval; train:0.5447; eval:0.5946; lr:0.000125
epoch:41; metric:emoval; train:0.5883; eval:0.6120; lr:0.000125
epoch:42; metric:emoval; train:0.5905; eval:0.5937; lr:0.000125
epoch:43; metric:emoval; train:0.5613; eval:0.5878; lr:0.000125
epoch:44; metric:emoval; train:0.6024; eval:0.5894; lr:0.000125
epoch:45; metric:emoval; train:0.5847; eval:0.5823; lr:0.000125
epoch:46; metric:emoval; train:0.5954; eval:0.5898; lr:0.000125
epoch:47; metric:emoval; train:0.5925; eval:0.5774; lr:0.000125
epoch:48; metric:emoval; train:0.6082; eval:0.5993; lr:0.000125
epoch:49; metric:emoval; train:0.6222; eval:0.5768; lr:0.000125
epoch:50; metric:emoval; train:0.6113; eval:0.5454; lr:0.000125
epoch:51; metric:emoval; train:0.5972; eval:0.5847; lr:0.000125
epoch:52; metric:emoval; train:0.5907; eval:0.5754; lr:0.000063
epoch:53; metric:emoval; train:0.6248; eval:0.5843; lr:0.000063
epoch:54; metric:emoval; train:0.6265; eval:0.5854; lr:0.000063
epoch:55; metric:emoval; train:0.6359; eval:0.5765; lr:0.000063
epoch:56; metric:emoval; train:0.6386; eval:0.5616; lr:0.000063
epoch:57; metric:emoval; train:0.6534; eval:0.5928; lr:0.000063
epoch:58; metric:emoval; train:0.6310; eval:0.5590; lr:0.000063
epoch:59; metric:emoval; train:0.6403; eval:0.5985; lr:0.000063
epoch:60; metric:emoval; train:0.6560; eval:0.5822; lr:0.000063
epoch:61; metric:emoval; train:0.6500; eval:0.5586; lr:0.000063
epoch:62; metric:emoval; train:0.6298; eval:0.5684; lr:0.000063
epoch:63; metric:emoval; train:0.6356; eval:0.5704; lr:0.000031
epoch:64; metric:emoval; train:0.6490; eval:0.5818; lr:0.000031
epoch:65; metric:emoval; train:0.6351; eval:0.5830; lr:0.000031
epoch:66; metric:emoval; train:0.6389; eval:0.5803; lr:0.000031
epoch:67; metric:emoval; train:0.6529; eval:0.5808; lr:0.000031
epoch:68; metric:emoval; train:0.6581; eval:0.5816; lr:0.000031
epoch:69; metric:emoval; train:0.6662; eval:0.5858; lr:0.000031
epoch:70; metric:emoval; train:0.6639; eval:0.5791; lr:0.000031
epoch:71; metric:emoval; train:0.6772; eval:0.5687; lr:0.000031
Early stopping at epoch 71, best epoch: 41
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 40, duration: 302.11988139152527 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3560; eval:-0.0511; lr:0.000500
epoch:2; metric:emoval; train:-0.0046; eval:0.0232; lr:0.000500
epoch:3; metric:emoval; train:0.1927; eval:0.3319; lr:0.000500
epoch:4; metric:emoval; train:0.3248; eval:0.2604; lr:0.000500
epoch:5; metric:emoval; train:0.3757; eval:0.3904; lr:0.000500
epoch:6; metric:emoval; train:0.4927; eval:0.4991; lr:0.000500
epoch:7; metric:emoval; train:0.5122; eval:0.5500; lr:0.000500
epoch:8; metric:emoval; train:0.5469; eval:0.5494; lr:0.000500
epoch:9; metric:emoval; train:0.5472; eval:0.4730; lr:0.000500
epoch:10; metric:emoval; train:0.5566; eval:0.5404; lr:0.000500
epoch:11; metric:emoval; train:0.5248; eval:0.4458; lr:0.000500
epoch:12; metric:emoval; train:0.5959; eval:0.4980; lr:0.000500
epoch:13; metric:emoval; train:0.5691; eval:0.5052; lr:0.000500
epoch:14; metric:emoval; train:0.5623; eval:0.5193; lr:0.000500
epoch:15; metric:emoval; train:0.5663; eval:0.5365; lr:0.000500
epoch:16; metric:emoval; train:0.5738; eval:0.5291; lr:0.000500
epoch:17; metric:emoval; train:0.5700; eval:0.5038; lr:0.000500
epoch:18; metric:emoval; train:0.5530; eval:0.5461; lr:0.000250
epoch:19; metric:emoval; train:0.5837; eval:0.5751; lr:0.000250
epoch:20; metric:emoval; train:0.5899; eval:0.5661; lr:0.000250
epoch:21; metric:emoval; train:0.5789; eval:0.5569; lr:0.000250
epoch:22; metric:emoval; train:0.5708; eval:0.6059; lr:0.000250
epoch:23; metric:emoval; train:0.6061; eval:0.5749; lr:0.000250
epoch:24; metric:emoval; train:0.5624; eval:0.5252; lr:0.000250
epoch:25; metric:emoval; train:0.5216; eval:0.5576; lr:0.000250
epoch:26; metric:emoval; train:0.5493; eval:0.5107; lr:0.000250
epoch:27; metric:emoval; train:0.5605; eval:0.5630; lr:0.000250
epoch:28; metric:emoval; train:0.5215; eval:0.5550; lr:0.000250
epoch:29; metric:emoval; train:0.5322; eval:0.5692; lr:0.000250
epoch:30; metric:emoval; train:0.5167; eval:0.5402; lr:0.000250
epoch:31; metric:emoval; train:0.5363; eval:0.5748; lr:0.000250
epoch:32; metric:emoval; train:0.5253; eval:0.5850; lr:0.000250
epoch:33; metric:emoval; train:0.5244; eval:0.5026; lr:0.000125
epoch:34; metric:emoval; train:0.5602; eval:0.5711; lr:0.000125
epoch:35; metric:emoval; train:0.5420; eval:0.5784; lr:0.000125
epoch:36; metric:emoval; train:0.5804; eval:0.5647; lr:0.000125
epoch:37; metric:emoval; train:0.5726; eval:0.5456; lr:0.000125
epoch:38; metric:emoval; train:0.5815; eval:0.5776; lr:0.000125
