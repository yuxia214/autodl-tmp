# AttentionRobust V2 æ·±åº¦æ”¹é€ æ–¹æ¡ˆ - åŸºäºP-RMFçš„æ¦‚ç‡åŒ–å¤šæ¨¡æ€èåˆ

## ğŸ“Œ ç›®æ ‡
å°† P-RMF (Proxy-Driven Robust Multimodal Framework) çš„æ ¸å¿ƒæŠ€æœ¯èå…¥ `attention_robust.py`ï¼Œé€šè¿‡**ä»ç¡®å®šæ€§ç‰¹å¾å­¦ä¹ è½¬å‘æ¦‚ç‡åˆ†å¸ƒå­¦ä¹ **ï¼Œæ˜¾è‘—æå‡ test2ï¼ˆæ¨¡æ€ç¼ºå¤±æµ‹è¯•ï¼‰çš„å‡†ç¡®ç‡ã€‚

---

## ğŸ§  æ ¸å¿ƒç†è®ºè½¬å˜

### å…³é”®æ´å¯Ÿï¼šä¸ºä»€ä¹ˆæ¦‚ç‡åˆ†å¸ƒæ¯”ç‚¹ä¼°è®¡æ›´å¥½ï¼Ÿ

| åœºæ™¯ | ç‚¹ä¼°è®¡ (ç°æœ‰æ–¹æ³•) | åˆ†å¸ƒä¼°è®¡ (P-RMFæ–¹æ³•) |
|------|-------------------|---------------------|
| æ¨¡æ€å®Œæ•´ | $h = f(x)$ âœ“ | $\mu = f(x), \sigma \approx 0$ âœ“ |
| æ¨¡æ€ç¼ºå¤± | $h = f(0) \approx 0$ âœ— å‰§çƒˆè·³å˜ | $\sigma \rightarrow \infty$ â†’ æƒé‡è‡ªåŠ¨é™ä½ âœ“ |
| æ¨¡æ€å™ªå£° | æ— æ³•åŒºåˆ† | $\sigma$ å¢å¤§ â†’ å¯è¯†åˆ« âœ“ |

**ç‰©ç†æ„ä¹‰**ï¼š
- **å‡å€¼ $\mu$**ï¼šæ¨¡æ€çš„ç¨³å®šè¯­ä¹‰ä¿¡æ¯
- **æ–¹å·® $\sigma^2$**ï¼šæ¨¡æ€çš„ä¸ç¡®å®šæ€§/å¯é æ€§åº¦é‡

---

## ğŸ—ï¸ å®Œæ•´æ¨¡å‹æ¶æ„è®¾è®¡

```
AttentionRobustV2 - æ¦‚ç‡åŒ–å¤šæ¨¡æ€èåˆæ¶æ„
==========================================

è¾“å…¥å±‚
------
audio [B, D_a]    text [B, D_t]    video [B, D_v]
      â†“                â†“                â†“

å˜åˆ†ç¼–ç å±‚ (æ–°å¢)
----------------
VariationalEncoder   VariationalEncoder   VariationalEncoder
      â†“                    â†“                    â†“
(z_a, Î¼_a, Ïƒ_a)      (z_t, Î¼_t, Ïƒ_t)      (z_v, Î¼_v, Ïƒ_v)
      â†“                    â†“                    â†“

é‡å»ºå±‚ (æ–°å¢) - ä»…è®­ç»ƒæ—¶ä½¿ç”¨
---------------------------
Decoder_a            Decoder_t            Decoder_v
      â†“                    â†“                    â†“
recon_a              recon_t              recon_v
      â†“                    â†“                    â†“
         â””â”€â”€â”€â”€â”€â”€ Reconstruction Loss â”€â”€â”€â”€â”€â”€â”˜

ä¸ç¡®å®šæ€§åŠ æƒèåˆå±‚ (æ ¸å¿ƒåˆ›æ–°)
---------------------------
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Weights    â”‚
                    â”‚ w = 1/(Ïƒ+Îµ)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
        Î¼_a â”€â”€â”€â”€â”€â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        Î¼_t â”€â”€â”€â”€â”€â”€â†’ â”‚ Weighted    â”‚ â”€â”€â†’ proxy [B, H]
        Î¼_v â”€â”€â”€â”€â”€â”€â†’ â”‚   Sum       â”‚     (ä»£ç†æ¨¡æ€)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä»£ç†æ¨¡æ€è·¨æ¨¡æ€æ³¨æ„åŠ›å±‚ (æ ¸å¿ƒåˆ›æ–°)
--------------------------------
              proxy
                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“           â†“           â†“
CrossAttn    CrossAttn    CrossAttn
(proxy,Î¼_a)  (proxy,Î¼_t)  (proxy,Î¼_v)
    â†“           â†“           â†“
  * w_a       * w_t       * w_v
    â†“           â†“           â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
           fused_feat

è¾“å‡ºå±‚
------
FC1 â†’ FC2 â†’ emos_out, vals_out

è¾…åŠ©æŸå¤± (é€šè¿‡interlossæ¥å£)
---------------------------
interloss = Î± * L_KL + Î² * L_recon + Î³ * L_cross_KL
```

---

## ğŸ“‹ å››é˜¶æ®µå®æ–½è®¡åˆ’

### ğŸ”§ é˜¶æ®µä¸€ï¼šå˜åˆ†ç¼–ç å™¨æ”¹é€ 

#### 1.1 æ–°å»ºæ–‡ä»¶: `toolkit/models/modules/variational_encoder.py`

```python
"""
å˜åˆ†ç¼–ç å™¨æ¨¡å— - æ¦‚ç‡åŒ–ç‰¹å¾è¡¨ç¤ºçš„æ ¸å¿ƒ
å°†ç¡®å®šæ€§ç¼–ç è½¬å˜ä¸ºé«˜æ–¯åˆ†å¸ƒå‚æ•°ä¼°è®¡
"""
import torch
import torch.nn as nn
import torch.nn.functional as F


class VariationalMLPEncoder(nn.Module):
    """
    å˜åˆ†MLPç¼–ç å™¨
    
    è¾“å…¥: åŸå§‹ç‰¹å¾ x [B, in_dim]
    è¾“å‡º: 
        - z: é‡‡æ ·çš„æ½œåœ¨å˜é‡ [B, hidden_dim] (ç”¨äºåç»­å¤„ç†)
        - mu: å‡å€¼ [B, hidden_dim] (ç¨³å®šè¯­ä¹‰)
        - logvar: å¯¹æ•°æ–¹å·® [B, hidden_dim] (ä¸ç¡®å®šæ€§)
        - std: æ ‡å‡†å·® [B, hidden_dim]
    """
    def __init__(self, in_size, hidden_size, dropout):
        super().__init__()
        self.drop = nn.Dropout(p=dropout)
        
        # å…±äº«ç‰¹å¾æå–å±‚ (ä¿æŒä¸åŸMLPEncoderç›¸ä¼¼çš„ç»“æ„)
        self.shared = nn.Sequential(
            nn.Linear(in_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
        )
        
        # åˆ†æ”¯1: å‡å€¼é¢„æµ‹
        self.mu_layer = nn.Linear(hidden_size, hidden_size)
        
        # åˆ†æ”¯2: å¯¹æ•°æ–¹å·®é¢„æµ‹
        self.logvar_layer = nn.Linear(hidden_size, hidden_size)
        
        # åˆå§‹åŒ–logvarå±‚ä½¿åˆå§‹æ–¹å·®æ¥è¿‘1
        nn.init.zeros_(self.logvar_layer.weight)
        nn.init.zeros_(self.logvar_layer.bias)
        
    def reparameterize(self, mu, logvar):
        """é‡å‚æ•°åŒ–æŠ€å·§: z = Î¼ + Îµ Ã— Ïƒ"""
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu  # æ¨ç†æ—¶ä½¿ç”¨å‡å€¼
    
    def forward(self, x):
        x = self.drop(x)
        h = self.shared(x)
        
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        
        # æ•°å€¼ç¨³å®šæ€§ï¼šé™åˆ¶logvarèŒƒå›´
        logvar = torch.clamp(logvar, min=-10, max=10)
        std = torch.exp(0.5 * logvar)
        
        z = self.reparameterize(mu, logvar)
        
        return z, mu, logvar, std


class ModalityDecoder(nn.Module):
    """
    æ¨¡æ€è§£ç å™¨ - ä»æ½œåœ¨å˜é‡é‡å»ºåŸå§‹ç‰¹å¾
    
    ä½œç”¨: å¼ºåˆ¶ç¼–ç å™¨å³ä½¿åœ¨è¾“å…¥æ®‹ç¼ºæ—¶ä¹Ÿè¦ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
    """
    def __init__(self, hidden_size, out_size, dropout):
        super().__init__()
        self.decoder = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(p=dropout),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, out_size)
        )
    
    def forward(self, z):
        return self.decoder(z)
```

#### 1.2 ä¿®æ”¹ `attention_robust.py` - æ›¿æ¢ç¼–ç å™¨

**ä¿®æ”¹å‰**:
```python
if args.feat_type in ['utt']:
    self.audio_encoder = MLPEncoder(audio_dim, hidden_dim, dropout)
    self.text_encoder  = MLPEncoder(text_dim,  hidden_dim, dropout)
    self.video_encoder = MLPEncoder(video_dim, hidden_dim, dropout)
```

**ä¿®æ”¹å**:
```python
from .modules.variational_encoder import VariationalMLPEncoder, ModalityDecoder

# æ–°å¢å‚æ•°
self.use_vae = getattr(args, 'use_vae', True)
self.kl_weight = getattr(args, 'kl_weight', 0.01)
self.recon_weight = getattr(args, 'recon_weight', 0.1)

if args.feat_type in ['utt']:
    if self.use_vae:
        self.audio_encoder = VariationalMLPEncoder(audio_dim, hidden_dim, dropout)
        self.text_encoder  = VariationalMLPEncoder(text_dim,  hidden_dim, dropout)
        self.video_encoder = VariationalMLPEncoder(video_dim, hidden_dim, dropout)
        
        # è§£ç å™¨ç”¨äºé‡å»ºæŸå¤±
        self.audio_decoder = ModalityDecoder(hidden_dim, audio_dim, dropout)
        self.text_decoder  = ModalityDecoder(hidden_dim, text_dim, dropout)
        self.video_decoder = ModalityDecoder(hidden_dim, video_dim, dropout)
    else:
        self.audio_encoder = MLPEncoder(audio_dim, hidden_dim, dropout)
        self.text_encoder  = MLPEncoder(text_dim,  hidden_dim, dropout)
        self.video_encoder = MLPEncoder(video_dim, hidden_dim, dropout)
```

---

### ğŸ¯ é˜¶æ®µäºŒï¼šä¸ç¡®å®šæ€§åŠ æƒèåˆ

#### 2.1 æ–°å¢èåˆæ¨¡å—

```python
class UncertaintyWeightedFusion(nn.Module):
    """
    åŸºäºä¸ç¡®å®šæ€§çš„åŠ¨æ€åŠ æƒèåˆ
    
    æ ¸å¿ƒå…¬å¼: w_m = softmax(1/Ïƒ_m)
    ç‰©ç†æ„ä¹‰: ä¸ç¡®å®šæ€§(æ–¹å·®)è¶Šå¤§çš„æ¨¡æ€ï¼Œèåˆæƒé‡è¶Šä½
    """
    def __init__(self, hidden_dim, temperature=1.0):
        super().__init__()
        self.temperature = temperature
        self.hidden_dim = hidden_dim
        
    def forward(self, mu_list, std_list):
        """
        Args:
            mu_list: [mu_audio, mu_text, mu_video]
            std_list: [std_audio, std_text, std_video]
        Returns:
            proxy: ä»£ç†æ¨¡æ€ [B, H]
            weights: å„æ¨¡æ€æƒé‡ [B, 3]
        """
        # è®¡ç®—æ¯ä¸ªæ¨¡æ€çš„å¹³å‡ä¸ç¡®å®šæ€§
        uncertainties = []
        for std in std_list:
            uncertainty = std.mean(dim=-1, keepdim=True)  # [B, 1]
            uncertainties.append(uncertainty)
        uncertainties = torch.cat(uncertainties, dim=1)  # [B, 3]
        
        # åå‘æ–¹å·®åŠ æƒ
        inv_uncertainties = 1.0 / (uncertainties + 1e-6)
        weights = F.softmax(inv_uncertainties / self.temperature, dim=1)  # [B, 3]
        
        # åŠ æƒèåˆç”Ÿæˆä»£ç†æ¨¡æ€
        mu_stack = torch.stack(mu_list, dim=1)  # [B, 3, H]
        weights_exp = weights.unsqueeze(-1)  # [B, 3, 1]
        proxy = (mu_stack * weights_exp).sum(dim=1)  # [B, H]
        
        return proxy, weights
```

#### 2.2 ä»£ç†æ¨¡æ€è·¨æ¨¡æ€æ³¨æ„åŠ›

```python
class ProxyCrossModalAttention(nn.Module):
    """
    ä»£ç†æ¨¡æ€å¼•å¯¼çš„è·¨æ¨¡æ€æ³¨æ„åŠ›
    
    ä½¿ç”¨proxyä½œä¸ºç¨³å®šçš„Queryï¼Œå¯¹å„åŸå§‹æ¨¡æ€åšåŠ æƒattention
    è¿™æ ·å³ä½¿æŸä¸ªæ¨¡æ€ç¼ºå¤±ï¼Œproxyä»èƒ½ä»å…¶ä»–æ¨¡æ€è·å–ä¿¡æ¯
    """
    def __init__(self, hidden_dim, num_heads=4, dropout=0.1):
        super().__init__()
        
        self.cross_attn_audio = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True)
        self.cross_attn_text = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True)
        self.cross_attn_video = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True)
        
        self.norm = nn.LayerNorm(hidden_dim)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, proxy, mu_audio, mu_text, mu_video, weights):
        """
        Args:
            proxy: ä»£ç†æ¨¡æ€ [B, H]
            mu_*: å„æ¨¡æ€å‡å€¼ [B, H]
            weights: ä¸ç¡®å®šæ€§æƒé‡ [B, 3]
        """
        # æ‰©å±•ç»´åº¦ [B, 1, H]
        proxy_exp = proxy.unsqueeze(1)
        audio_exp = mu_audio.unsqueeze(1)
        text_exp = mu_text.unsqueeze(1)
        video_exp = mu_video.unsqueeze(1)
        
        # Cross attention
        attn_a, _ = self.cross_attn_audio(proxy_exp, audio_exp, audio_exp)
        attn_t, _ = self.cross_attn_text(proxy_exp, text_exp, text_exp)
        attn_v, _ = self.cross_attn_video(proxy_exp, video_exp, video_exp)
        
        # Squeeze [B, H]
        attn_a = attn_a.squeeze(1)
        attn_t = attn_t.squeeze(1)
        attn_v = attn_v.squeeze(1)
        
        # ä¸ç¡®å®šæ€§åŠ æƒèåˆ
        weighted = (weights[:, 0:1] * attn_a + 
                    weights[:, 1:2] * attn_t + 
                    weights[:, 2:3] * attn_v)
        
        # æ®‹å·® + FFN
        fused = self.norm(proxy + weighted)
        fused = fused + self.ffn(fused)
        
        return fused
```

---

### ğŸ“Š é˜¶æ®µä¸‰ï¼šæŸå¤±å‡½æ•°è®¾è®¡

#### 3.1 VAEæŸå¤±è®¡ç®—å™¨

```python
class VAELossComputer:
    """
    è®¡ç®—VAEç›¸å…³çš„è¾…åŠ©æŸå¤±ï¼Œèµ‹å€¼ç»™interloss
    
    æ€»æŸå¤± = Î± * L_KL + Î² * L_recon + Î³ * L_cross_KL
    """
    def __init__(self, kl_weight=0.01, recon_weight=0.1, cross_kl_weight=0.01):
        self.kl_weight = kl_weight
        self.recon_weight = recon_weight
        self.cross_kl_weight = cross_kl_weight
        self.mse = nn.MSELoss()
    
    def kl_divergence_to_standard_normal(self, mu, logvar):
        """
        KL(q(z|x) || N(0,I))
        = -0.5 * Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
        
        ä½œç”¨: æ­£åˆ™åŒ–ï¼Œé˜²æ­¢æ½œåœ¨ç©ºé—´è¿‡æ‹Ÿåˆ
        """
        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)
        return kl.mean()
    
    def reconstruction_loss(self, original, reconstructed):
        """
        é‡å»ºæŸå¤± = MSE(x, x_recon)
        
        ä½œç”¨: å¼ºåˆ¶ç¼–ç å™¨ä¿æŒè¯­ä¹‰ä¿¡æ¯
        """
        return self.mse(reconstructed, original)
    
    def cross_modal_kl(self, mu_list, logvar_list):
        """
        è·¨æ¨¡æ€KLæ•£åº¦
        KL(q_a || q_t) + KL(q_a || q_v) + KL(q_t || q_v)
        
        ä½œç”¨: é¼“åŠ±å„æ¨¡æ€å­¦ä¹ ç›¸ä¼¼çš„æ½œåœ¨ç©ºé—´ï¼Œä¾¿äºè·¨æ¨¡æ€è¡¥å……
        """
        def kl_gaussian(mu1, lv1, mu2, lv2):
            var1, var2 = torch.exp(lv1), torch.exp(lv2)
            return 0.5 * (lv2 - lv1 + var1/var2 + (mu1-mu2).pow(2)/var2 - 1).mean()
        
        mu_a, mu_t, mu_v = mu_list
        lv_a, lv_t, lv_v = logvar_list
        
        return (kl_gaussian(mu_a, lv_a, mu_t, lv_t) +
                kl_gaussian(mu_a, lv_a, mu_v, lv_v) +
                kl_gaussian(mu_t, lv_t, mu_v, lv_v)) / 3
    
    def compute(self, mu_list, logvar_list, originals, reconstructions):
        """è®¡ç®—æ€»çš„è¾…åŠ©æŸå¤±"""
        # 1. KLæ•£åº¦æŸå¤± (æ¯ä¸ªæ¨¡æ€å¯¹æ ‡å‡†æ­£æ€)
        kl_loss = sum(self.kl_divergence_to_standard_normal(mu, lv) 
                      for mu, lv in zip(mu_list, logvar_list)) / 3
        
        # 2. é‡å»ºæŸå¤±
        recon_loss = sum(self.reconstruction_loss(orig, recon) 
                         for orig, recon in zip(originals, reconstructions)) / 3
        
        # 3. è·¨æ¨¡æ€KLæŸå¤±
        cross_kl_loss = self.cross_modal_kl(mu_list, logvar_list)
        
        total = (self.kl_weight * kl_loss + 
                 self.recon_weight * recon_loss + 
                 self.cross_kl_weight * cross_kl_loss)
        
        return total
```

---

### ğŸ”„ é˜¶æ®µå››ï¼šå®Œæ•´æ¨¡å‹ä»£ç 

#### 4.1 `attention_robust_v2.py` å®Œæ•´å®ç°

```python
'''
AttentionRobustV2 - åŸºäºP-RMFçš„æ¦‚ç‡åŒ–å¤šæ¨¡æ€èåˆæ¨¡å‹
æ ¸å¿ƒæ”¹è¿›ï¼šä»ç¡®å®šæ€§ç‰¹å¾å­¦ä¹ è½¬å‘æ¦‚ç‡åˆ†å¸ƒå­¦ä¹ 
'''
import torch
import torch.nn as nn
import torch.nn.functional as F
from .modules.encoder import MLPEncoder, LSTMEncoder
from .modules.variational_encoder import VariationalMLPEncoder, ModalityDecoder


class UncertaintyWeightedFusion(nn.Module):
    """ä¸ç¡®å®šæ€§åŠ æƒèåˆ - ç”Ÿæˆä»£ç†æ¨¡æ€"""
    # ... (ä»£ç è§ä¸Šæ–‡)


class ProxyCrossModalAttention(nn.Module):
    """ä»£ç†æ¨¡æ€è·¨æ¨¡æ€æ³¨æ„åŠ›"""
    # ... (ä»£ç è§ä¸Šæ–‡)


class VAELossComputer:
    """VAEæŸå¤±è®¡ç®—å™¨"""
    # ... (ä»£ç è§ä¸Šæ–‡)


class AttentionRobustV2(nn.Module):
    def __init__(self, args):
        super().__init__()
        
        # åŸºç¡€å‚æ•°
        text_dim = args.text_dim
        audio_dim = args.audio_dim
        video_dim = args.video_dim
        output_dim1 = args.output_dim1
        output_dim2 = args.output_dim2
        dropout = args.dropout
        hidden_dim = args.hidden_dim
        self.grad_clip = args.grad_clip
        self.hidden_dim = hidden_dim
        
        # VAEå‚æ•°
        self.use_vae = getattr(args, 'use_vae', True)
        self.kl_weight = getattr(args, 'kl_weight', 0.01)
        self.recon_weight = getattr(args, 'recon_weight', 0.1)
        self.cross_kl_weight = getattr(args, 'cross_kl_weight', 0.01)
        
        # ä»£ç†æ¨¡æ€å‚æ•°
        self.use_proxy_attention = getattr(args, 'use_proxy_attention', True)
        self.fusion_temperature = getattr(args, 'fusion_temperature', 1.0)
        
        # æ¨¡æ€dropoutå‚æ•° (ä¿ç•™åŸæœ‰åŠŸèƒ½)
        self.modality_dropout = getattr(args, 'modality_dropout', 0.2)
        self.use_modality_dropout = getattr(args, 'use_modality_dropout', True)
        self.warmup_epochs = getattr(args, 'modality_dropout_warmup', 0)
        self.current_epoch = 0
        
        # ========== ç¼–ç å™¨ ==========
        if args.feat_type in ['utt']:
            if self.use_vae:
                self.audio_encoder = VariationalMLPEncoder(audio_dim, hidden_dim, dropout)
                self.text_encoder = VariationalMLPEncoder(text_dim, hidden_dim, dropout)
                self.video_encoder = VariationalMLPEncoder(video_dim, hidden_dim, dropout)
                
                # è§£ç å™¨
                self.audio_decoder = ModalityDecoder(hidden_dim, audio_dim, dropout)
                self.text_decoder = ModalityDecoder(hidden_dim, text_dim, dropout)
                self.video_decoder = ModalityDecoder(hidden_dim, video_dim, dropout)
            else:
                self.audio_encoder = MLPEncoder(audio_dim, hidden_dim, dropout)
                self.text_encoder = MLPEncoder(text_dim, hidden_dim, dropout)
                self.video_encoder = MLPEncoder(video_dim, hidden_dim, dropout)
        
        # ========== èåˆæ¨¡å— ==========
        if self.use_vae:
            self.uncertainty_fusion = UncertaintyWeightedFusion(
                hidden_dim, self.fusion_temperature)
            
            if self.use_proxy_attention:
                self.proxy_attention = ProxyCrossModalAttention(
                    hidden_dim, num_heads=4, dropout=dropout)
            
            self.loss_computer = VAELossComputer(
                self.kl_weight, self.recon_weight, self.cross_kl_weight)
        else:
            # ä¿ç•™åŸæœ‰çš„attentionèåˆ
            self.attention_mlp = MLPEncoder(hidden_dim * 3, hidden_dim, dropout)
            self.fc_att = nn.Linear(hidden_dim, 3)
        
        # ========== è¾“å‡ºå±‚ ==========
        self.feat_dropout = nn.Dropout(p=dropout)
        self.fc_out_1 = nn.Linear(hidden_dim, output_dim1)
        self.fc_out_2 = nn.Linear(hidden_dim, output_dim2)
    
    def set_epoch(self, epoch):
        self.current_epoch = epoch
    
    def apply_modality_dropout(self, z_audio, z_text, z_video):
        """æ¨¡æ€dropout - æ”¯æŒVAEæ¨¡å¼"""
        if not self.training or not self.use_modality_dropout:
            return z_audio, z_text, z_video
        
        if self.current_epoch < self.warmup_epochs:
            return z_audio, z_text, z_video
        
        # è®¡ç®—æœ‰æ•ˆdropoutç‡
        if self.warmup_epochs > 0:
            progress = min(1.0, (self.current_epoch - self.warmup_epochs) / self.warmup_epochs)
            effective_dropout = self.modality_dropout * progress
        else:
            effective_dropout = self.modality_dropout
        
        batch_size = z_audio.size(0)
        device = z_audio.device
        
        masks = torch.ones(batch_size, 3, device=device)
        
        for i in range(batch_size):
            if torch.rand(1).item() < effective_dropout:
                drop_mode = torch.randint(0, 6, (1,)).item()
                if drop_mode == 0:
                    masks[i, 0] = 0
                elif drop_mode == 1:
                    masks[i, 1] = 0
                elif drop_mode == 2:
                    masks[i, 2] = 0
                elif drop_mode == 3:
                    masks[i, 0] = 0
                    masks[i, 1] = 0
                elif drop_mode == 4:
                    masks[i, 0] = 0
                    masks[i, 2] = 0
                elif drop_mode == 5:
                    masks[i, 1] = 0
                    masks[i, 2] = 0
        
        z_audio = z_audio * masks[:, 0:1]
        z_text = z_text * masks[:, 1:2]
        z_video = z_video * masks[:, 2:3]
        
        return z_audio, z_text, z_video
    
    def forward(self, batch):
        if self.use_vae:
            return self.forward_vae(batch)
        else:
            return self.forward_original(batch)
    
    def forward_vae(self, batch):
        """VAEæ¨¡å¼çš„å‰å‘ä¼ æ’­"""
        audios = batch['audios']
        texts = batch['texts']
        videos = batch['videos']
        
        # 1. å˜åˆ†ç¼–ç 
        z_a, mu_a, logvar_a, std_a = self.audio_encoder(audios)
        z_t, mu_t, logvar_t, std_t = self.text_encoder(texts)
        z_v, mu_v, logvar_v, std_v = self.video_encoder(videos)
        
        # 2. æ¨¡æ€dropout (å¯é€‰)
        z_a, z_t, z_v = self.apply_modality_dropout(z_a, z_t, z_v)
        
        # 3. ä¸ç¡®å®šæ€§åŠ æƒèåˆ â†’ ç”Ÿæˆä»£ç†æ¨¡æ€
        proxy, weights = self.uncertainty_fusion(
            [mu_a, mu_t, mu_v], 
            [std_a, std_t, std_v]
        )
        
        # 4. ä»£ç†æ¨¡æ€è·¨æ¨¡æ€æ³¨æ„åŠ›
        if self.use_proxy_attention:
            fused = self.proxy_attention(proxy, mu_a, mu_t, mu_v, weights)
        else:
            fused = proxy
        
        # 5. è¾“å‡º
        features = self.feat_dropout(fused)
        emos_out = self.fc_out_1(features)
        vals_out = self.fc_out_2(features)
        
        # 6. è®¡ç®—è¾…åŠ©æŸå¤± (interloss)
        if self.training:
            # é‡å»º
            recon_a = self.audio_decoder(z_a)
            recon_t = self.text_decoder(z_t)
            recon_v = self.video_decoder(z_v)
            
            interloss = self.loss_computer.compute(
                [mu_a, mu_t, mu_v],
                [logvar_a, logvar_t, logvar_v],
                [audios, texts, videos],
                [recon_a, recon_t, recon_v]
            )
        else:
            interloss = torch.tensor(0.0, device=audios.device)
        
        return features, emos_out, vals_out, interloss
    
    def forward_original(self, batch):
        """åŸå§‹æ¨¡å¼ (å…¼å®¹)"""
        audio_hidden = self.audio_encoder(batch['audios'])
        text_hidden = self.text_encoder(batch['texts'])
        video_hidden = self.video_encoder(batch['videos'])
        
        audio_hidden, text_hidden, video_hidden = self.apply_modality_dropout(
            audio_hidden, text_hidden, video_hidden)
        
        multi_hidden1 = torch.cat([audio_hidden, text_hidden, video_hidden], dim=1)
        attention = self.attention_mlp(multi_hidden1)
        attention = self.fc_att(attention)
        attention = F.softmax(attention, dim=1)
        attention = attention.unsqueeze(2)
        
        multi_hidden2 = torch.stack([audio_hidden, text_hidden, video_hidden], dim=2)
        fused_feat = torch.matmul(multi_hidden2, attention)
        
        features = fused_feat.squeeze(2)
        features = self.feat_dropout(features)
        
        emos_out = self.fc_out_1(features)
        vals_out = self.fc_out_2(features)
        interloss = torch.tensor(0.0).cuda()
        
        return features, emos_out, vals_out, interloss
```

---

## âš™ï¸ è¶…å‚æ•°é…ç½®

### æ¨èé…ç½® (model-tune.yaml)

```yaml
attention_robust_v2:
  # åŸºç¡€å‚æ•°
  hidden_dim: 128
  dropout: 0.35
  grad_clip: 1.0
  
  # VAEå‚æ•°
  use_vae: true
  kl_weight: 0.01          # KLæ•£åº¦æƒé‡
  recon_weight: 0.1        # é‡å»ºæŸå¤±æƒé‡
  cross_kl_weight: 0.01    # è·¨æ¨¡æ€KLæƒé‡
  
  # ä»£ç†æ¨¡æ€å‚æ•°
  use_proxy_attention: true
  fusion_temperature: 1.0  # æ¸©åº¦å‚æ•°ï¼Œè¶Šå¤§æƒé‡è¶Šå‡åŒ€
  
  # æ¨¡æ€dropout
  modality_dropout: 0.15
  use_modality_dropout: true
  modality_dropout_warmup: 20
  
  # è®­ç»ƒå‚æ•°
  lr: 5e-4
  l2: 5e-5
  epochs: 100
  early_stopping_patience: 30
  batch_size: 32
```

### æŸå¤±æƒé‡è°ƒä¼˜æŒ‡å—

| å‚æ•° | èŒƒå›´ | ä½œç”¨ | è°ƒä¼˜å»ºè®® |
|------|------|------|----------|
| `kl_weight` | 0.001~0.1 | æ­£åˆ™åŒ–å¼ºåº¦ | ä»0.01å¼€å§‹ï¼Œè¿‡å¤§ä¼šå¯¼è‡´æ¨¡æ€å¡Œç¼© |
| `recon_weight` | 0.05~0.3 | è¯­ä¹‰ä¿æŒ | 0.1æ˜¯è¾ƒå¥½çš„èµ·ç‚¹ |
| `cross_kl_weight` | 0.005~0.05 | è·¨æ¨¡æ€å¯¹é½ | 0.01ï¼Œè¿‡å¤§å„æ¨¡æ€æ— åŒºåˆ† |
| `fusion_temperature` | 0.5~2.0 | æƒé‡åˆ†å¸ƒ | 1.0ï¼Œè¶Šå°æƒé‡è¶Šæç«¯ |

---

## ğŸ§ª å®éªŒè®¡åˆ’

### é˜¶æ®µ1: åŸºç¡€éªŒè¯ (2-3å¤©)
```bash
# ä»…VAEç¼–ç ï¼Œä¸åŠ ä»£ç†æ³¨æ„åŠ›
python main-robust.py --model='attention_robust_v2' \
    --use_vae --use_proxy_attention=False \
    --kl_weight=0.01 --recon_weight=0.1
```

### é˜¶æ®µ2: å®Œæ•´æ¨¡å‹ (2-3å¤©)
```bash
# æ·»åŠ ä»£ç†æ¨¡æ€æ³¨æ„åŠ›
python main-robust.py --model='attention_robust_v2' \
    --use_vae --use_proxy_attention \
    --kl_weight=0.01 --recon_weight=0.1 --cross_kl_weight=0.01
```

### é˜¶æ®µ3: è¶…å‚è°ƒä¼˜ (3-5å¤©)
- Grid Search: `kl_weight âˆˆ {0.005, 0.01, 0.02}`
- Grid Search: `recon_weight âˆˆ {0.05, 0.1, 0.2}`
- Grid Search: `fusion_temperature âˆˆ {0.5, 1.0, 2.0}`

### é˜¶æ®µ4: æ¶ˆèå®éªŒ
| é…ç½® | VAE | Proxy Attn | Cross KL | é¢„æœŸtest2 |
|------|-----|------------|----------|-----------|
| Baseline | âœ— | âœ— | âœ— | 0.7476 |
| +VAE | âœ“ | âœ— | âœ— | ~0.76 |
| +VAE+Proxy | âœ“ | âœ“ | âœ— | ~0.77 |
| +All | âœ“ | âœ“ | âœ“ | ~0.78-0.79 |

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

| æŒ‡æ ‡ | Baseline | V3 (å½“å‰æœ€ä½³) | V2 (æœ¬æ–¹æ¡ˆ) | æå‡ |
|------|----------|--------------|-------------|------|
| **test2** | 0.7476 | 0.7621 | **0.78~0.79** | +2~3% |
| test1 | 0.7956 | 0.8248 | 0.83~0.84 | +1% |
| test3 | 0.8645 | 0.8873 | ~0.89 | Â±0.5% |

---

## ğŸ“ æ–‡ä»¶ä¿®æ”¹æ¸…å•

| æ–‡ä»¶ | æ“ä½œ | è¯´æ˜ |
|------|------|------|
| `toolkit/models/modules/variational_encoder.py` | **æ–°å»º** | VAEç¼–ç å™¨å’Œè§£ç å™¨ |
| `toolkit/models/attention_robust_v2.py` | **æ–°å»º** | V2å®Œæ•´æ¨¡å‹ |
| `toolkit/models/__init__.py` | ä¿®æ”¹ | æ³¨å†Œæ–°æ¨¡å‹ |
| `toolkit/model-tune.yaml` | ä¿®æ”¹ | æ·»åŠ V2è¶…å‚é…ç½® |
| `main-robust.py` | ä¿®æ”¹ | æ”¯æŒæ–°å‚æ•° |

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ•°å€¼ç¨³å®šæ€§**: `logvar` éœ€è¦clampé˜²æ­¢expçˆ†ç‚¸
2. **åˆå§‹åŒ–**: `logvar_layer` å»ºè®®é›¶åˆå§‹åŒ–ï¼Œä½¿åˆå§‹æ–¹å·®â‰ˆ1
3. **æ¸è¿›å¼è®­ç»ƒ**: å»ºè®®å…ˆè®­ç»ƒå‡ ä¸ªepochä¸åŠ KLæŸå¤±ï¼Œå†é€æ­¥åŠ å…¥
4. **å…¼å®¹æ€§**: é€šè¿‡`use_vae`å¼€å…³ä¿æŒå‘åå…¼å®¹

---

*æ–‡æ¡£æ›´æ–°æ—¥æœŸ: 2026å¹´1æœˆ30æ—¥*  
*åŸºäº: P-RMF (ACL 2025) + AIæ·±åº¦åˆ†æ*
