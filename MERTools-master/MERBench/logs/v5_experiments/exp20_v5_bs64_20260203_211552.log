====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=64, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v5', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s]  3%|▎         | 97/3373 [00:00<00:03, 966.87it/s]  6%|▌         | 194/3373 [00:00<00:03, 941.96it/s]  9%|▉         | 309/3373 [00:00<00:02, 1032.05it/s] 13%|█▎        | 428/3373 [00:00<00:02, 1093.58it/s] 16%|█▌        | 538/3373 [00:00<00:05, 535.79it/s]  18%|█▊        | 619/3373 [00:00<00:05, 492.49it/s] 21%|██        | 711/3373 [00:01<00:04, 574.59it/s] 23%|██▎       | 786/3373 [00:01<00:04, 608.17it/s] 26%|██▌       | 870/3373 [00:01<00:03, 661.36it/s] 29%|██▊       | 969/3373 [00:01<00:03, 602.03it/s] 31%|███       | 1039/3373 [00:01<00:04, 508.70it/s] 33%|███▎      | 1123/3373 [00:01<00:03, 575.82it/s] 36%|███▌      | 1208/3373 [00:01<00:03, 637.31it/s] 38%|███▊      | 1280/3373 [00:02<00:03, 523.77it/s] 40%|████      | 1355/3373 [00:02<00:03, 572.58it/s] 43%|████▎     | 1437/3373 [00:02<00:03, 625.65it/s] 45%|████▍     | 1507/3373 [00:02<00:03, 525.17it/s] 46%|████▋     | 1567/3373 [00:02<00:03, 541.75it/s] 50%|████▉     | 1670/3373 [00:02<00:02, 660.77it/s] 52%|█████▏    | 1743/3373 [00:02<00:03, 536.09it/s] 54%|█████▎    | 1809/3373 [00:03<00:02, 563.29it/s] 57%|█████▋    | 1937/3373 [00:03<00:01, 736.32it/s] 60%|█████▉    | 2019/3373 [00:03<00:01, 755.62it/s] 62%|██████▏   | 2101/3373 [00:03<00:01, 770.75it/s] 65%|██████▍   | 2183/3373 [00:03<00:01, 623.86it/s] 67%|██████▋   | 2253/3373 [00:03<00:01, 635.17it/s] 69%|██████▉   | 2337/3373 [00:03<00:01, 686.36it/s] 71%|███████▏  | 2411/3373 [00:03<00:01, 697.05it/s] 74%|███████▍  | 2495/3373 [00:03<00:01, 735.29it/s] 76%|███████▋  | 2575/3373 [00:04<00:01, 751.05it/s] 79%|███████▉  | 2659/3373 [00:04<00:00, 774.56it/s] 81%|████████  | 2738/3373 [00:04<00:01, 619.43it/s] 83%|████████▎ | 2806/3373 [00:04<00:00, 630.39it/s] 85%|████████▌ | 2874/3373 [00:04<00:00, 633.03it/s] 88%|████████▊ | 2957/3373 [00:04<00:00, 685.48it/s] 92%|█████████▏| 3115/3373 [00:04<00:00, 928.79it/s] 96%|█████████▌| 3240/3373 [00:04<00:00, 1019.22it/s]100%|█████████▉| 3357/3373 [00:04<00:00, 1062.00it/s]100%|██████████| 3373/3373 [00:04<00:00, 687.27it/s] 
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s]  2%|▏         | 81/3373 [00:00<00:04, 790.70it/s]  5%|▍         | 161/3373 [00:00<00:08, 376.71it/s]  6%|▌         | 210/3373 [00:00<00:09, 317.48it/s]  7%|▋         | 248/3373 [00:00<00:09, 325.63it/s]  9%|▊         | 290/3373 [00:00<00:10, 285.80it/s] 10%|▉         | 322/3373 [00:00<00:10, 291.86it/s] 11%|█         | 363/3373 [00:01<00:09, 319.94it/s] 12%|█▏        | 398/3373 [00:01<00:11, 261.29it/s] 14%|█▎        | 460/3373 [00:01<00:08, 341.91it/s] 15%|█▍        | 500/3373 [00:01<00:10, 286.82it/s] 16%|█▋        | 553/3373 [00:01<00:08, 339.40it/s] 19%|█▉        | 634/3373 [00:01<00:06, 450.87it/s] 20%|██        | 686/3373 [00:01<00:07, 374.29it/s] 22%|██▏       | 730/3373 [00:02<00:08, 313.92it/s] 23%|██▎       | 768/3373 [00:02<00:12, 204.29it/s] 25%|██▌       | 849/3373 [00:02<00:09, 258.47it/s] 27%|██▋       | 902/3373 [00:02<00:08, 301.87it/s] 28%|██▊       | 958/3373 [00:02<00:06, 349.15it/s] 30%|██▉       | 1002/3373 [00:03<00:07, 306.12it/s] 31%|███       | 1040/3373 [00:03<00:07, 316.73it/s] 32%|███▏      | 1083/3373 [00:03<00:06, 340.52it/s] 33%|███▎      | 1122/3373 [00:03<00:12, 181.92it/s] 34%|███▍      | 1152/3373 [00:04<00:11, 197.58it/s] 35%|███▌      | 1181/3373 [00:04<00:11, 182.76it/s] 36%|███▋      | 1225/3373 [00:04<00:09, 227.56it/s] 38%|███▊      | 1271/3373 [00:04<00:07, 273.36it/s] 40%|███▉      | 1341/3373 [00:04<00:06, 302.75it/s] 41%|████      | 1390/3373 [00:04<00:08, 242.36it/s] 43%|████▎     | 1434/3373 [00:04<00:07, 275.24it/s] 45%|████▍     | 1511/3373 [00:05<00:05, 371.92it/s] 46%|████▋     | 1561/3373 [00:05<00:05, 332.32it/s] 47%|████▋     | 1602/3373 [00:05<00:05, 345.27it/s] 49%|████▊     | 1642/3373 [00:05<00:05, 293.77it/s] 51%|█████     | 1708/3373 [00:05<00:04, 364.97it/s] 53%|█████▎    | 1776/3373 [00:05<00:03, 432.80it/s] 54%|█████▍    | 1826/3373 [00:06<00:05, 260.22it/s] 55%|█████▌    | 1868/3373 [00:06<00:06, 215.55it/s] 56%|█████▋    | 1899/3373 [00:06<00:07, 199.81it/s] 58%|█████▊    | 1950/3373 [00:06<00:06, 216.57it/s] 59%|█████▉    | 1990/3373 [00:06<00:05, 246.58it/s] 61%|██████    | 2048/3373 [00:07<00:04, 308.59it/s] 62%|██████▏   | 2089/3373 [00:07<00:03, 328.57it/s] 63%|██████▎   | 2128/3373 [00:07<00:04, 281.49it/s] 64%|██████▍   | 2162/3373 [00:07<00:04, 290.38it/s] 65%|██████▌   | 2195/3373 [00:07<00:03, 298.50it/s] 66%|██████▌   | 2228/3373 [00:07<00:04, 248.69it/s] 67%|██████▋   | 2257/3373 [00:07<00:05, 210.17it/s] 68%|██████▊   | 2281/3373 [00:08<00:05, 213.96it/s] 68%|██████▊   | 2305/3373 [00:08<00:04, 219.70it/s] 69%|██████▉   | 2329/3373 [00:08<00:04, 224.60it/s] 71%|███████   | 2392/3373 [00:08<00:03, 265.97it/s] 72%|███████▏  | 2419/3373 [00:08<00:04, 216.85it/s] 72%|███████▏  | 2445/3373 [00:08<00:04, 223.87it/s] 75%|███████▍  | 2523/3373 [00:08<00:02, 348.93it/s] 76%|███████▌  | 2562/3373 [00:09<00:02, 289.53it/s] 77%|███████▋  | 2596/3373 [00:09<00:02, 299.51it/s] 78%|███████▊  | 2630/3373 [00:09<00:02, 252.46it/s] 80%|███████▉  | 2698/3373 [00:09<00:01, 344.91it/s] 81%|████████  | 2739/3373 [00:09<00:02, 291.72it/s] 83%|████████▎ | 2802/3373 [00:09<00:01, 363.74it/s] 85%|████████▍ | 2863/3373 [00:09<00:01, 418.13it/s] 88%|████████▊ | 2971/3373 [00:10<00:00, 470.84it/s] 90%|████████▉ | 3021/3373 [00:10<00:01, 329.60it/s] 91%|█████████ | 3061/3373 [00:10<00:01, 287.55it/s] 92%|█████████▏| 3111/3373 [00:10<00:00, 279.37it/s] 94%|█████████▍| 3164/3373 [00:10<00:00, 324.29it/s] 95%|█████████▍| 3202/3373 [00:10<00:00, 334.60it/s] 96%|█████████▌| 3240/3373 [00:11<00:00, 341.80it/s] 97%|█████████▋| 3278/3373 [00:11<00:00, 348.05it/s] 98%|█████████▊| 3316/3373 [00:11<00:00, 287.62it/s]100%|██████████| 3373/3373 [00:11<00:00, 293.48it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s]  0%|          | 3/3373 [00:00<03:40, 15.25it/s]  1%|          | 22/3373 [00:00<00:53, 62.53it/s]  3%|▎         | 91/3373 [00:00<00:13, 247.40it/s]  4%|▍         | 147/3373 [00:00<00:12, 265.16it/s]  6%|▌         | 201/3373 [00:00<00:09, 331.81it/s] 10%|▉         | 327/3373 [00:00<00:05, 566.73it/s] 12%|█▏        | 394/3373 [00:01<00:06, 471.42it/s] 13%|█▎        | 451/3373 [00:01<00:05, 490.94it/s] 17%|█▋        | 568/3373 [00:01<00:04, 659.69it/s] 19%|█▉        | 643/3373 [00:01<00:05, 543.78it/s] 21%|██        | 707/3373 [00:01<00:04, 563.25it/s] 23%|██▎       | 771/3373 [00:01<00:05, 474.67it/s] 24%|██▍       | 826/3373 [00:01<00:05, 479.31it/s] 26%|██▌       | 879/3373 [00:02<00:06, 404.89it/s] 31%|███       | 1046/3373 [00:02<00:03, 674.22it/s] 33%|███▎      | 1126/3373 [00:02<00:03, 572.97it/s] 36%|███▌      | 1217/3373 [00:02<00:03, 645.47it/s] 38%|███▊      | 1292/3373 [00:02<00:03, 662.49it/s] 40%|████      | 1366/3373 [00:02<00:03, 544.65it/s] 42%|████▏     | 1429/3373 [00:02<00:03, 562.11it/s] 45%|████▌     | 1519/3373 [00:03<00:02, 642.97it/s] 49%|████▉     | 1657/3373 [00:03<00:02, 830.49it/s] 52%|█████▏    | 1748/3373 [00:03<00:02, 561.83it/s] 57%|█████▋    | 1939/3373 [00:03<00:01, 832.81it/s] 61%|██████    | 2048/3373 [00:03<00:01, 886.15it/s] 64%|██████▍   | 2156/3373 [00:03<00:01, 753.97it/s] 67%|██████▋   | 2248/3373 [00:04<00:01, 645.32it/s] 71%|███████   | 2388/3373 [00:04<00:01, 797.28it/s] 74%|███████▎  | 2485/3373 [00:04<00:01, 579.65it/s] 77%|███████▋  | 2607/3373 [00:04<00:01, 695.79it/s] 80%|███████▉  | 2698/3373 [00:04<00:01, 608.13it/s] 83%|████████▎ | 2812/3373 [00:04<00:00, 606.10it/s] 86%|████████▌ | 2909/3373 [00:04<00:00, 675.35it/s] 89%|████████▊ | 2989/3373 [00:05<00:00, 700.79it/s] 92%|█████████▏| 3114/3373 [00:05<00:00, 827.32it/s] 96%|█████████▌| 3245/3373 [00:05<00:00, 946.14it/s] 99%|█████████▉| 3352/3373 [00:05<00:00, 978.59it/s]100%|██████████| 3373/3373 [00:05<00:00, 624.92it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]  0%|          | 1/411 [00:00<01:15,  5.42it/s] 11%|█         | 46/411 [00:00<00:01, 199.44it/s] 30%|███       | 124/411 [00:00<00:00, 422.32it/s] 43%|████▎     | 175/411 [00:00<00:00, 441.45it/s]100%|██████████| 411/411 [00:00<00:00, 694.22it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]  8%|▊         | 31/411 [00:00<00:02, 156.35it/s] 43%|████▎     | 176/411 [00:00<00:00, 697.93it/s] 76%|███████▌  | 312/411 [00:00<00:00, 707.76it/s] 96%|█████████▌| 395/411 [00:00<00:00, 736.69it/s]100%|██████████| 411/411 [00:00<00:00, 692.74it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s] 35%|███▍      | 142/411 [00:00<00:00, 1415.41it/s] 69%|██████▉   | 284/411 [00:00<00:00, 902.98it/s] 100%|██████████| 411/411 [00:00<00:00, 1352.78it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s] 19%|█▉        | 79/412 [00:00<00:00, 786.29it/s] 60%|█████▉    | 247/412 [00:00<00:00, 1290.13it/s] 99%|█████████▊| 406/412 [00:00<00:00, 1424.42it/s]100%|██████████| 412/412 [00:00<00:00, 1354.88it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]  6%|▌         | 25/412 [00:00<00:01, 246.81it/s] 19%|█▉        | 80/412 [00:00<00:00, 422.29it/s] 50%|████▉     | 204/412 [00:00<00:00, 781.12it/s] 68%|██████▊   | 282/412 [00:00<00:00, 780.05it/s] 94%|█████████▍| 388/412 [00:00<00:00, 879.93it/s]100%|██████████| 412/412 [00:00<00:00, 813.42it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]  2%|▏         | 8/412 [00:00<00:05, 78.44it/s] 36%|███▌      | 149/412 [00:00<00:00, 565.86it/s] 62%|██████▏   | 254/412 [00:00<00:00, 731.85it/s]100%|██████████| 412/412 [00:00<00:00, 840.14it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]  2%|▏         | 16/834 [00:00<00:09, 81.87it/s] 14%|█▍        | 116/834 [00:00<00:01, 467.51it/s] 32%|███▏      | 266/834 [00:00<00:00, 827.12it/s] 44%|████▎     | 363/834 [00:00<00:00, 655.13it/s] 53%|█████▎    | 441/834 [00:00<00:00, 454.36it/s] 66%|██████▌   | 551/834 [00:00<00:00, 584.31it/s] 81%|████████▏ | 678/834 [00:01<00:00, 736.49it/s] 93%|█████████▎| 777/834 [00:01<00:00, 793.79it/s]100%|██████████| 834/834 [00:01<00:00, 693.71it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]  0%|          | 1/834 [00:00<04:08,  3.36it/s]  2%|▏         | 13/834 [00:00<00:20, 40.64it/s]  7%|▋         | 60/834 [00:00<00:04, 175.31it/s] 15%|█▍        | 121/834 [00:00<00:02, 306.91it/s] 19%|█▉        | 162/834 [00:00<00:02, 264.15it/s] 25%|██▌       | 211/834 [00:00<00:02, 256.02it/s] 35%|███▌      | 296/834 [00:01<00:02, 232.79it/s] 44%|████▎     | 363/834 [00:01<00:01, 302.53it/s] 48%|████▊     | 403/834 [00:01<00:01, 268.63it/s] 58%|█████▊    | 481/834 [00:01<00:01, 308.60it/s] 64%|██████▍   | 534/834 [00:01<00:00, 348.28it/s] 69%|██████▉   | 575/834 [00:02<00:00, 358.85it/s] 74%|███████▍  | 616/834 [00:02<00:00, 369.43it/s] 79%|███████▉  | 657/834 [00:02<00:00, 374.00it/s] 85%|████████▍ | 708/834 [00:02<00:00, 327.46it/s] 95%|█████████▌| 795/834 [00:02<00:00, 448.93it/s]100%|██████████| 834/834 [00:02<00:00, 308.90it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s] 13%|█▎        | 108/834 [00:00<00:00, 1077.48it/s] 26%|██▌       | 216/834 [00:00<00:00, 672.44it/s]  37%|███▋      | 305/834 [00:00<00:00, 554.61it/s] 45%|████▍     | 375/834 [00:00<00:00, 593.22it/s] 54%|█████▎    | 447/834 [00:00<00:00, 400.08it/s] 62%|██████▏   | 521/834 [00:01<00:00, 397.46it/s] 84%|████████▍ | 701/834 [00:01<00:00, 671.62it/s] 95%|█████████▍| 791/834 [00:01<00:00, 711.75it/s]100%|██████████| 834/834 [00:01<00:00, 636.97it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.5053; eval:-0.1309; lr:0.000500
epoch:2; metric:emoval; train:-0.0041; eval:0.3066; lr:0.000500
epoch:3; metric:emoval; train:0.3384; eval:0.4601; lr:0.000500
epoch:4; metric:emoval; train:0.4943; eval:0.5036; lr:0.000500
epoch:5; metric:emoval; train:0.5915; eval:0.4615; lr:0.000500
epoch:6; metric:emoval; train:0.6581; eval:0.5439; lr:0.000500
epoch:7; metric:emoval; train:0.7107; eval:0.4974; lr:0.000500
epoch:8; metric:emoval; train:0.7522; eval:0.5137; lr:0.000500
epoch:9; metric:emoval; train:0.7694; eval:0.4867; lr:0.000500
epoch:10; metric:emoval; train:0.7995; eval:0.5496; lr:0.000500
epoch:11; metric:emoval; train:0.8228; eval:0.5382; lr:0.000500
epoch:12; metric:emoval; train:0.8420; eval:0.4977; lr:0.000500
epoch:13; metric:emoval; train:0.8492; eval:0.4899; lr:0.000500
epoch:14; metric:emoval; train:0.8732; eval:0.4262; lr:0.000500
epoch:15; metric:emoval; train:0.8522; eval:0.5152; lr:0.000500
epoch:16; metric:emoval; train:0.8890; eval:0.4432; lr:0.000500
epoch:17; metric:emoval; train:0.8901; eval:0.4802; lr:0.000500
epoch:18; metric:emoval; train:0.8859; eval:0.3640; lr:0.000500
epoch:19; metric:emoval; train:0.8755; eval:0.4790; lr:0.000500
epoch:20; metric:emoval; train:0.8866; eval:0.4755; lr:0.000500
epoch:21; metric:emoval; train:0.9050; eval:0.4884; lr:0.000250
epoch:22; metric:emoval; train:0.9312; eval:0.5343; lr:0.000250
epoch:23; metric:emoval; train:0.9467; eval:0.4852; lr:0.000250
epoch:24; metric:emoval; train:0.9421; eval:0.5232; lr:0.000250
epoch:25; metric:emoval; train:0.9336; eval:0.4854; lr:0.000250
epoch:26; metric:emoval; train:0.9386; eval:0.5004; lr:0.000250
epoch:27; metric:emoval; train:0.9381; eval:0.5162; lr:0.000250
epoch:28; metric:emoval; train:0.9269; eval:0.4408; lr:0.000250
epoch:29; metric:emoval; train:0.9202; eval:0.4716; lr:0.000250
epoch:30; metric:emoval; train:0.9166; eval:0.5064; lr:0.000250
epoch:31; metric:emoval; train:0.9106; eval:0.4803; lr:0.000250
epoch:32; metric:emoval; train:0.8997; eval:0.4879; lr:0.000125
epoch:33; metric:emoval; train:0.9291; eval:0.5056; lr:0.000125
epoch:34; metric:emoval; train:0.9301; eval:0.5211; lr:0.000125
epoch:35; metric:emoval; train:0.9188; eval:0.5112; lr:0.000125
epoch:36; metric:emoval; train:0.9303; eval:0.5049; lr:0.000125
epoch:37; metric:emoval; train:0.9154; eval:0.5012; lr:0.000125
epoch:38; metric:emoval; train:0.9237; eval:0.5326; lr:0.000125
epoch:39; metric:emoval; train:0.9108; eval:0.5073; lr:0.000125
epoch:40; metric:emoval; train:0.8960; eval:0.5188; lr:0.000125
Early stopping at epoch 40, best epoch: 10
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 9, duration: 1651.1945815086365 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2478; eval:-0.0133; lr:0.000500
epoch:2; metric:emoval; train:0.1497; eval:0.2240; lr:0.000500
epoch:3; metric:emoval; train:0.3737; eval:0.4411; lr:0.000500
epoch:4; metric:emoval; train:0.5038; eval:0.5018; lr:0.000500
epoch:5; metric:emoval; train:0.5931; eval:0.4103; lr:0.000500
epoch:6; metric:emoval; train:0.6620; eval:0.3923; lr:0.000500
epoch:7; metric:emoval; train:0.6780; eval:0.4822; lr:0.000500
epoch:8; metric:emoval; train:0.7474; eval:0.5221; lr:0.000500
epoch:9; metric:emoval; train:0.7786; eval:0.3731; lr:0.000500
epoch:10; metric:emoval; train:0.7815; eval:0.5331; lr:0.000500
epoch:11; metric:emoval; train:0.8471; eval:0.4987; lr:0.000500
epoch:12; metric:emoval; train:0.8495; eval:0.5367; lr:0.000500
epoch:13; metric:emoval; train:0.8670; eval:0.4960; lr:0.000500
epoch:14; metric:emoval; train:0.8622; eval:0.4724; lr:0.000500
epoch:15; metric:emoval; train:0.8118; eval:0.5184; lr:0.000500
epoch:16; metric:emoval; train:0.8671; eval:0.5349; lr:0.000500
epoch:17; metric:emoval; train:0.8895; eval:0.5197; lr:0.000500
epoch:18; metric:emoval; train:0.8971; eval:0.5211; lr:0.000500
epoch:19; metric:emoval; train:0.8933; eval:0.4922; lr:0.000500
epoch:20; metric:emoval; train:0.8935; eval:0.5333; lr:0.000500
epoch:21; metric:emoval; train:0.8971; eval:0.4972; lr:0.000500
epoch:22; metric:emoval; train:0.8956; eval:0.5138; lr:0.000500
epoch:23; metric:emoval; train:0.9029; eval:0.4144; lr:0.000250
epoch:24; metric:emoval; train:0.9146; eval:0.5549; lr:0.000250
epoch:25; metric:emoval; train:0.9341; eval:0.5256; lr:0.000250
epoch:26; metric:emoval; train:0.9261; eval:0.4994; lr:0.000250
epoch:27; metric:emoval; train:0.9299; eval:0.5204; lr:0.000250
epoch:28; metric:emoval; train:0.9341; eval:0.5359; lr:0.000250
epoch:29; metric:emoval; train:0.9314; eval:0.5283; lr:0.000250
epoch:30; metric:emoval; train:0.9310; eval:0.5211; lr:0.000250
epoch:31; metric:emoval; train:0.9223; eval:0.5380; lr:0.000250
epoch:32; metric:emoval; train:0.9179; eval:0.5207; lr:0.000250
epoch:33; metric:emoval; train:0.9089; eval:0.4697; lr:0.000250
epoch:34; metric:emoval; train:0.9070; eval:0.4887; lr:0.000250
epoch:35; metric:emoval; train:0.8948; eval:0.5086; lr:0.000125
epoch:36; metric:emoval; train:0.9080; eval:0.5489; lr:0.000125
epoch:37; metric:emoval; train:0.9136; eval:0.5318; lr:0.000125
epoch:38; metric:emoval; train:0.9202; eval:0.5176; lr:0.000125
epoch:39; metric:emoval; train:0.9298; eval:0.5159; lr:0.000125
epoch:40; metric:emoval; train:0.9190; eval:0.5348; lr:0.000125
epoch:41; metric:emoval; train:0.9131; eval:0.5045; lr:0.000125
epoch:42; metric:emoval; train:0.9076; eval:0.5101; lr:0.000125
epoch:43; metric:emoval; train:0.9096; eval:0.5078; lr:0.000125
epoch:44; metric:emoval; train:0.9103; eval:0.5055; lr:0.000125
epoch:45; metric:emoval; train:0.9208; eval:0.4885; lr:0.000125
epoch:46; metric:emoval; train:0.9220; eval:0.5306; lr:0.000063
epoch:47; metric:emoval; train:0.9226; eval:0.5321; lr:0.000063
epoch:48; metric:emoval; train:0.9246; eval:0.5094; lr:0.000063
epoch:49; metric:emoval; train:0.9365; eval:0.5035; lr:0.000063
epoch:50; metric:emoval; train:0.9232; eval:0.5272; lr:0.000063
epoch:51; metric:emoval; train:0.9368; eval:0.5270; lr:0.000063
epoch:52; metric:emoval; train:0.9322; eval:0.5029; lr:0.000063
epoch:53; metric:emoval; train:0.9356; eval:0.5092; lr:0.000063
epoch:54; metric:emoval; train:0.9369; eval:0.4825; lr:0.000063
Early stopping at epoch 54, best epoch: 24
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 23, duration: 2154.6065566539764 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.4529; eval:-0.2136; lr:0.000500
epoch:2; metric:emoval; train:-0.0997; eval:-0.0076; lr:0.000500
epoch:3; metric:emoval; train:0.2609; eval:0.3325; lr:0.000500
epoch:4; metric:emoval; train:0.4715; eval:0.4401; lr:0.000500
epoch:5; metric:emoval; train:0.5854; eval:0.4512; lr:0.000500
epoch:6; metric:emoval; train:0.6040; eval:0.5075; lr:0.000500
epoch:7; metric:emoval; train:0.6863; eval:0.4673; lr:0.000500
epoch:8; metric:emoval; train:0.7584; eval:0.4791; lr:0.000500
epoch:9; metric:emoval; train:0.7853; eval:0.4765; lr:0.000500
epoch:10; metric:emoval; train:0.7858; eval:0.4761; lr:0.000500
epoch:11; metric:emoval; train:0.7990; eval:0.4514; lr:0.000500
epoch:12; metric:emoval; train:0.7999; eval:0.4889; lr:0.000500
epoch:13; metric:emoval; train:0.8261; eval:0.4946; lr:0.000500
epoch:14; metric:emoval; train:0.8411; eval:0.5026; lr:0.000500
epoch:15; metric:emoval; train:0.8753; eval:0.4529; lr:0.000500
epoch:16; metric:emoval; train:0.8772; eval:0.4250; lr:0.000500
epoch:17; metric:emoval; train:0.8360; eval:0.4313; lr:0.000250
epoch:18; metric:emoval; train:0.8798; eval:0.4659; lr:0.000250
epoch:19; metric:emoval; train:0.9219; eval:0.5049; lr:0.000250
epoch:20; metric:emoval; train:0.9368; eval:0.5135; lr:0.000250
epoch:21; metric:emoval; train:0.9378; eval:0.4983; lr:0.000250
epoch:22; metric:emoval; train:0.9311; eval:0.4392; lr:0.000250
epoch:23; metric:emoval; train:0.9309; eval:0.4777; lr:0.000250
epoch:24; metric:emoval; train:0.9136; eval:0.4818; lr:0.000250
epoch:25; metric:emoval; train:0.9219; eval:0.5024; lr:0.000250
epoch:26; metric:emoval; train:0.9225; eval:0.4310; lr:0.000250
epoch:27; metric:emoval; train:0.8949; eval:0.4325; lr:0.000250
epoch:28; metric:emoval; train:0.8946; eval:0.4643; lr:0.000250
epoch:29; metric:emoval; train:0.8954; eval:0.4908; lr:0.000250
epoch:30; metric:emoval; train:0.9238; eval:0.3749; lr:0.000250
epoch:31; metric:emoval; train:0.8550; eval:0.4986; lr:0.000125
epoch:32; metric:emoval; train:0.9125; eval:0.5422; lr:0.000125
epoch:33; metric:emoval; train:0.9265; eval:0.5232; lr:0.000125
epoch:34; metric:emoval; train:0.9225; eval:0.5346; lr:0.000125
epoch:35; metric:emoval; train:0.9074; eval:0.4894; lr:0.000125
epoch:36; metric:emoval; train:0.9158; eval:0.4895; lr:0.000125
epoch:37; metric:emoval; train:0.9124; eval:0.4781; lr:0.000125
epoch:38; metric:emoval; train:0.9113; eval:0.5052; lr:0.000125
epoch:39; metric:emoval; train:0.9139; eval:0.4890; lr:0.000125
epoch:40; metric:emoval; train:0.9122; eval:0.5255; lr:0.000125
epoch:41; metric:emoval; train:0.9150; eval:0.5189; lr:0.000125
epoch:42; metric:emoval; train:0.9241; eval:0.5087; lr:0.000125
epoch:43; metric:emoval; train:0.9200; eval:0.4965; lr:0.000063
epoch:44; metric:emoval; train:0.9160; eval:0.4880; lr:0.000063
epoch:45; metric:emoval; train:0.9216; eval:0.4988; lr:0.000063
epoch:46; metric:emoval; train:0.9284; eval:0.4682; lr:0.000063
epoch:47; metric:emoval; train:0.9301; eval:0.4822; lr:0.000063
epoch:48; metric:emoval; train:0.9285; eval:0.4696; lr:0.000063
epoch:49; metric:emoval; train:0.9250; eval:0.4912; lr:0.000063
epoch:50; metric:emoval; train:0.9338; eval:0.4805; lr:0.000063
epoch:51; metric:emoval; train:0.9083; eval:0.4927; lr:0.000063
epoch:52; metric:emoval; train:0.9179; eval:0.4872; lr:0.000063
epoch:53; metric:emoval; train:0.9313; eval:0.4877; lr:0.000063
epoch:54; metric:emoval; train:0.9216; eval:0.4835; lr:0.000031
epoch:55; metric:emoval; train:0.9340; eval:0.4933; lr:0.000031
epoch:56; metric:emoval; train:0.9302; eval:0.4632; lr:0.000031
epoch:57; metric:emoval; train:0.9407; eval:0.4919; lr:0.000031
epoch:58; metric:emoval; train:0.9405; eval:0.4785; lr:0.000031
epoch:59; metric:emoval; train:0.9376; eval:0.4967; lr:0.000031
epoch:60; metric:emoval; train:0.9261; eval:0.4872; lr:0.000031
epoch:61; metric:emoval; train:0.9340; eval:0.4883; lr:0.000031
epoch:62; metric:emoval; train:0.9331; eval:0.4873; lr:0.000031
Early stopping at epoch 62, best epoch: 32
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 31, duration: 2477.4943623542786 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2909; eval:0.0363; lr:0.000500
epoch:2; metric:emoval; train:0.1393; eval:0.3707; lr:0.000500
epoch:3; metric:emoval; train:0.3714; eval:0.4117; lr:0.000500
epoch:4; metric:emoval; train:0.4941; eval:0.5000; lr:0.000500
epoch:5; metric:emoval; train:0.5939; eval:0.5733; lr:0.000500
epoch:6; metric:emoval; train:0.6721; eval:0.5361; lr:0.000500
epoch:7; metric:emoval; train:0.7290; eval:0.4575; lr:0.000500
epoch:8; metric:emoval; train:0.7543; eval:0.4459; lr:0.000500
epoch:9; metric:emoval; train:0.7786; eval:0.3646; lr:0.000500
epoch:10; metric:emoval; train:0.7835; eval:0.5246; lr:0.000500
epoch:11; metric:emoval; train:0.8044; eval:0.5507; lr:0.000500
epoch:12; metric:emoval; train:0.8603; eval:0.4395; lr:0.000500
epoch:13; metric:emoval; train:0.8408; eval:0.5453; lr:0.000500
epoch:14; metric:emoval; train:0.8392; eval:0.4965; lr:0.000500
epoch:15; metric:emoval; train:0.8535; eval:0.5504; lr:0.000500
epoch:16; metric:emoval; train:0.8483; eval:0.5163; lr:0.000250
epoch:17; metric:emoval; train:0.9047; eval:0.5515; lr:0.000250
epoch:18; metric:emoval; train:0.9296; eval:0.5079; lr:0.000250
epoch:19; metric:emoval; train:0.9301; eval:0.5478; lr:0.000250
epoch:20; metric:emoval; train:0.9358; eval:0.5251; lr:0.000250
epoch:21; metric:emoval; train:0.9424; eval:0.4857; lr:0.000250
epoch:22; metric:emoval; train:0.9256; eval:0.5301; lr:0.000250
epoch:23; metric:emoval; train:0.9338; eval:0.5400; lr:0.000250
epoch:24; metric:emoval; train:0.9204; eval:0.5462; lr:0.000250
epoch:25; metric:emoval; train:0.9239; eval:0.4748; lr:0.000250
epoch:26; metric:emoval; train:0.9128; eval:0.4785; lr:0.000250
epoch:27; metric:emoval; train:0.9022; eval:0.5497; lr:0.000125
epoch:28; metric:emoval; train:0.9142; eval:0.5413; lr:0.000125
epoch:29; metric:emoval; train:0.9312; eval:0.5254; lr:0.000125
epoch:30; metric:emoval; train:0.9206; eval:0.5435; lr:0.000125
epoch:31; metric:emoval; train:0.9242; eval:0.5178; lr:0.000125
epoch:32; metric:emoval; train:0.9236; eval:0.5381; lr:0.000125
epoch:33; metric:emoval; train:0.9262; eval:0.5325; lr:0.000125
epoch:34; metric:emoval; train:0.9089; eval:0.5330; lr:0.000125
epoch:35; metric:emoval; train:0.9077; eval:0.5261; lr:0.000125
Early stopping at epoch 35, best epoch: 5
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 4, duration: 1291.6982474327087 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.4328; eval:-0.1548; lr:0.000500
epoch:2; metric:emoval; train:-0.0225; eval:0.1850; lr:0.000500
epoch:3; metric:emoval; train:0.2982; eval:0.0751; lr:0.000500
epoch:4; metric:emoval; train:0.4428; eval:0.4790; lr:0.000500
epoch:5; metric:emoval; train:0.5860; eval:0.4458; lr:0.000500
epoch:6; metric:emoval; train:0.6442; eval:0.4396; lr:0.000500
epoch:7; metric:emoval; train:0.6667; eval:0.4583; lr:0.000500
epoch:8; metric:emoval; train:0.7268; eval:0.5014; lr:0.000500
epoch:9; metric:emoval; train:0.7506; eval:0.3302; lr:0.000500
epoch:10; metric:emoval; train:0.7730; eval:0.5264; lr:0.000500
epoch:11; metric:emoval; train:0.8068; eval:0.5056; lr:0.000500
epoch:12; metric:emoval; train:0.8144; eval:0.3622; lr:0.000500
epoch:13; metric:emoval; train:0.7669; eval:0.4306; lr:0.000500
epoch:14; metric:emoval; train:0.8252; eval:0.5069; lr:0.000500
epoch:15; metric:emoval; train:0.8712; eval:0.4866; lr:0.000500
epoch:16; metric:emoval; train:0.8862; eval:0.4457; lr:0.000500
epoch:17; metric:emoval; train:0.8762; eval:0.4962; lr:0.000500
epoch:18; metric:emoval; train:0.8843; eval:0.5339; lr:0.000500
epoch:19; metric:emoval; train:0.8990; eval:0.4292; lr:0.000500
epoch:20; metric:emoval; train:0.8636; eval:0.3869; lr:0.000500
epoch:21; metric:emoval; train:0.8762; eval:0.4964; lr:0.000500
epoch:22; metric:emoval; train:0.9081; eval:0.4702; lr:0.000500
epoch:23; metric:emoval; train:0.9109; eval:0.5159; lr:0.000500
epoch:24; metric:emoval; train:0.8894; eval:0.4619; lr:0.000500
epoch:25; metric:emoval; train:0.8619; eval:0.5131; lr:0.000500
epoch:26; metric:emoval; train:0.8507; eval:0.5066; lr:0.000500
epoch:27; metric:emoval; train:0.8565; eval:0.5039; lr:0.000500
epoch:28; metric:emoval; train:0.8592; eval:0.4420; lr:0.000500
epoch:29; metric:emoval; train:0.8563; eval:0.4723; lr:0.000250
epoch:30; metric:emoval; train:0.8937; eval:0.5388; lr:0.000250
epoch:31; metric:emoval; train:0.9097; eval:0.5180; lr:0.000250
epoch:32; metric:emoval; train:0.9165; eval:0.5530; lr:0.000250
epoch:33; metric:emoval; train:0.9244; eval:0.5266; lr:0.000250
epoch:34; metric:emoval; train:0.9162; eval:0.5298; lr:0.000250
epoch:35; metric:emoval; train:0.9141; eval:0.4775; lr:0.000250
epoch:36; metric:emoval; train:0.8828; eval:0.5035; lr:0.000250
epoch:37; metric:emoval; train:0.8963; eval:0.5557; lr:0.000250
epoch:38; metric:emoval; train:0.8973; eval:0.4866; lr:0.000250
epoch:39; metric:emoval; train:0.8860; eval:0.5422; lr:0.000250
epoch:40; metric:emoval; train:0.8995; eval:0.5527; lr:0.000250
epoch:41; metric:emoval; train:0.8925; eval:0.4955; lr:0.000250
epoch:42; metric:emoval; train:0.8946; eval:0.5292; lr:0.000250
epoch:43; metric:emoval; train:0.9134; eval:0.5136; lr:0.000250
epoch:44; metric:emoval; train:0.8880; eval:0.5468; lr:0.000250
epoch:45; metric:emoval; train:0.8929; eval:0.4738; lr:0.000250
epoch:46; metric:emoval; train:0.9097; eval:0.4723; lr:0.000250
epoch:47; metric:emoval; train:0.9005; eval:0.4994; lr:0.000250
epoch:48; metric:emoval; train:0.9105; eval:0.5092; lr:0.000125
epoch:49; metric:emoval; train:0.9260; eval:0.5334; lr:0.000125
epoch:50; metric:emoval; train:0.9225; eval:0.5187; lr:0.000125
epoch:51; metric:emoval; train:0.9137; eval:0.5212; lr:0.000125
epoch:52; metric:emoval; train:0.9222; eval:0.5207; lr:0.000125
epoch:53; metric:emoval; train:0.9279; eval:0.5437; lr:0.000125
epoch:54; metric:emoval; train:0.9260; eval:0.5293; lr:0.000125
epoch:55; metric:emoval; train:0.9209; eval:0.5323; lr:0.000125
epoch:56; metric:emoval; train:0.9232; eval:0.5472; lr:0.000125
epoch:57; metric:emoval; train:0.9146; eval:0.4889; lr:0.000125
epoch:58; metric:emoval; train:0.9138; eval:0.5351; lr:0.000125
epoch:59; metric:emoval; train:0.9309; eval:0.5199; lr:0.000063
epoch:60; metric:emoval; train:0.9357; eval:0.5268; lr:0.000063
epoch:61; metric:emoval; train:0.9380; eval:0.5252; lr:0.000063
epoch:62; metric:emoval; train:0.9315; eval:0.5366; lr:0.000063
epoch:63; metric:emoval; train:0.9310; eval:0.5348; lr:0.000063
epoch:64; metric:emoval; train:0.9397; eval:0.5296; lr:0.000063
epoch:65; metric:emoval; train:0.9385; eval:0.5288; lr:0.000063
epoch:66; metric:emoval; train:0.9316; eval:0.5190; lr:0.000063
epoch:67; metric:emoval; train:0.9412; eval:0.5267; lr:0.000063
Early stopping at epoch 67, best epoch: 37
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 36, duration: 2445.29980802536 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7358_acc:0.7373_val:0.7225_1770132216.3017373.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.8013_acc:0.8005_val:0.6880_1770132216.3017373.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7670_acc:0.7694_val:0.6508_1770132216.3017373.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.8814_acc:0.8849_val:78.8876_1770132216.3017373.npz
