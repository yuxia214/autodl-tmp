====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=2.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v5', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s]  2%|▏         | 68/3373 [00:00<00:04, 676.12it/s]  4%|▍         | 136/3373 [00:00<00:04, 661.63it/s]  7%|▋         | 224/3373 [00:00<00:04, 759.64it/s]  9%|▉         | 301/3373 [00:00<00:04, 760.34it/s] 11%|█         | 378/3373 [00:00<00:05, 579.67it/s] 13%|█▎        | 442/3373 [00:00<00:04, 590.92it/s] 15%|█▌        | 511/3373 [00:00<00:04, 613.22it/s] 17%|█▋        | 584/3373 [00:00<00:04, 645.90it/s] 21%|██        | 700/3373 [00:01<00:04, 625.17it/s] 23%|██▎       | 765/3373 [00:01<00:04, 623.89it/s] 25%|██▍       | 837/3373 [00:01<00:03, 638.54it/s] 27%|██▋       | 903/3373 [00:01<00:04, 517.54it/s] 29%|██▉       | 978/3373 [00:01<00:04, 571.58it/s] 32%|███▏      | 1067/3373 [00:01<00:03, 648.18it/s] 34%|███▎      | 1137/3373 [00:01<00:05, 436.35it/s] 36%|███▌      | 1209/3373 [00:02<00:04, 491.02it/s] 38%|███▊      | 1289/3373 [00:02<00:03, 558.87it/s] 40%|████      | 1356/3373 [00:02<00:04, 476.73it/s] 43%|████▎     | 1463/3373 [00:02<00:03, 605.47it/s] 46%|████▌     | 1535/3373 [00:02<00:02, 625.75it/s] 48%|████▊     | 1606/3373 [00:02<00:02, 645.49it/s] 50%|████▉     | 1679/3373 [00:02<00:02, 663.78it/s] 52%|█████▏    | 1750/3373 [00:02<00:02, 675.06it/s] 55%|█████▌    | 1863/3373 [00:03<00:02, 638.54it/s] 59%|█████▉    | 1999/3373 [00:03<00:01, 808.96it/s] 62%|██████▏   | 2087/3373 [00:03<00:01, 806.41it/s] 64%|██████▍   | 2172/3373 [00:03<00:01, 650.09it/s] 68%|██████▊   | 2281/3373 [00:03<00:01, 750.30it/s] 71%|███████   | 2395/3373 [00:03<00:01, 845.23it/s] 74%|███████▍  | 2488/3373 [00:03<00:01, 701.83it/s] 76%|███████▌  | 2568/3373 [00:03<00:01, 723.61it/s] 79%|███████▊  | 2648/3373 [00:04<00:00, 730.85it/s] 81%|████████  | 2727/3373 [00:04<00:01, 600.53it/s] 83%|████████▎ | 2799/3373 [00:04<00:00, 619.49it/s] 86%|████████▋ | 2917/3373 [00:04<00:00, 756.28it/s] 89%|████████▉ | 3000/3373 [00:04<00:00, 620.34it/s] 91%|█████████ | 3071/3373 [00:04<00:00, 640.24it/s] 93%|█████████▎| 3152/3373 [00:04<00:00, 553.75it/s] 96%|█████████▌| 3239/3373 [00:05<00:00, 621.46it/s] 98%|█████████▊| 3308/3373 [00:05<00:00, 637.57it/s]100%|██████████| 3373/3373 [00:05<00:00, 637.02it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s]  1%|          | 22/3373 [00:00<00:15, 218.15it/s]  2%|▏         | 54/3373 [00:00<00:12, 271.82it/s]  4%|▍         | 130/3373 [00:00<00:09, 344.26it/s]  5%|▍         | 163/3373 [00:00<00:12, 261.28it/s]  7%|▋         | 234/3373 [00:00<00:10, 301.13it/s]  8%|▊         | 264/3373 [00:00<00:10, 295.45it/s] 11%|█         | 360/3373 [00:01<00:13, 230.88it/s] 12%|█▏        | 415/3373 [00:01<00:12, 243.27it/s] 13%|█▎        | 442/3373 [00:01<00:15, 190.18it/s] 14%|█▍        | 479/3373 [00:01<00:13, 216.17it/s] 15%|█▌        | 520/3373 [00:02<00:13, 213.60it/s] 17%|█▋        | 559/3373 [00:02<00:11, 243.17it/s] 18%|█▊        | 593/3373 [00:02<00:10, 262.01it/s] 18%|█▊        | 624/3373 [00:02<00:10, 269.65it/s] 19%|█▉        | 654/3373 [00:02<00:12, 225.77it/s] 20%|██        | 691/3373 [00:02<00:10, 255.21it/s] 22%|██▏       | 743/3373 [00:03<00:12, 217.59it/s] 24%|██▍       | 823/3373 [00:03<00:07, 324.04it/s] 26%|██▌       | 864/3373 [00:03<00:07, 341.45it/s] 27%|██▋       | 926/3373 [00:03<00:06, 405.23it/s] 29%|██▉       | 974/3373 [00:04<00:12, 189.30it/s] 30%|███       | 1022/3373 [00:04<00:12, 181.62it/s] 31%|███       | 1052/3373 [00:04<00:11, 195.88it/s] 32%|███▏      | 1096/3373 [00:04<00:11, 204.15it/s] 34%|███▍      | 1154/3373 [00:04<00:08, 265.97it/s] 35%|███▌      | 1191/3373 [00:04<00:07, 283.03it/s] 36%|███▋      | 1228/3373 [00:04<00:08, 251.18it/s] 37%|███▋      | 1259/3373 [00:05<00:08, 259.37it/s] 38%|███▊      | 1290/3373 [00:05<00:09, 224.23it/s] 39%|███▉      | 1317/3373 [00:05<00:08, 230.76it/s] 40%|████      | 1356/3373 [00:05<00:07, 266.44it/s] 41%|████      | 1386/3373 [00:05<00:08, 222.14it/s] 42%|████▏     | 1430/3373 [00:05<00:08, 222.35it/s] 43%|████▎     | 1467/3373 [00:05<00:07, 250.51it/s] 45%|████▍     | 1509/3373 [00:06<00:06, 287.80it/s] 46%|████▌     | 1541/3373 [00:06<00:09, 201.13it/s] 47%|████▋     | 1597/3373 [00:06<00:06, 269.21it/s] 49%|████▊     | 1642/3373 [00:06<00:05, 306.58it/s] 50%|████▉     | 1680/3373 [00:06<00:06, 261.79it/s] 51%|█████     | 1712/3373 [00:07<00:08, 193.08it/s] 52%|█████▏    | 1738/3373 [00:07<00:09, 174.68it/s] 53%|█████▎    | 1785/3373 [00:07<00:07, 224.60it/s] 54%|█████▍    | 1814/3373 [00:07<00:07, 199.69it/s] 55%|█████▍    | 1850/3373 [00:07<00:06, 230.43it/s] 56%|█████▋    | 1905/3373 [00:07<00:05, 247.40it/s] 57%|█████▋    | 1933/3373 [00:08<00:06, 208.12it/s] 62%|██████▏   | 2076/3373 [00:08<00:02, 442.55it/s] 64%|██████▎   | 2146/3373 [00:08<00:02, 497.77it/s] 65%|██████▌   | 2209/3373 [00:08<00:03, 361.90it/s] 67%|██████▋   | 2260/3373 [00:08<00:02, 387.40it/s] 69%|██████▉   | 2342/3373 [00:08<00:02, 477.75it/s] 72%|███████▏  | 2428/3373 [00:08<00:02, 467.45it/s] 74%|███████▎  | 2484/3373 [00:09<00:01, 477.16it/s] 75%|███████▌  | 2538/3373 [00:09<00:02, 342.37it/s] 77%|███████▋  | 2592/3373 [00:09<00:02, 378.77it/s] 78%|███████▊  | 2639/3373 [00:09<00:01, 397.48it/s] 80%|███████▉  | 2686/3373 [00:09<00:01, 413.77it/s] 81%|████████  | 2733/3373 [00:09<00:01, 343.45it/s] 82%|████████▏ | 2773/3373 [00:09<00:01, 355.45it/s] 83%|████████▎ | 2813/3373 [00:10<00:01, 365.76it/s] 86%|████████▌ | 2894/3373 [00:10<00:01, 476.91it/s] 87%|████████▋ | 2946/3373 [00:10<00:01, 382.88it/s] 89%|████████▊ | 2990/3373 [00:10<00:01, 320.42it/s] 90%|████████▉ | 3028/3373 [00:10<00:01, 236.27it/s] 91%|█████████ | 3058/3373 [00:11<00:01, 212.00it/s] 93%|█████████▎| 3152/3373 [00:11<00:00, 336.34it/s] 95%|█████████▍| 3197/3373 [00:11<00:00, 302.65it/s] 97%|█████████▋| 3256/3373 [00:11<00:00, 353.46it/s] 98%|█████████▊| 3301/3373 [00:11<00:00, 372.50it/s] 99%|█████████▉| 3345/3373 [00:11<00:00, 318.48it/s]100%|██████████| 3373/3373 [00:11<00:00, 285.90it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s]  0%|          | 1/3373 [00:00<10:47,  5.21it/s]  3%|▎         | 118/3373 [00:00<00:09, 349.67it/s]  5%|▍         | 153/3373 [00:00<00:09, 343.94it/s]  7%|▋         | 239/3373 [00:00<00:08, 390.25it/s]  8%|▊         | 278/3373 [00:00<00:09, 311.37it/s]  9%|▉         | 312/3373 [00:00<00:09, 316.37it/s] 11%|█         | 365/3373 [00:01<00:08, 366.77it/s] 13%|█▎        | 451/3373 [00:01<00:06, 484.90it/s] 15%|█▍        | 504/3373 [00:01<00:07, 401.07it/s] 16%|█▋        | 549/3373 [00:01<00:06, 412.05it/s] 18%|█▊        | 594/3373 [00:01<00:06, 419.45it/s] 21%|██        | 708/3373 [00:01<00:04, 599.80it/s] 26%|██▌       | 869/3373 [00:01<00:04, 575.70it/s] 31%|███       | 1031/3373 [00:02<00:02, 784.94it/s] 33%|███▎      | 1122/3373 [00:02<00:03, 665.58it/s] 36%|███▌      | 1200/3373 [00:02<00:03, 682.86it/s] 38%|███▊      | 1277/3373 [00:02<00:03, 578.22it/s] 41%|████      | 1383/3373 [00:02<00:02, 679.26it/s] 44%|████▍     | 1478/3373 [00:02<00:02, 741.55it/s] 48%|████▊     | 1609/3373 [00:02<00:02, 878.63it/s] 51%|█████     | 1706/3373 [00:02<00:01, 892.27it/s] 53%|█████▎    | 1802/3373 [00:03<00:01, 900.22it/s] 56%|█████▌    | 1897/3373 [00:03<00:02, 724.09it/s] 59%|█████▊    | 1978/3373 [00:03<00:02, 609.07it/s] 62%|██████▏   | 2088/3373 [00:03<00:01, 715.77it/s] 64%|██████▍   | 2170/3373 [00:03<00:01, 737.03it/s] 67%|██████▋   | 2252/3373 [00:03<00:01, 753.69it/s] 69%|██████▉   | 2333/3373 [00:03<00:01, 755.23it/s] 72%|███████▏  | 2413/3373 [00:04<00:01, 607.07it/s] 74%|███████▎  | 2482/3373 [00:04<00:01, 621.49it/s] 77%|███████▋  | 2607/3373 [00:04<00:00, 777.95it/s] 81%|████████▏ | 2745/3373 [00:04<00:00, 935.55it/s] 84%|████████▍ | 2846/3373 [00:04<00:00, 952.63it/s] 87%|████████▋ | 2947/3373 [00:04<00:00, 764.24it/s] 90%|████████▉ | 3033/3373 [00:04<00:00, 781.59it/s] 95%|█████████▍| 3199/3373 [00:04<00:00, 819.15it/s] 99%|█████████▊| 3323/3373 [00:05<00:00, 915.02it/s]100%|██████████| 3373/3373 [00:05<00:00, 662.07it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]  9%|▊         | 35/411 [00:00<00:01, 345.98it/s] 36%|███▋      | 150/411 [00:00<00:00, 804.17it/s] 56%|█████▌    | 231/411 [00:00<00:00, 564.69it/s] 97%|█████████▋| 397/411 [00:00<00:00, 903.24it/s]100%|██████████| 411/411 [00:00<00:00, 822.14it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]  8%|▊         | 34/411 [00:00<00:02, 185.23it/s] 13%|█▎        | 53/411 [00:00<00:01, 183.60it/s] 18%|█▊        | 72/411 [00:00<00:03, 107.57it/s] 33%|███▎      | 134/411 [00:00<00:01, 229.35it/s] 41%|████▏     | 170/411 [00:00<00:01, 210.03it/s] 69%|██████▉   | 284/411 [00:00<00:00, 418.69it/s] 98%|█████████▊| 401/411 [00:01<00:00, 601.21it/s]100%|██████████| 411/411 [00:01<00:00, 379.97it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]  0%|          | 2/411 [00:00<00:40, 10.06it/s]  8%|▊         | 34/411 [00:00<00:02, 138.93it/s] 31%|███       | 126/411 [00:00<00:00, 427.50it/s] 49%|████▉     | 203/411 [00:00<00:00, 536.11it/s] 89%|████████▉ | 367/411 [00:00<00:00, 895.46it/s]100%|██████████| 411/411 [00:00<00:00, 586.50it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]  8%|▊         | 34/412 [00:00<00:01, 339.12it/s] 44%|████▎     | 180/412 [00:00<00:00, 997.70it/s] 80%|████████  | 330/412 [00:00<00:00, 1226.56it/s]100%|██████████| 412/412 [00:00<00:00, 1035.98it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]  9%|▊         | 36/412 [00:00<00:01, 359.87it/s] 27%|██▋       | 111/412 [00:00<00:00, 588.24it/s] 41%|████▏     | 170/412 [00:00<00:00, 410.52it/s] 76%|███████▌  | 314/412 [00:00<00:00, 729.13it/s]100%|██████████| 412/412 [00:00<00:00, 816.57it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s] 15%|█▍        | 61/412 [00:00<00:00, 598.26it/s] 29%|██▉       | 121/412 [00:00<00:00, 381.87it/s] 62%|██████▏   | 255/412 [00:00<00:00, 714.16it/s] 83%|████████▎ | 340/412 [00:00<00:00, 754.73it/s]100%|██████████| 412/412 [00:00<00:00, 814.67it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]  3%|▎         | 22/834 [00:00<00:07, 112.13it/s] 13%|█▎        | 109/834 [00:00<00:01, 430.92it/s] 23%|██▎       | 194/834 [00:00<00:01, 584.97it/s] 32%|███▏      | 263/834 [00:00<00:00, 605.88it/s] 41%|████▏     | 346/834 [00:00<00:00, 678.12it/s] 53%|█████▎    | 445/834 [00:00<00:00, 775.47it/s] 65%|██████▍   | 538/834 [00:00<00:00, 646.78it/s] 73%|███████▎  | 609/834 [00:01<00:00, 435.09it/s] 94%|█████████▍| 787/834 [00:01<00:00, 697.46it/s]100%|██████████| 834/834 [00:01<00:00, 557.00it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]  0%|          | 2/834 [00:00<00:42, 19.37it/s]  4%|▍         | 37/834 [00:00<00:03, 210.41it/s]  7%|▋         | 62/834 [00:00<00:03, 226.96it/s] 10%|█         | 85/834 [00:00<00:04, 166.76it/s] 13%|█▎        | 107/834 [00:00<00:05, 141.03it/s] 18%|█▊        | 149/834 [00:00<00:04, 168.10it/s] 21%|██        | 176/834 [00:01<00:03, 189.25it/s] 26%|██▌       | 218/834 [00:01<00:02, 240.78it/s] 29%|██▉       | 245/834 [00:01<00:02, 200.80it/s] 32%|███▏      | 268/834 [00:01<00:02, 206.74it/s] 35%|███▍      | 291/834 [00:01<00:02, 210.70it/s] 38%|███▊      | 314/834 [00:01<00:02, 214.01it/s] 41%|████      | 341/834 [00:01<00:02, 183.05it/s] 45%|████▍     | 373/834 [00:02<00:02, 175.26it/s] 49%|████▉     | 410/834 [00:02<00:01, 216.64it/s] 52%|█████▏    | 435/834 [00:02<00:01, 222.65it/s] 58%|█████▊    | 482/834 [00:02<00:01, 282.61it/s] 62%|██████▏   | 513/834 [00:02<00:01, 228.99it/s] 65%|██████▍   | 542/834 [00:02<00:01, 199.39it/s] 72%|███████▏  | 597/834 [00:02<00:00, 272.74it/s] 76%|███████▌  | 630/834 [00:02<00:00, 284.36it/s] 79%|███████▉  | 663/834 [00:03<00:00, 239.47it/s] 83%|████████▎ | 691/834 [00:03<00:00, 246.89it/s] 86%|████████▌ | 719/834 [00:03<00:00, 208.10it/s] 89%|████████▉ | 743/834 [00:03<00:00, 214.82it/s] 98%|█████████▊| 818/834 [00:03<00:00, 340.59it/s]100%|██████████| 834/834 [00:03<00:00, 231.53it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]  6%|▋         | 53/834 [00:00<00:02, 278.80it/s] 10%|▉         | 81/834 [00:00<00:02, 279.11it/s] 26%|██▌       | 213/834 [00:00<00:00, 656.45it/s] 42%|████▏     | 354/834 [00:00<00:00, 685.45it/s] 52%|█████▏    | 435/834 [00:00<00:00, 569.67it/s] 59%|█████▉    | 496/834 [00:00<00:00, 578.01it/s] 81%|████████  | 677/834 [00:01<00:00, 713.12it/s] 96%|█████████▌| 800/834 [00:01<00:00, 828.26it/s]100%|██████████| 834/834 [00:01<00:00, 699.63it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3726; eval:-0.0740; lr:0.000500
epoch:2; metric:emoval; train:0.1390; eval:0.3263; lr:0.000500
epoch:3; metric:emoval; train:0.4311; eval:0.4021; lr:0.000500
epoch:4; metric:emoval; train:0.5343; eval:0.4300; lr:0.000500
epoch:5; metric:emoval; train:0.6204; eval:0.4672; lr:0.000500
epoch:6; metric:emoval; train:0.6874; eval:0.4625; lr:0.000500
epoch:7; metric:emoval; train:0.7155; eval:0.2731; lr:0.000500
epoch:8; metric:emoval; train:0.7356; eval:0.4469; lr:0.000500
epoch:9; metric:emoval; train:0.7768; eval:0.4668; lr:0.000500
epoch:10; metric:emoval; train:0.7785; eval:0.4911; lr:0.000500
epoch:11; metric:emoval; train:0.8146; eval:0.4433; lr:0.000500
epoch:12; metric:emoval; train:0.8187; eval:0.4797; lr:0.000500
epoch:13; metric:emoval; train:0.8449; eval:0.4811; lr:0.000500
epoch:14; metric:emoval; train:0.8548; eval:0.4326; lr:0.000500
epoch:15; metric:emoval; train:0.8560; eval:0.4783; lr:0.000500
epoch:16; metric:emoval; train:0.8727; eval:0.3650; lr:0.000500
epoch:17; metric:emoval; train:0.8786; eval:0.4776; lr:0.000500
epoch:18; metric:emoval; train:0.8883; eval:0.4898; lr:0.000500
epoch:19; metric:emoval; train:0.8759; eval:0.5104; lr:0.000500
epoch:20; metric:emoval; train:0.9068; eval:0.4597; lr:0.000500
epoch:21; metric:emoval; train:0.8915; eval:0.4920; lr:0.000500
epoch:22; metric:emoval; train:0.8959; eval:0.5050; lr:0.000500
epoch:23; metric:emoval; train:0.8895; eval:0.4846; lr:0.000500
epoch:24; metric:emoval; train:0.8832; eval:0.4306; lr:0.000500
epoch:25; metric:emoval; train:0.8846; eval:0.4548; lr:0.000500
epoch:26; metric:emoval; train:0.8899; eval:0.4767; lr:0.000500
epoch:27; metric:emoval; train:0.8821; eval:0.4660; lr:0.000500
epoch:28; metric:emoval; train:0.8519; eval:0.4256; lr:0.000500
epoch:29; metric:emoval; train:0.8873; eval:0.5039; lr:0.000500
epoch:30; metric:emoval; train:0.8832; eval:0.3498; lr:0.000250
epoch:31; metric:emoval; train:0.8598; eval:0.5218; lr:0.000250
epoch:32; metric:emoval; train:0.9077; eval:0.4411; lr:0.000250
epoch:33; metric:emoval; train:0.9001; eval:0.4703; lr:0.000250
epoch:34; metric:emoval; train:0.9109; eval:0.4670; lr:0.000250
epoch:35; metric:emoval; train:0.9195; eval:0.4813; lr:0.000250
epoch:36; metric:emoval; train:0.9061; eval:0.4884; lr:0.000250
epoch:37; metric:emoval; train:0.8970; eval:0.5230; lr:0.000250
epoch:38; metric:emoval; train:0.8980; eval:0.5412; lr:0.000250
epoch:39; metric:emoval; train:0.9017; eval:0.4905; lr:0.000250
epoch:40; metric:emoval; train:0.8981; eval:0.5214; lr:0.000250
epoch:41; metric:emoval; train:0.8939; eval:0.4673; lr:0.000250
epoch:42; metric:emoval; train:0.8896; eval:0.4976; lr:0.000250
epoch:43; metric:emoval; train:0.8925; eval:0.5223; lr:0.000250
epoch:44; metric:emoval; train:0.8920; eval:0.5378; lr:0.000250
epoch:45; metric:emoval; train:0.8997; eval:0.4892; lr:0.000250
epoch:46; metric:emoval; train:0.9011; eval:0.5191; lr:0.000250
epoch:47; metric:emoval; train:0.8992; eval:0.5140; lr:0.000250
epoch:48; metric:emoval; train:0.9007; eval:0.4660; lr:0.000250
epoch:49; metric:emoval; train:0.8974; eval:0.4701; lr:0.000125
epoch:50; metric:emoval; train:0.9073; eval:0.4906; lr:0.000125
epoch:51; metric:emoval; train:0.9229; eval:0.5168; lr:0.000125
epoch:52; metric:emoval; train:0.9243; eval:0.5226; lr:0.000125
epoch:53; metric:emoval; train:0.9198; eval:0.5252; lr:0.000125
epoch:54; metric:emoval; train:0.9290; eval:0.5126; lr:0.000125
epoch:55; metric:emoval; train:0.9244; eval:0.5243; lr:0.000125
epoch:56; metric:emoval; train:0.9183; eval:0.5044; lr:0.000125
epoch:57; metric:emoval; train:0.9303; eval:0.5332; lr:0.000125
epoch:58; metric:emoval; train:0.9248; eval:0.5454; lr:0.000125
epoch:59; metric:emoval; train:0.9275; eval:0.5294; lr:0.000125
epoch:60; metric:emoval; train:0.9212; eval:0.5337; lr:0.000125
epoch:61; metric:emoval; train:0.9141; eval:0.5261; lr:0.000125
epoch:62; metric:emoval; train:0.9277; eval:0.5175; lr:0.000125
epoch:63; metric:emoval; train:0.9202; eval:0.5191; lr:0.000125
epoch:64; metric:emoval; train:0.9274; eval:0.5105; lr:0.000125
epoch:65; metric:emoval; train:0.9308; eval:0.4825; lr:0.000125
epoch:66; metric:emoval; train:0.9203; eval:0.5090; lr:0.000125
epoch:67; metric:emoval; train:0.9278; eval:0.5052; lr:0.000125
epoch:68; metric:emoval; train:0.9357; eval:0.5036; lr:0.000125
epoch:69; metric:emoval; train:0.9166; eval:0.4957; lr:0.000063
epoch:70; metric:emoval; train:0.9338; eval:0.4952; lr:0.000063
epoch:71; metric:emoval; train:0.9330; eval:0.5068; lr:0.000063
epoch:72; metric:emoval; train:0.9375; eval:0.5100; lr:0.000063
epoch:73; metric:emoval; train:0.9375; eval:0.4970; lr:0.000063
epoch:74; metric:emoval; train:0.9419; eval:0.4843; lr:0.000063
epoch:75; metric:emoval; train:0.9390; eval:0.4995; lr:0.000063
epoch:76; metric:emoval; train:0.9421; eval:0.4583; lr:0.000063
epoch:77; metric:emoval; train:0.9422; eval:0.4939; lr:0.000063
epoch:78; metric:emoval; train:0.9390; eval:0.4946; lr:0.000063
epoch:79; metric:emoval; train:0.9391; eval:0.4944; lr:0.000063
epoch:80; metric:emoval; train:0.9470; eval:0.5017; lr:0.000031
epoch:81; metric:emoval; train:0.9505; eval:0.4959; lr:0.000031
epoch:82; metric:emoval; train:0.9487; eval:0.4938; lr:0.000031
epoch:83; metric:emoval; train:0.9479; eval:0.4911; lr:0.000031
epoch:84; metric:emoval; train:0.9405; eval:0.4884; lr:0.000031
epoch:85; metric:emoval; train:0.9469; eval:0.5033; lr:0.000031
epoch:86; metric:emoval; train:0.9513; eval:0.4920; lr:0.000031
epoch:87; metric:emoval; train:0.9474; eval:0.4896; lr:0.000031
epoch:88; metric:emoval; train:0.9521; eval:0.4946; lr:0.000031
Early stopping at epoch 88, best epoch: 58
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 57, duration: 5721.001980304718 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3132; eval:0.0528; lr:0.000500
epoch:2; metric:emoval; train:0.1689; eval:0.3485; lr:0.000500
epoch:3; metric:emoval; train:0.4465; eval:0.4540; lr:0.000500
epoch:4; metric:emoval; train:0.5512; eval:0.5521; lr:0.000500
epoch:5; metric:emoval; train:0.6188; eval:0.4292; lr:0.000500
epoch:6; metric:emoval; train:0.6996; eval:0.5184; lr:0.000500
epoch:7; metric:emoval; train:0.6919; eval:0.5135; lr:0.000500
epoch:8; metric:emoval; train:0.7686; eval:0.5508; lr:0.000500
epoch:9; metric:emoval; train:0.7887; eval:0.4937; lr:0.000500
epoch:10; metric:emoval; train:0.7995; eval:0.5111; lr:0.000500
epoch:11; metric:emoval; train:0.8341; eval:0.4931; lr:0.000500
epoch:12; metric:emoval; train:0.8492; eval:0.5331; lr:0.000500
epoch:13; metric:emoval; train:0.8475; eval:0.5180; lr:0.000500
epoch:14; metric:emoval; train:0.8539; eval:0.5149; lr:0.000500
epoch:15; metric:emoval; train:0.8553; eval:0.5147; lr:0.000250
epoch:16; metric:emoval; train:0.9193; eval:0.5542; lr:0.000250
epoch:17; metric:emoval; train:0.9353; eval:0.5521; lr:0.000250
epoch:18; metric:emoval; train:0.9429; eval:0.5193; lr:0.000250
epoch:19; metric:emoval; train:0.9512; eval:0.5314; lr:0.000250
epoch:20; metric:emoval; train:0.9494; eval:0.5325; lr:0.000250
epoch:21; metric:emoval; train:0.9452; eval:0.5356; lr:0.000250
epoch:22; metric:emoval; train:0.9426; eval:0.4809; lr:0.000250
epoch:23; metric:emoval; train:0.9343; eval:0.5254; lr:0.000250
epoch:24; metric:emoval; train:0.9337; eval:0.5151; lr:0.000250
epoch:25; metric:emoval; train:0.9076; eval:0.4996; lr:0.000250
epoch:26; metric:emoval; train:0.9163; eval:0.5352; lr:0.000250
epoch:27; metric:emoval; train:0.8922; eval:0.5438; lr:0.000125
epoch:28; metric:emoval; train:0.9194; eval:0.5685; lr:0.000125
epoch:29; metric:emoval; train:0.9296; eval:0.5433; lr:0.000125
epoch:30; metric:emoval; train:0.9293; eval:0.5300; lr:0.000125
epoch:31; metric:emoval; train:0.9226; eval:0.4993; lr:0.000125
epoch:32; metric:emoval; train:0.9317; eval:0.5348; lr:0.000125
epoch:33; metric:emoval; train:0.9304; eval:0.5164; lr:0.000125
epoch:34; metric:emoval; train:0.9281; eval:0.5541; lr:0.000125
epoch:35; metric:emoval; train:0.9251; eval:0.5236; lr:0.000125
epoch:36; metric:emoval; train:0.9131; eval:0.5193; lr:0.000125
epoch:37; metric:emoval; train:0.9195; eval:0.5219; lr:0.000125
epoch:38; metric:emoval; train:0.9175; eval:0.5022; lr:0.000125
epoch:39; metric:emoval; train:0.9101; eval:0.5079; lr:0.000063
epoch:40; metric:emoval; train:0.9145; eval:0.5133; lr:0.000063
epoch:41; metric:emoval; train:0.9298; eval:0.5449; lr:0.000063
epoch:42; metric:emoval; train:0.9174; eval:0.5171; lr:0.000063
epoch:43; metric:emoval; train:0.9257; eval:0.5257; lr:0.000063
epoch:44; metric:emoval; train:0.9244; eval:0.5243; lr:0.000063
epoch:45; metric:emoval; train:0.9242; eval:0.4908; lr:0.000063
epoch:46; metric:emoval; train:0.9280; eval:0.5226; lr:0.000063
epoch:47; metric:emoval; train:0.9205; eval:0.5261; lr:0.000063
epoch:48; metric:emoval; train:0.9253; eval:0.5094; lr:0.000063
epoch:49; metric:emoval; train:0.9245; eval:0.5032; lr:0.000063
epoch:50; metric:emoval; train:0.9254; eval:0.4893; lr:0.000031
epoch:51; metric:emoval; train:0.9214; eval:0.5106; lr:0.000031
epoch:52; metric:emoval; train:0.9337; eval:0.5124; lr:0.000031
epoch:53; metric:emoval; train:0.9420; eval:0.5160; lr:0.000031
epoch:54; metric:emoval; train:0.9350; eval:0.5193; lr:0.000031
epoch:55; metric:emoval; train:0.9364; eval:0.5135; lr:0.000031
epoch:56; metric:emoval; train:0.9379; eval:0.4896; lr:0.000031
epoch:57; metric:emoval; train:0.9368; eval:0.5119; lr:0.000031
epoch:58; metric:emoval; train:0.9318; eval:0.4984; lr:0.000031
Early stopping at epoch 58, best epoch: 28
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 27, duration: 3627.7977454662323 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3550; eval:0.1308; lr:0.000500
epoch:2; metric:emoval; train:0.1086; eval:0.2969; lr:0.000500
epoch:3; metric:emoval; train:0.3617; eval:0.4944; lr:0.000500
epoch:4; metric:emoval; train:0.5436; eval:0.4907; lr:0.000500
epoch:5; metric:emoval; train:0.6102; eval:0.5285; lr:0.000500
epoch:6; metric:emoval; train:0.6572; eval:0.4875; lr:0.000500
epoch:7; metric:emoval; train:0.7143; eval:0.5112; lr:0.000500
epoch:8; metric:emoval; train:0.7334; eval:0.5225; lr:0.000500
epoch:9; metric:emoval; train:0.7517; eval:0.5046; lr:0.000500
epoch:10; metric:emoval; train:0.7874; eval:0.5116; lr:0.000500
epoch:11; metric:emoval; train:0.8026; eval:0.5136; lr:0.000500
epoch:12; metric:emoval; train:0.8291; eval:0.5435; lr:0.000500
epoch:13; metric:emoval; train:0.8493; eval:0.5168; lr:0.000500
epoch:14; metric:emoval; train:0.8380; eval:0.5534; lr:0.000500
epoch:15; metric:emoval; train:0.8651; eval:0.4888; lr:0.000500
epoch:16; metric:emoval; train:0.8722; eval:0.4144; lr:0.000500
epoch:17; metric:emoval; train:0.8601; eval:0.5606; lr:0.000500
epoch:18; metric:emoval; train:0.8767; eval:0.4716; lr:0.000500
epoch:19; metric:emoval; train:0.8763; eval:0.3762; lr:0.000500
epoch:20; metric:emoval; train:0.8500; eval:0.4041; lr:0.000500
epoch:21; metric:emoval; train:0.8576; eval:0.5631; lr:0.000500
epoch:22; metric:emoval; train:0.8809; eval:0.5182; lr:0.000500
epoch:23; metric:emoval; train:0.8710; eval:0.5202; lr:0.000500
epoch:24; metric:emoval; train:0.8851; eval:0.5681; lr:0.000500
epoch:25; metric:emoval; train:0.8915; eval:0.5700; lr:0.000500
epoch:26; metric:emoval; train:0.8792; eval:0.5269; lr:0.000500
epoch:27; metric:emoval; train:0.8632; eval:0.4713; lr:0.000500
epoch:28; metric:emoval; train:0.9024; eval:0.5109; lr:0.000500
epoch:29; metric:emoval; train:0.8917; eval:0.5263; lr:0.000500
epoch:30; metric:emoval; train:0.8822; eval:0.5402; lr:0.000500
epoch:31; metric:emoval; train:0.8474; eval:0.5434; lr:0.000500
epoch:32; metric:emoval; train:0.8577; eval:0.4857; lr:0.000500
epoch:33; metric:emoval; train:0.8395; eval:0.5672; lr:0.000500
epoch:34; metric:emoval; train:0.8510; eval:0.4957; lr:0.000500
epoch:35; metric:emoval; train:0.8505; eval:0.5232; lr:0.000500
epoch:36; metric:emoval; train:0.8646; eval:0.4917; lr:0.000250
epoch:37; metric:emoval; train:0.8870; eval:0.5692; lr:0.000250
epoch:38; metric:emoval; train:0.9010; eval:0.5530; lr:0.000250
epoch:39; metric:emoval; train:0.8996; eval:0.5743; lr:0.000250
epoch:40; metric:emoval; train:0.9065; eval:0.5312; lr:0.000250
epoch:41; metric:emoval; train:0.8783; eval:0.5458; lr:0.000250
epoch:42; metric:emoval; train:0.8908; eval:0.5281; lr:0.000250
epoch:43; metric:emoval; train:0.8976; eval:0.5063; lr:0.000250
epoch:44; metric:emoval; train:0.8991; eval:0.5167; lr:0.000250
epoch:45; metric:emoval; train:0.8985; eval:0.5393; lr:0.000250
epoch:46; metric:emoval; train:0.9189; eval:0.5602; lr:0.000250
epoch:47; metric:emoval; train:0.9063; eval:0.5306; lr:0.000250
epoch:48; metric:emoval; train:0.9045; eval:0.5302; lr:0.000250
epoch:49; metric:emoval; train:0.9002; eval:0.5259; lr:0.000250
epoch:50; metric:emoval; train:0.8963; eval:0.5292; lr:0.000125
epoch:51; metric:emoval; train:0.9144; eval:0.5351; lr:0.000125
epoch:52; metric:emoval; train:0.9018; eval:0.5370; lr:0.000125
epoch:53; metric:emoval; train:0.9174; eval:0.5380; lr:0.000125
epoch:54; metric:emoval; train:0.9239; eval:0.5417; lr:0.000125
epoch:55; metric:emoval; train:0.9334; eval:0.5237; lr:0.000125
epoch:56; metric:emoval; train:0.9208; eval:0.5439; lr:0.000125
epoch:57; metric:emoval; train:0.9250; eval:0.5242; lr:0.000125
epoch:58; metric:emoval; train:0.9248; eval:0.5391; lr:0.000125
epoch:59; metric:emoval; train:0.9377; eval:0.5338; lr:0.000125
epoch:60; metric:emoval; train:0.9230; eval:0.5282; lr:0.000125
epoch:61; metric:emoval; train:0.9296; eval:0.4707; lr:0.000063
epoch:62; metric:emoval; train:0.9352; eval:0.5394; lr:0.000063
epoch:63; metric:emoval; train:0.9365; eval:0.5200; lr:0.000063
epoch:64; metric:emoval; train:0.9334; eval:0.5410; lr:0.000063
epoch:65; metric:emoval; train:0.9311; eval:0.5303; lr:0.000063
epoch:66; metric:emoval; train:0.9310; eval:0.5322; lr:0.000063
epoch:67; metric:emoval; train:0.9413; eval:0.5179; lr:0.000063
epoch:68; metric:emoval; train:0.9341; eval:0.5080; lr:0.000063
epoch:69; metric:emoval; train:0.9367; eval:0.5123; lr:0.000063
Early stopping at epoch 69, best epoch: 39
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 38, duration: 2927.3195514678955 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3483; eval:-0.1042; lr:0.000500
epoch:2; metric:emoval; train:0.1513; eval:0.4169; lr:0.000500
epoch:3; metric:emoval; train:0.4364; eval:0.4590; lr:0.000500
epoch:4; metric:emoval; train:0.5728; eval:0.5025; lr:0.000500
epoch:5; metric:emoval; train:0.6256; eval:0.5339; lr:0.000500
epoch:6; metric:emoval; train:0.6458; eval:0.4965; lr:0.000500
epoch:7; metric:emoval; train:0.7219; eval:0.4106; lr:0.000500
epoch:8; metric:emoval; train:0.7433; eval:0.5623; lr:0.000500
epoch:9; metric:emoval; train:0.7899; eval:0.4786; lr:0.000500
epoch:10; metric:emoval; train:0.7972; eval:0.5332; lr:0.000500
epoch:11; metric:emoval; train:0.8011; eval:0.5106; lr:0.000500
epoch:12; metric:emoval; train:0.8270; eval:0.5579; lr:0.000500
epoch:13; metric:emoval; train:0.8374; eval:0.4747; lr:0.000500
epoch:14; metric:emoval; train:0.8534; eval:0.5423; lr:0.000500
epoch:15; metric:emoval; train:0.8647; eval:0.5018; lr:0.000500
epoch:16; metric:emoval; train:0.8744; eval:0.5387; lr:0.000500
epoch:17; metric:emoval; train:0.8809; eval:0.4158; lr:0.000500
epoch:18; metric:emoval; train:0.8599; eval:0.3554; lr:0.000500
epoch:19; metric:emoval; train:0.8720; eval:0.5419; lr:0.000250
epoch:20; metric:emoval; train:0.9264; eval:0.5261; lr:0.000250
epoch:21; metric:emoval; train:0.9413; eval:0.5390; lr:0.000250
epoch:22; metric:emoval; train:0.9463; eval:0.5099; lr:0.000250
epoch:23; metric:emoval; train:0.9406; eval:0.5515; lr:0.000250
epoch:24; metric:emoval; train:0.9266; eval:0.5280; lr:0.000250
epoch:25; metric:emoval; train:0.9309; eval:0.5152; lr:0.000250
epoch:26; metric:emoval; train:0.9247; eval:0.4929; lr:0.000250
epoch:27; metric:emoval; train:0.9307; eval:0.5344; lr:0.000250
epoch:28; metric:emoval; train:0.9082; eval:0.4841; lr:0.000250
epoch:29; metric:emoval; train:0.9006; eval:0.5319; lr:0.000250
epoch:30; metric:emoval; train:0.8901; eval:0.5106; lr:0.000125
epoch:31; metric:emoval; train:0.9300; eval:0.5327; lr:0.000125
epoch:32; metric:emoval; train:0.9349; eval:0.5268; lr:0.000125
epoch:33; metric:emoval; train:0.9285; eval:0.5236; lr:0.000125
epoch:34; metric:emoval; train:0.9275; eval:0.5206; lr:0.000125
epoch:35; metric:emoval; train:0.9182; eval:0.5238; lr:0.000125
epoch:36; metric:emoval; train:0.9196; eval:0.5257; lr:0.000125
epoch:37; metric:emoval; train:0.9291; eval:0.5251; lr:0.000125
epoch:38; metric:emoval; train:0.9163; eval:0.5140; lr:0.000125
Early stopping at epoch 38, best epoch: 8
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 7, duration: 721.9298002719879 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2953; eval:-0.0808; lr:0.000500
epoch:2; metric:emoval; train:0.2265; eval:0.4935; lr:0.000500
epoch:3; metric:emoval; train:0.4451; eval:0.4282; lr:0.000500
epoch:4; metric:emoval; train:0.5808; eval:0.5181; lr:0.000500
epoch:5; metric:emoval; train:0.6384; eval:0.5558; lr:0.000500
epoch:6; metric:emoval; train:0.6714; eval:0.5141; lr:0.000500
epoch:7; metric:emoval; train:0.7140; eval:0.5013; lr:0.000500
epoch:8; metric:emoval; train:0.7417; eval:0.5245; lr:0.000500
epoch:9; metric:emoval; train:0.7337; eval:0.4534; lr:0.000500
epoch:10; metric:emoval; train:0.7900; eval:0.4645; lr:0.000500
epoch:11; metric:emoval; train:0.8194; eval:0.5202; lr:0.000500
epoch:12; metric:emoval; train:0.8111; eval:0.5702; lr:0.000500
epoch:13; metric:emoval; train:0.8610; eval:0.4958; lr:0.000500
epoch:14; metric:emoval; train:0.8363; eval:0.5601; lr:0.000500
epoch:15; metric:emoval; train:0.8786; eval:0.5292; lr:0.000500
epoch:16; metric:emoval; train:0.8864; eval:0.5531; lr:0.000500
epoch:17; metric:emoval; train:0.8578; eval:0.5449; lr:0.000500
epoch:18; metric:emoval; train:0.8736; eval:0.5477; lr:0.000500
epoch:19; metric:emoval; train:0.9008; eval:0.5340; lr:0.000500
epoch:20; metric:emoval; train:0.9000; eval:0.5127; lr:0.000500
epoch:21; metric:emoval; train:0.9025; eval:0.5573; lr:0.000500
epoch:22; metric:emoval; train:0.8805; eval:0.5555; lr:0.000500
epoch:23; metric:emoval; train:0.8997; eval:0.5030; lr:0.000250
epoch:24; metric:emoval; train:0.9291; eval:0.5374; lr:0.000250
epoch:25; metric:emoval; train:0.9371; eval:0.5555; lr:0.000250
epoch:26; metric:emoval; train:0.9376; eval:0.5607; lr:0.000250
epoch:27; metric:emoval; train:0.9297; eval:0.5468; lr:0.000250
epoch:28; metric:emoval; train:0.9370; eval:0.5556; lr:0.000250
epoch:29; metric:emoval; train:0.9236; eval:0.5148; lr:0.000250
epoch:30; metric:emoval; train:0.9149; eval:0.5144; lr:0.000250
epoch:31; metric:emoval; train:0.9108; eval:0.5509; lr:0.000250
epoch:32; metric:emoval; train:0.9295; eval:0.5216; lr:0.000250
epoch:33; metric:emoval; train:0.9166; eval:0.5474; lr:0.000250
epoch:34; metric:emoval; train:0.9142; eval:0.5347; lr:0.000125
epoch:35; metric:emoval; train:0.9192; eval:0.5519; lr:0.000125
epoch:36; metric:emoval; train:0.9320; eval:0.5467; lr:0.000125
epoch:37; metric:emoval; train:0.9332; eval:0.5425; lr:0.000125
epoch:38; metric:emoval; train:0.9267; eval:0.5517; lr:0.000125
epoch:39; metric:emoval; train:0.9197; eval:0.5463; lr:0.000125
epoch:40; metric:emoval; train:0.9227; eval:0.5402; lr:0.000125
epoch:41; metric:emoval; train:0.9056; eval:0.5446; lr:0.000125
epoch:42; metric:emoval; train:0.9180; eval:0.5266; lr:0.000125
Early stopping at epoch 42, best epoch: 12
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 11, duration: 338.0872049331665 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7372_acc:0.7385_val:0.6921_1770137640.5263186.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7996_acc:0.8005_val:0.6814_1770137640.5263186.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7562_acc:0.7621_val:0.6619_1770137640.5263186.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.8727_acc:0.8765_val:80.0885_1770137640.5263186.npz
