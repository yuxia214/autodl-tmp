====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.25, modality_dropout_warmup=20, model='attention_robust_v5', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s]  3%|▎         | 110/3373 [00:00<00:02, 1093.26it/s]  7%|▋         | 241/3373 [00:00<00:02, 1217.68it/s] 11%|█         | 371/3373 [00:00<00:02, 1254.99it/s] 15%|█▍        | 497/3373 [00:00<00:03, 912.22it/s]  19%|█▉        | 652/3373 [00:00<00:02, 1085.82it/s] 23%|██▎       | 771/3373 [00:00<00:02, 1107.12it/s] 26%|██▋       | 889/3373 [00:00<00:03, 718.88it/s]  29%|██▉       | 982/3373 [00:01<00:03, 755.26it/s] 32%|███▏      | 1074/3373 [00:01<00:04, 547.97it/s] 35%|███▍      | 1173/3373 [00:01<00:03, 629.17it/s] 37%|███▋      | 1254/3373 [00:01<00:03, 666.67it/s] 40%|███▉      | 1335/3373 [00:01<00:02, 690.38it/s] 42%|████▏     | 1415/3373 [00:02<00:04, 485.77it/s] 45%|████▌     | 1520/3373 [00:02<00:03, 593.43it/s] 48%|████▊     | 1616/3373 [00:02<00:02, 670.38it/s] 51%|█████     | 1707/3373 [00:02<00:02, 726.15it/s] 53%|█████▎    | 1792/3373 [00:02<00:02, 612.78it/s] 56%|█████▌    | 1885/3373 [00:02<00:02, 682.98it/s] 59%|█████▉    | 1985/3373 [00:02<00:01, 744.64it/s] 62%|██████▏   | 2083/3373 [00:02<00:01, 803.99it/s] 65%|██████▌   | 2204/3373 [00:02<00:01, 911.76it/s] 68%|██████▊   | 2302/3373 [00:03<00:01, 607.57it/s] 71%|███████   | 2402/3373 [00:03<00:01, 686.79it/s] 75%|███████▌  | 2532/3373 [00:03<00:01, 822.80it/s] 78%|███████▊  | 2630/3373 [00:03<00:00, 856.66it/s] 81%|████████  | 2734/3373 [00:03<00:00, 902.65it/s] 84%|████████▍ | 2834/3373 [00:03<00:00, 731.77it/s] 87%|████████▋ | 2919/3373 [00:03<00:00, 623.52it/s] 89%|████████▉ | 3001/3373 [00:04<00:00, 664.74it/s] 92%|█████████▏| 3115/3373 [00:04<00:00, 775.74it/s] 95%|█████████▌| 3211/3373 [00:04<00:00, 821.72it/s]100%|█████████▉| 3365/3373 [00:04<00:00, 1010.70it/s]100%|██████████| 3373/3373 [00:04<00:00, 766.78it/s] 
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s]  2%|▏         | 57/3373 [00:00<00:05, 567.23it/s]  4%|▍         | 132/3373 [00:00<00:07, 429.06it/s]  6%|▌         | 205/3373 [00:00<00:05, 531.75it/s]  8%|▊         | 267/3373 [00:00<00:05, 559.64it/s] 11%|█         | 363/3373 [00:00<00:04, 685.68it/s] 14%|█▎        | 463/3373 [00:00<00:03, 781.89it/s] 16%|█▌        | 545/3373 [00:00<00:04, 603.04it/s] 18%|█▊        | 613/3373 [00:01<00:05, 501.42it/s] 20%|█▉        | 671/3373 [00:01<00:05, 518.66it/s] 22%|██▏       | 748/3373 [00:01<00:04, 578.84it/s] 24%|██▍       | 823/3373 [00:01<00:04, 622.16it/s] 26%|██▋       | 890/3373 [00:01<00:03, 623.61it/s] 28%|██▊       | 956/3373 [00:01<00:04, 510.18it/s] 30%|███       | 1013/3373 [00:01<00:04, 522.28it/s] 32%|███▏      | 1096/3373 [00:01<00:03, 600.28it/s] 34%|███▍      | 1161/3373 [00:02<00:03, 603.46it/s] 36%|███▋      | 1231/3373 [00:02<00:03, 629.50it/s] 38%|███▊      | 1298/3373 [00:02<00:03, 639.75it/s] 40%|████      | 1364/3373 [00:02<00:03, 514.68it/s] 42%|████▏     | 1421/3373 [00:02<00:04, 420.08it/s] 45%|████▍     | 1507/3373 [00:02<00:03, 515.74it/s] 46%|████▋     | 1567/3373 [00:02<00:04, 435.34it/s] 48%|████▊     | 1632/3373 [00:02<00:03, 481.27it/s] 50%|█████     | 1703/3373 [00:03<00:03, 535.04it/s] 52%|█████▏    | 1763/3373 [00:03<00:04, 367.53it/s] 54%|█████▎    | 1811/3373 [00:03<00:04, 387.85it/s] 55%|█████▌    | 1859/3373 [00:03<00:03, 407.57it/s] 58%|█████▊    | 1945/3373 [00:03<00:02, 513.87it/s] 59%|█████▉    | 2005/3373 [00:03<00:03, 426.41it/s] 61%|██████    | 2056/3373 [00:04<00:02, 439.47it/s] 63%|██████▎   | 2135/3373 [00:04<00:02, 430.16it/s] 65%|██████▌   | 2207/3373 [00:04<00:02, 493.29it/s] 67%|██████▋   | 2262/3373 [00:04<00:02, 495.04it/s] 69%|██████▊   | 2316/3373 [00:04<00:02, 409.57it/s] 70%|███████   | 2362/3373 [00:04<00:02, 340.86it/s] 71%|███████▏  | 2404/3373 [00:04<00:03, 302.22it/s] 73%|███████▎  | 2461/3373 [00:05<00:02, 354.74it/s] 75%|███████▌  | 2532/3373 [00:05<00:01, 432.77it/s] 77%|███████▋  | 2582/3373 [00:05<00:01, 444.70it/s] 78%|███████▊  | 2632/3373 [00:05<00:01, 452.96it/s] 79%|███████▉  | 2681/3373 [00:05<00:01, 373.03it/s] 81%|████████  | 2738/3373 [00:05<00:01, 418.53it/s] 84%|████████▎ | 2821/3373 [00:05<00:01, 513.98it/s] 85%|████████▌ | 2883/3373 [00:05<00:00, 541.36it/s] 87%|████████▋ | 2943/3373 [00:06<00:00, 556.94it/s] 89%|████████▉ | 3002/3373 [00:06<00:00, 564.71it/s] 91%|█████████ | 3061/3373 [00:06<00:00, 444.19it/s] 93%|█████████▎| 3146/3373 [00:06<00:00, 541.29it/s] 95%|█████████▌| 3212/3373 [00:06<00:00, 463.23it/s] 98%|█████████▊| 3292/3373 [00:06<00:00, 539.10it/s]100%|██████████| 3373/3373 [00:06<00:00, 502.80it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s]  0%|          | 6/3373 [00:00<00:56, 59.52it/s]  2%|▏         | 63/3373 [00:00<00:19, 169.02it/s]  6%|▌         | 196/3373 [00:00<00:06, 496.96it/s]  8%|▊         | 265/3373 [00:00<00:07, 425.13it/s] 11%|█         | 363/3373 [00:00<00:06, 452.38it/s] 12%|█▏        | 420/3373 [00:00<00:06, 477.81it/s] 14%|█▍        | 476/3373 [00:01<00:07, 402.28it/s] 17%|█▋        | 582/3373 [00:01<00:06, 445.71it/s] 19%|█▉        | 637/3373 [00:01<00:05, 462.72it/s] 22%|██▏       | 743/3373 [00:01<00:04, 593.85it/s] 24%|██▍       | 810/3373 [00:01<00:06, 415.83it/s] 26%|██▌       | 864/3373 [00:02<00:07, 320.93it/s] 27%|██▋       | 907/3373 [00:02<00:08, 291.10it/s] 29%|██▉       | 981/3373 [00:02<00:06, 365.22it/s] 32%|███▏      | 1090/3373 [00:02<00:04, 500.37it/s] 34%|███▍      | 1155/3373 [00:02<00:04, 443.82it/s] 36%|███▌      | 1210/3373 [00:02<00:04, 461.31it/s] 38%|███▊      | 1265/3373 [00:03<00:05, 395.78it/s] 39%|███▉      | 1321/3373 [00:03<00:04, 428.64it/s] 41%|████      | 1371/3373 [00:03<00:04, 443.50it/s] 42%|████▏     | 1421/3373 [00:03<00:04, 457.14it/s] 44%|████▎     | 1471/3373 [00:03<00:07, 265.06it/s] 45%|████▍     | 1510/3373 [00:03<00:06, 283.54it/s] 47%|████▋     | 1598/3373 [00:04<00:05, 337.71it/s] 49%|████▉     | 1666/3373 [00:04<00:04, 397.98it/s] 52%|█████▏    | 1754/3373 [00:04<00:03, 499.83it/s] 54%|█████▍    | 1814/3373 [00:04<00:03, 425.39it/s] 55%|█████▌    | 1871/3373 [00:04<00:03, 378.90it/s] 57%|█████▋    | 1916/3373 [00:04<00:03, 388.27it/s] 59%|█████▉    | 1994/3373 [00:04<00:03, 395.91it/s] 60%|██████    | 2037/3373 [00:05<00:04, 333.55it/s] 62%|██████▏   | 2098/3373 [00:05<00:03, 387.54it/s] 65%|██████▍   | 2190/3373 [00:05<00:02, 412.53it/s] 66%|██████▋   | 2239/3373 [00:05<00:03, 357.47it/s] 68%|██████▊   | 2285/3373 [00:05<00:02, 376.22it/s] 69%|██████▉   | 2326/3373 [00:05<00:03, 314.23it/s] 71%|███████   | 2398/3373 [00:06<00:02, 396.19it/s] 72%|███████▏  | 2444/3373 [00:06<00:02, 334.44it/s] 75%|███████▍  | 2514/3373 [00:06<00:02, 409.23it/s] 77%|███████▋  | 2590/3373 [00:06<00:01, 488.95it/s] 78%|███████▊  | 2647/3373 [00:06<00:01, 507.18it/s] 80%|████████  | 2710/3373 [00:06<00:01, 434.29it/s] 83%|████████▎ | 2784/3373 [00:06<00:01, 503.40it/s] 84%|████████▍ | 2841/3373 [00:07<00:01, 348.09it/s] 86%|████████▌ | 2907/3373 [00:07<00:01, 340.72it/s] 90%|████████▉ | 3034/3373 [00:07<00:00, 433.68it/s] 92%|█████████▏| 3108/3373 [00:07<00:00, 482.77it/s] 95%|█████████▌| 3220/3373 [00:07<00:00, 519.15it/s] 97%|█████████▋| 3276/3373 [00:07<00:00, 523.90it/s] 99%|█████████▉| 3339/3373 [00:08<00:00, 546.62it/s]100%|██████████| 3373/3373 [00:08<00:00, 416.53it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]  0%|          | 1/411 [00:00<02:04,  3.30it/s]  9%|▊         | 35/411 [00:00<00:03, 111.10it/s] 14%|█▎        | 56/411 [00:00<00:02, 141.53it/s] 45%|████▍     | 184/411 [00:00<00:00, 491.91it/s] 79%|███████▉  | 326/411 [00:00<00:00, 773.34it/s]100%|██████████| 411/411 [00:00<00:00, 577.65it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]  4%|▍         | 18/411 [00:00<00:02, 172.18it/s] 14%|█▍        | 58/411 [00:00<00:01, 196.52it/s] 27%|██▋       | 111/411 [00:00<00:00, 308.34it/s] 37%|███▋      | 152/411 [00:00<00:00, 339.74it/s] 49%|████▉     | 202/411 [00:00<00:00, 390.51it/s] 64%|██████▍   | 264/411 [00:00<00:00, 360.73it/s] 74%|███████▎  | 303/411 [00:00<00:00, 364.37it/s] 90%|████████▉ | 369/411 [00:01<00:00, 438.48it/s]100%|██████████| 411/411 [00:01<00:00, 375.07it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s] 23%|██▎       | 93/411 [00:00<00:00, 487.78it/s] 54%|█████▎    | 220/411 [00:00<00:00, 819.27it/s] 96%|█████████▋| 396/411 [00:00<00:00, 1165.76it/s]100%|██████████| 411/411 [00:00<00:00, 1042.12it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s] 27%|██▋       | 112/412 [00:00<00:00, 1112.26it/s] 54%|█████▍    | 224/412 [00:00<00:00, 714.69it/s]  74%|███████▍  | 304/412 [00:00<00:00, 729.52it/s]100%|██████████| 412/412 [00:00<00:00, 824.98it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]  1%|          | 4/412 [00:00<00:10, 39.70it/s]  5%|▌         | 22/412 [00:00<00:03, 120.48it/s] 21%|██        | 85/412 [00:00<00:00, 350.90it/s] 37%|███▋      | 154/412 [00:00<00:00, 351.51it/s] 50%|█████     | 207/412 [00:00<00:00, 402.95it/s] 60%|██████    | 249/412 [00:00<00:00, 407.70it/s] 80%|████████  | 331/412 [00:00<00:00, 529.00it/s] 94%|█████████▎| 386/412 [00:00<00:00, 524.14it/s]100%|██████████| 412/412 [00:00<00:00, 412.83it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s] 16%|█▌        | 66/412 [00:00<00:00, 653.11it/s] 34%|███▎      | 139/412 [00:00<00:00, 453.50it/s] 53%|█████▎    | 220/412 [00:00<00:00, 427.63it/s] 65%|██████▍   | 266/412 [00:00<00:00, 429.69it/s]100%|██████████| 412/412 [00:00<00:00, 590.32it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]  0%|          | 3/834 [00:00<00:28, 29.17it/s]  5%|▍         | 38/834 [00:00<00:05, 137.28it/s] 17%|█▋        | 144/834 [00:00<00:01, 455.18it/s] 24%|██▍       | 199/834 [00:00<00:01, 481.85it/s] 30%|███       | 253/834 [00:00<00:01, 389.75it/s] 36%|███▌      | 302/834 [00:00<00:01, 414.72it/s] 50%|█████     | 419/834 [00:00<00:00, 618.50it/s] 59%|█████▉    | 490/834 [00:00<00:00, 642.72it/s] 68%|██████▊   | 571/834 [00:01<00:00, 682.53it/s] 82%|████████▏ | 688/834 [00:01<00:00, 821.73it/s] 98%|█████████▊| 821/834 [00:01<00:00, 968.54it/s]100%|██████████| 834/834 [00:01<00:00, 640.45it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]  3%|▎         | 26/834 [00:00<00:03, 259.11it/s] 11%|█         | 92/834 [00:00<00:01, 491.21it/s] 19%|█▉        | 157/834 [00:00<00:01, 562.04it/s] 26%|██▌       | 214/834 [00:00<00:01, 563.65it/s] 34%|███▍      | 284/834 [00:00<00:00, 612.34it/s] 45%|████▍     | 372/834 [00:00<00:00, 701.69it/s] 54%|█████▍    | 451/834 [00:00<00:00, 726.95it/s] 63%|██████▎   | 529/834 [00:00<00:00, 743.53it/s] 78%|███████▊  | 653/834 [00:00<00:00, 897.82it/s] 93%|█████████▎| 777/834 [00:01<00:00, 1002.86it/s]100%|██████████| 834/834 [00:01<00:00, 754.28it/s] 
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]  6%|▌         | 46/834 [00:00<00:03, 237.73it/s] 20%|█▉        | 163/834 [00:00<00:01, 628.96it/s] 33%|███▎      | 274/834 [00:00<00:00, 809.62it/s] 45%|████▍     | 374/834 [00:00<00:00, 871.15it/s] 56%|█████▌    | 469/834 [00:00<00:00, 881.59it/s] 69%|██████▉   | 574/834 [00:00<00:00, 934.69it/s] 87%|████████▋ | 722/834 [00:00<00:00, 1104.11it/s]100%|██████████| 834/834 [00:00<00:00, 926.08it/s] 
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3904; eval:-0.2959; lr:0.000500
epoch:2; metric:emoval; train:0.1187; eval:0.3294; lr:0.000500
epoch:3; metric:emoval; train:0.4371; eval:0.4488; lr:0.000500
epoch:4; metric:emoval; train:0.5597; eval:0.4285; lr:0.000500
epoch:5; metric:emoval; train:0.6323; eval:0.4827; lr:0.000500
epoch:6; metric:emoval; train:0.6769; eval:0.4708; lr:0.000500
epoch:7; metric:emoval; train:0.7209; eval:0.5320; lr:0.000500
epoch:8; metric:emoval; train:0.7389; eval:0.5077; lr:0.000500
epoch:9; metric:emoval; train:0.7934; eval:0.2304; lr:0.000500
epoch:10; metric:emoval; train:0.7939; eval:0.4852; lr:0.000500
epoch:11; metric:emoval; train:0.8189; eval:0.5307; lr:0.000500
epoch:12; metric:emoval; train:0.8439; eval:0.5092; lr:0.000500
epoch:13; metric:emoval; train:0.8476; eval:0.5494; lr:0.000500
epoch:14; metric:emoval; train:0.8716; eval:0.5105; lr:0.000500
epoch:15; metric:emoval; train:0.8751; eval:0.4877; lr:0.000500
epoch:16; metric:emoval; train:0.8917; eval:0.4752; lr:0.000500
epoch:17; metric:emoval; train:0.8328; eval:0.4267; lr:0.000500
epoch:18; metric:emoval; train:0.8730; eval:0.5173; lr:0.000500
epoch:19; metric:emoval; train:0.8728; eval:0.4886; lr:0.000500
epoch:20; metric:emoval; train:0.9033; eval:0.5391; lr:0.000500
epoch:21; metric:emoval; train:0.9028; eval:0.5334; lr:0.000500
epoch:22; metric:emoval; train:0.8933; eval:0.4581; lr:0.000500
epoch:23; metric:emoval; train:0.8704; eval:0.5143; lr:0.000500
epoch:24; metric:emoval; train:0.8476; eval:0.5034; lr:0.000250
epoch:25; metric:emoval; train:0.9139; eval:0.5080; lr:0.000250
epoch:26; metric:emoval; train:0.9222; eval:0.5276; lr:0.000250
epoch:27; metric:emoval; train:0.9054; eval:0.5038; lr:0.000250
epoch:28; metric:emoval; train:0.9158; eval:0.4935; lr:0.000250
epoch:29; metric:emoval; train:0.9167; eval:0.5201; lr:0.000250
epoch:30; metric:emoval; train:0.9088; eval:0.5234; lr:0.000250
epoch:31; metric:emoval; train:0.8920; eval:0.5295; lr:0.000250
epoch:32; metric:emoval; train:0.8804; eval:0.5325; lr:0.000250
epoch:33; metric:emoval; train:0.8862; eval:0.5188; lr:0.000250
epoch:34; metric:emoval; train:0.8822; eval:0.5195; lr:0.000250
epoch:35; metric:emoval; train:0.8917; eval:0.5213; lr:0.000125
epoch:36; metric:emoval; train:0.9079; eval:0.5287; lr:0.000125
epoch:37; metric:emoval; train:0.8993; eval:0.5175; lr:0.000125
epoch:38; metric:emoval; train:0.8918; eval:0.5247; lr:0.000125
epoch:39; metric:emoval; train:0.8805; eval:0.5191; lr:0.000125
epoch:40; metric:emoval; train:0.8776; eval:0.5267; lr:0.000125
epoch:41; metric:emoval; train:0.8975; eval:0.5352; lr:0.000125
epoch:42; metric:emoval; train:0.8804; eval:0.5061; lr:0.000125
epoch:43; metric:emoval; train:0.8825; eval:0.5071; lr:0.000125
Early stopping at epoch 43, best epoch: 13
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 12, duration: 2721.101263523102 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2446; eval:0.1482; lr:0.000500
epoch:2; metric:emoval; train:0.1943; eval:0.3643; lr:0.000500
epoch:3; metric:emoval; train:0.4557; eval:0.4549; lr:0.000500
epoch:4; metric:emoval; train:0.5284; eval:0.5289; lr:0.000500
epoch:5; metric:emoval; train:0.6286; eval:0.5026; lr:0.000500
epoch:6; metric:emoval; train:0.6633; eval:0.5716; lr:0.000500
epoch:7; metric:emoval; train:0.7157; eval:0.5667; lr:0.000500
epoch:8; metric:emoval; train:0.7399; eval:0.5702; lr:0.000500
epoch:9; metric:emoval; train:0.7648; eval:0.5082; lr:0.000500
epoch:10; metric:emoval; train:0.7851; eval:0.4101; lr:0.000500
epoch:11; metric:emoval; train:0.8184; eval:0.5219; lr:0.000500
epoch:12; metric:emoval; train:0.8177; eval:0.5518; lr:0.000500
epoch:13; metric:emoval; train:0.8291; eval:0.5693; lr:0.000500
epoch:14; metric:emoval; train:0.8462; eval:0.5879; lr:0.000500
epoch:15; metric:emoval; train:0.8391; eval:0.5179; lr:0.000500
epoch:16; metric:emoval; train:0.8639; eval:0.5637; lr:0.000500
epoch:17; metric:emoval; train:0.8945; eval:0.5297; lr:0.000500
epoch:18; metric:emoval; train:0.8971; eval:0.4642; lr:0.000500
epoch:19; metric:emoval; train:0.8925; eval:0.5388; lr:0.000500
epoch:20; metric:emoval; train:0.8452; eval:0.5619; lr:0.000500
epoch:21; metric:emoval; train:0.8612; eval:0.5442; lr:0.000500
epoch:22; metric:emoval; train:0.8602; eval:0.5319; lr:0.000500
epoch:23; metric:emoval; train:0.8712; eval:0.5282; lr:0.000500
epoch:24; metric:emoval; train:0.8461; eval:0.5762; lr:0.000500
epoch:25; metric:emoval; train:0.8852; eval:0.5194; lr:0.000250
epoch:26; metric:emoval; train:0.8876; eval:0.5484; lr:0.000250
epoch:27; metric:emoval; train:0.9053; eval:0.5487; lr:0.000250
epoch:28; metric:emoval; train:0.9095; eval:0.4832; lr:0.000250
epoch:29; metric:emoval; train:0.9023; eval:0.5309; lr:0.000250
epoch:30; metric:emoval; train:0.8911; eval:0.5552; lr:0.000250
epoch:31; metric:emoval; train:0.8998; eval:0.5595; lr:0.000250
epoch:32; metric:emoval; train:0.8878; eval:0.5353; lr:0.000250
epoch:33; metric:emoval; train:0.8876; eval:0.5551; lr:0.000250
epoch:34; metric:emoval; train:0.8820; eval:0.5832; lr:0.000250
epoch:35; metric:emoval; train:0.8770; eval:0.5542; lr:0.000250
epoch:36; metric:emoval; train:0.8569; eval:0.5645; lr:0.000125
epoch:37; metric:emoval; train:0.8931; eval:0.5594; lr:0.000125
epoch:38; metric:emoval; train:0.8934; eval:0.5646; lr:0.000125
epoch:39; metric:emoval; train:0.8887; eval:0.5382; lr:0.000125
epoch:40; metric:emoval; train:0.8831; eval:0.5623; lr:0.000125
epoch:41; metric:emoval; train:0.8771; eval:0.5265; lr:0.000125
epoch:42; metric:emoval; train:0.8942; eval:0.5714; lr:0.000125
epoch:43; metric:emoval; train:0.8893; eval:0.5773; lr:0.000125
epoch:44; metric:emoval; train:0.8894; eval:0.5645; lr:0.000125
Early stopping at epoch 44, best epoch: 14
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 13, duration: 2700.0042457580566 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1662; eval:0.1671; lr:0.000500
epoch:2; metric:emoval; train:0.1994; eval:0.2622; lr:0.000500
epoch:3; metric:emoval; train:0.4262; eval:0.4209; lr:0.000500
epoch:4; metric:emoval; train:0.5558; eval:0.5008; lr:0.000500
epoch:5; metric:emoval; train:0.6382; eval:0.3901; lr:0.000500
epoch:6; metric:emoval; train:0.6784; eval:0.3813; lr:0.000500
epoch:7; metric:emoval; train:0.7038; eval:0.5424; lr:0.000500
epoch:8; metric:emoval; train:0.7744; eval:0.5266; lr:0.000500
epoch:9; metric:emoval; train:0.7392; eval:0.4667; lr:0.000500
epoch:10; metric:emoval; train:0.7681; eval:0.5479; lr:0.000500
epoch:11; metric:emoval; train:0.8331; eval:0.5425; lr:0.000500
epoch:12; metric:emoval; train:0.8462; eval:0.4917; lr:0.000500
epoch:13; metric:emoval; train:0.8429; eval:0.4127; lr:0.000500
epoch:14; metric:emoval; train:0.8499; eval:0.5230; lr:0.000500
epoch:15; metric:emoval; train:0.8517; eval:0.4795; lr:0.000500
epoch:16; metric:emoval; train:0.8717; eval:0.4857; lr:0.000500
epoch:17; metric:emoval; train:0.8691; eval:0.4503; lr:0.000500
epoch:18; metric:emoval; train:0.8790; eval:0.4305; lr:0.000500
epoch:19; metric:emoval; train:0.8789; eval:0.4822; lr:0.000500
epoch:20; metric:emoval; train:0.8554; eval:0.4719; lr:0.000500
epoch:21; metric:emoval; train:0.9026; eval:0.4094; lr:0.000250
epoch:22; metric:emoval; train:0.9262; eval:0.4868; lr:0.000250
epoch:23; metric:emoval; train:0.9325; eval:0.4884; lr:0.000250
epoch:24; metric:emoval; train:0.9365; eval:0.5186; lr:0.000250
epoch:25; metric:emoval; train:0.9260; eval:0.4506; lr:0.000250
epoch:26; metric:emoval; train:0.9220; eval:0.4630; lr:0.000250
epoch:27; metric:emoval; train:0.8971; eval:0.4940; lr:0.000250
epoch:28; metric:emoval; train:0.9038; eval:0.4706; lr:0.000250
epoch:29; metric:emoval; train:0.9033; eval:0.5047; lr:0.000250
epoch:30; metric:emoval; train:0.8972; eval:0.4427; lr:0.000250
epoch:31; metric:emoval; train:0.8793; eval:0.4826; lr:0.000250
epoch:32; metric:emoval; train:0.8803; eval:0.4883; lr:0.000125
epoch:33; metric:emoval; train:0.9040; eval:0.5231; lr:0.000125
epoch:34; metric:emoval; train:0.8969; eval:0.5163; lr:0.000125
epoch:35; metric:emoval; train:0.8875; eval:0.5012; lr:0.000125
epoch:36; metric:emoval; train:0.8918; eval:0.5048; lr:0.000125
epoch:37; metric:emoval; train:0.9022; eval:0.5000; lr:0.000125
epoch:38; metric:emoval; train:0.8959; eval:0.4870; lr:0.000125
epoch:39; metric:emoval; train:0.8761; eval:0.5194; lr:0.000125
epoch:40; metric:emoval; train:0.8854; eval:0.5168; lr:0.000125
Early stopping at epoch 40, best epoch: 10
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 9, duration: 2547.9971759319305 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3902; eval:0.0597; lr:0.000500
epoch:2; metric:emoval; train:0.0989; eval:0.2407; lr:0.000500
epoch:3; metric:emoval; train:0.4039; eval:0.4760; lr:0.000500
epoch:4; metric:emoval; train:0.5235; eval:0.4475; lr:0.000500
epoch:5; metric:emoval; train:0.5913; eval:0.5077; lr:0.000500
epoch:6; metric:emoval; train:0.6478; eval:0.5428; lr:0.000500
epoch:7; metric:emoval; train:0.6826; eval:0.5057; lr:0.000500
epoch:8; metric:emoval; train:0.7370; eval:0.5570; lr:0.000500
epoch:9; metric:emoval; train:0.7355; eval:0.5532; lr:0.000500
epoch:10; metric:emoval; train:0.7703; eval:0.5224; lr:0.000500
epoch:11; metric:emoval; train:0.7978; eval:0.4727; lr:0.000500
epoch:12; metric:emoval; train:0.8155; eval:0.4879; lr:0.000500
epoch:13; metric:emoval; train:0.7978; eval:0.4617; lr:0.000500
epoch:14; metric:emoval; train:0.8290; eval:0.5388; lr:0.000500
epoch:15; metric:emoval; train:0.8653; eval:0.5379; lr:0.000500
epoch:16; metric:emoval; train:0.8894; eval:0.5266; lr:0.000500
epoch:17; metric:emoval; train:0.8808; eval:0.5333; lr:0.000500
epoch:18; metric:emoval; train:0.8770; eval:0.5536; lr:0.000500
epoch:19; metric:emoval; train:0.9048; eval:0.5771; lr:0.000500
epoch:20; metric:emoval; train:0.8719; eval:0.5258; lr:0.000500
epoch:21; metric:emoval; train:0.8812; eval:0.5211; lr:0.000500
epoch:22; metric:emoval; train:0.8980; eval:0.5676; lr:0.000500
epoch:23; metric:emoval; train:0.8783; eval:0.5207; lr:0.000500
epoch:24; metric:emoval; train:0.8968; eval:0.5274; lr:0.000500
epoch:25; metric:emoval; train:0.8684; eval:0.5309; lr:0.000500
epoch:26; metric:emoval; train:0.8561; eval:0.4716; lr:0.000500
epoch:27; metric:emoval; train:0.8464; eval:0.4336; lr:0.000500
epoch:28; metric:emoval; train:0.8396; eval:0.5187; lr:0.000500
epoch:29; metric:emoval; train:0.8369; eval:0.5328; lr:0.000500
epoch:30; metric:emoval; train:0.8356; eval:0.5043; lr:0.000250
epoch:31; metric:emoval; train:0.8557; eval:0.5674; lr:0.000250
epoch:32; metric:emoval; train:0.8860; eval:0.5445; lr:0.000250
epoch:33; metric:emoval; train:0.8698; eval:0.5278; lr:0.000250
epoch:34; metric:emoval; train:0.8891; eval:0.5365; lr:0.000250
epoch:35; metric:emoval; train:0.8713; eval:0.5542; lr:0.000250
epoch:36; metric:emoval; train:0.8770; eval:0.5358; lr:0.000250
epoch:37; metric:emoval; train:0.8768; eval:0.5111; lr:0.000250
epoch:38; metric:emoval; train:0.8830; eval:0.5535; lr:0.000250
epoch:39; metric:emoval; train:0.8609; eval:0.5517; lr:0.000250
epoch:40; metric:emoval; train:0.8723; eval:0.5366; lr:0.000250
epoch:41; metric:emoval; train:0.8552; eval:0.4965; lr:0.000125
epoch:42; metric:emoval; train:0.8760; eval:0.5383; lr:0.000125
epoch:43; metric:emoval; train:0.8831; eval:0.5689; lr:0.000125
epoch:44; metric:emoval; train:0.8786; eval:0.5456; lr:0.000125
epoch:45; metric:emoval; train:0.8810; eval:0.5405; lr:0.000125
epoch:46; metric:emoval; train:0.8843; eval:0.5405; lr:0.000125
epoch:47; metric:emoval; train:0.8896; eval:0.5624; lr:0.000125
epoch:48; metric:emoval; train:0.8947; eval:0.5045; lr:0.000125
epoch:49; metric:emoval; train:0.8889; eval:0.5515; lr:0.000125
Early stopping at epoch 49, best epoch: 19
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 18, duration: 2855.5007133483887 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3895; eval:-0.1098; lr:0.000500
epoch:2; metric:emoval; train:0.0832; eval:0.2371; lr:0.000500
epoch:3; metric:emoval; train:0.3990; eval:0.4339; lr:0.000500
epoch:4; metric:emoval; train:0.5034; eval:0.4708; lr:0.000500
epoch:5; metric:emoval; train:0.5889; eval:0.4869; lr:0.000500
epoch:6; metric:emoval; train:0.6333; eval:0.5150; lr:0.000500
epoch:7; metric:emoval; train:0.6982; eval:0.4500; lr:0.000500
epoch:8; metric:emoval; train:0.7061; eval:0.4846; lr:0.000500
epoch:9; metric:emoval; train:0.7494; eval:0.5756; lr:0.000500
epoch:10; metric:emoval; train:0.7717; eval:0.5084; lr:0.000500
epoch:11; metric:emoval; train:0.7984; eval:0.4607; lr:0.000500
epoch:12; metric:emoval; train:0.8203; eval:0.5126; lr:0.000500
epoch:13; metric:emoval; train:0.8267; eval:0.5743; lr:0.000500
epoch:14; metric:emoval; train:0.8328; eval:0.5203; lr:0.000500
epoch:15; metric:emoval; train:0.8629; eval:0.4815; lr:0.000500
epoch:16; metric:emoval; train:0.8590; eval:0.5214; lr:0.000500
epoch:17; metric:emoval; train:0.8318; eval:0.5221; lr:0.000500
epoch:18; metric:emoval; train:0.8724; eval:0.5344; lr:0.000500
epoch:19; metric:emoval; train:0.8747; eval:0.4861; lr:0.000500
epoch:20; metric:emoval; train:0.8919; eval:0.5489; lr:0.000250
epoch:21; metric:emoval; train:0.9344; eval:0.5501; lr:0.000250
epoch:22; metric:emoval; train:0.9342; eval:0.5545; lr:0.000250
epoch:23; metric:emoval; train:0.9217; eval:0.5678; lr:0.000250
epoch:24; metric:emoval; train:0.9220; eval:0.5481; lr:0.000250
epoch:25; metric:emoval; train:0.9188; eval:0.5458; lr:0.000250
epoch:26; metric:emoval; train:0.9138; eval:0.5254; lr:0.000250
epoch:27; metric:emoval; train:0.9024; eval:0.5526; lr:0.000250
epoch:28; metric:emoval; train:0.9034; eval:0.5367; lr:0.000250
epoch:29; metric:emoval; train:0.8608; eval:0.5081; lr:0.000250
epoch:30; metric:emoval; train:0.8774; eval:0.5635; lr:0.000250
epoch:31; metric:emoval; train:0.8898; eval:0.4962; lr:0.000125
epoch:32; metric:emoval; train:0.9004; eval:0.5217; lr:0.000125
epoch:33; metric:emoval; train:0.9011; eval:0.5649; lr:0.000125
epoch:34; metric:emoval; train:0.9006; eval:0.5559; lr:0.000125
epoch:35; metric:emoval; train:0.9102; eval:0.5281; lr:0.000125
epoch:36; metric:emoval; train:0.9005; eval:0.5342; lr:0.000125
epoch:37; metric:emoval; train:0.8908; eval:0.5324; lr:0.000125
epoch:38; metric:emoval; train:0.8867; eval:0.5530; lr:0.000125
epoch:39; metric:emoval; train:0.8990; eval:0.5368; lr:0.000125
Early stopping at epoch 39, best epoch: 9
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 8, duration: 1399.2027564048767 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7336_acc:0.7385_val:0.6641_1770135454.2951674.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7951_acc:0.7932_val:0.6647_1770135454.2951674.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7595_acc:0.7597_val:0.6155_1770135454.2951674.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.8904_acc:0.8933_val:78.1976_1770135454.2951674.npz
