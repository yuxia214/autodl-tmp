====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.3, modality_dropout_warmup=0, model='attention_robust_v5', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_mixup=False, use_modality_dropout=False, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 11%|█▏        | 382/3373 [00:00<00:00, 3779.82it/s] 23%|██▎       | 760/3373 [00:00<00:00, 3751.81it/s] 34%|███▎      | 1136/3373 [00:00<00:00, 3643.76it/s] 45%|████▍     | 1501/3373 [00:00<00:00, 2748.07it/s] 53%|█████▎    | 1800/3373 [00:00<00:00, 2782.51it/s] 63%|██████▎   | 2112/3373 [00:00<00:00, 2879.91it/s] 72%|███████▏  | 2413/3373 [00:00<00:00, 2288.98it/s] 79%|███████▉  | 2666/3373 [00:00<00:00, 2340.73it/s] 89%|████████▊ | 2993/3373 [00:01<00:00, 2500.58it/s] 97%|█████████▋| 3259/3373 [00:01<00:00, 2542.63it/s]100%|██████████| 3373/3373 [00:01<00:00, 2619.53it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s]  7%|▋         | 223/3373 [00:00<00:01, 2227.85it/s] 14%|█▍        | 483/3373 [00:00<00:01, 2443.80it/s] 22%|██▏       | 728/3373 [00:00<00:01, 1827.05it/s] 27%|██▋       | 925/3373 [00:00<00:01, 1802.01it/s] 33%|███▎      | 1114/3373 [00:00<00:01, 1416.92it/s] 38%|███▊      | 1270/3373 [00:00<00:01, 1390.30it/s] 42%|████▏     | 1418/3373 [00:00<00:01, 1162.11it/s] 47%|████▋     | 1570/3373 [00:01<00:01, 1245.23it/s] 51%|█████     | 1705/3373 [00:01<00:01, 1267.05it/s] 55%|█████▍    | 1852/3373 [00:01<00:01, 1320.14it/s] 59%|█████▉    | 2000/3373 [00:01<00:01, 1362.30it/s] 64%|██████▎   | 2149/3373 [00:01<00:00, 1395.01it/s] 69%|██████▊   | 2312/3373 [00:01<00:00, 1461.98it/s] 73%|███████▎  | 2473/3373 [00:01<00:00, 1503.80it/s] 78%|███████▊  | 2626/3373 [00:01<00:00, 1499.13it/s] 82%|████████▏ | 2778/3373 [00:01<00:00, 1492.89it/s] 87%|████████▋ | 2929/3373 [00:02<00:00, 1192.23it/s] 91%|█████████ | 3059/3373 [00:02<00:00, 1204.55it/s] 94%|█████████▍| 3187/3373 [00:02<00:00, 1218.91it/s] 98%|█████████▊| 3321/3373 [00:02<00:00, 1251.39it/s]100%|██████████| 3373/3373 [00:02<00:00, 1408.69it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s]  8%|▊         | 257/3373 [00:00<00:01, 2561.15it/s] 16%|█▌        | 543/3373 [00:00<00:01, 2702.89it/s] 25%|██▌       | 853/3373 [00:00<00:00, 2867.38it/s] 34%|███▍      | 1140/3373 [00:00<00:00, 2829.48it/s] 42%|████▏     | 1423/3373 [00:00<00:00, 2193.79it/s] 49%|████▉     | 1661/3373 [00:00<00:00, 2212.28it/s] 56%|█████▌    | 1895/3373 [00:00<00:00, 2209.65it/s] 64%|██████▎   | 2148/3373 [00:00<00:00, 2300.11it/s] 72%|███████▏  | 2416/3373 [00:01<00:00, 2404.60it/s] 79%|███████▉  | 2662/3373 [00:01<00:00, 2391.50it/s] 86%|████████▌ | 2905/3373 [00:01<00:00, 2373.33it/s] 97%|█████████▋| 3255/3373 [00:01<00:00, 2695.49it/s]100%|██████████| 3373/3373 [00:01<00:00, 2426.41it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s] 47%|████▋     | 195/411 [00:00<00:00, 1884.31it/s]100%|██████████| 411/411 [00:00<00:00, 3441.93it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s] 67%|██████▋   | 277/411 [00:00<00:00, 2761.80it/s]100%|██████████| 411/411 [00:00<00:00, 2333.44it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s] 49%|████▊     | 200/411 [00:00<00:00, 1999.78it/s]100%|██████████| 411/411 [00:00<00:00, 3610.61it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s] 64%|██████▎   | 262/412 [00:00<00:00, 1508.89it/s]100%|██████████| 412/412 [00:00<00:00, 2165.28it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s] 50%|████▉     | 205/412 [00:00<00:00, 2044.59it/s]100%|█████████▉| 410/412 [00:00<00:00, 1924.57it/s]100%|██████████| 412/412 [00:00<00:00, 1944.79it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s] 56%|█████▌    | 231/412 [00:00<00:00, 2307.92it/s]100%|██████████| 412/412 [00:00<00:00, 3592.02it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s] 26%|██▋       | 220/834 [00:00<00:00, 2196.19it/s] 62%|██████▏   | 521/834 [00:00<00:00, 2672.21it/s]100%|██████████| 834/834 [00:00<00:00, 2844.06it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s] 33%|███▎      | 273/834 [00:00<00:00, 2724.62it/s] 65%|██████▌   | 546/834 [00:00<00:00, 2677.97it/s]100%|██████████| 834/834 [00:00<00:00, 2795.36it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s] 33%|███▎      | 277/834 [00:00<00:00, 1778.48it/s] 97%|█████████▋| 813/834 [00:00<00:00, 3480.45it/s]100%|██████████| 834/834 [00:00<00:00, 3245.19it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3618; eval:0.0191; lr:0.000500
epoch:2; metric:emoval; train:0.1434; eval:0.4069; lr:0.000500
epoch:3; metric:emoval; train:0.4038; eval:0.5404; lr:0.000500
epoch:4; metric:emoval; train:0.5573; eval:0.4676; lr:0.000500
epoch:5; metric:emoval; train:0.5960; eval:0.4957; lr:0.000500
epoch:6; metric:emoval; train:0.6515; eval:0.5705; lr:0.000500
epoch:7; metric:emoval; train:0.7290; eval:0.4953; lr:0.000500
epoch:8; metric:emoval; train:0.7297; eval:0.5578; lr:0.000500
epoch:9; metric:emoval; train:0.7958; eval:0.4932; lr:0.000500
epoch:10; metric:emoval; train:0.7894; eval:0.5128; lr:0.000500
epoch:11; metric:emoval; train:0.8027; eval:0.5516; lr:0.000500
epoch:12; metric:emoval; train:0.8334; eval:0.5063; lr:0.000500
epoch:13; metric:emoval; train:0.8312; eval:0.5393; lr:0.000500
epoch:14; metric:emoval; train:0.8117; eval:0.5602; lr:0.000500
epoch:15; metric:emoval; train:0.8700; eval:0.5037; lr:0.000500
epoch:16; metric:emoval; train:0.8728; eval:0.5111; lr:0.000500
epoch:17; metric:emoval; train:0.8707; eval:0.4512; lr:0.000250
epoch:18; metric:emoval; train:0.9182; eval:0.5493; lr:0.000250
epoch:19; metric:emoval; train:0.9360; eval:0.5454; lr:0.000250
epoch:20; metric:emoval; train:0.9446; eval:0.5293; lr:0.000250
epoch:21; metric:emoval; train:0.9436; eval:0.5201; lr:0.000250
epoch:22; metric:emoval; train:0.9474; eval:0.4962; lr:0.000250
epoch:23; metric:emoval; train:0.9446; eval:0.5055; lr:0.000250
epoch:24; metric:emoval; train:0.9502; eval:0.5467; lr:0.000250
epoch:25; metric:emoval; train:0.9532; eval:0.5423; lr:0.000250
epoch:26; metric:emoval; train:0.9526; eval:0.5343; lr:0.000250
epoch:27; metric:emoval; train:0.9461; eval:0.5505; lr:0.000250
epoch:28; metric:emoval; train:0.9454; eval:0.5466; lr:0.000125
epoch:29; metric:emoval; train:0.9616; eval:0.5332; lr:0.000125
epoch:30; metric:emoval; train:0.9677; eval:0.5387; lr:0.000125
epoch:31; metric:emoval; train:0.9696; eval:0.5360; lr:0.000125
epoch:32; metric:emoval; train:0.9706; eval:0.5182; lr:0.000125
epoch:33; metric:emoval; train:0.9728; eval:0.4910; lr:0.000125
epoch:34; metric:emoval; train:0.9682; eval:0.5436; lr:0.000125
epoch:35; metric:emoval; train:0.9714; eval:0.5379; lr:0.000125
epoch:36; metric:emoval; train:0.9706; eval:0.5234; lr:0.000125
Early stopping at epoch 36, best epoch: 6
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 5, duration: 578.5069081783295 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2909; eval:0.1096; lr:0.000500
epoch:2; metric:emoval; train:0.2066; eval:0.3395; lr:0.000500
epoch:3; metric:emoval; train:0.4926; eval:0.3884; lr:0.000500
epoch:4; metric:emoval; train:0.5515; eval:0.4589; lr:0.000500
epoch:5; metric:emoval; train:0.6242; eval:0.4211; lr:0.000500
epoch:6; metric:emoval; train:0.6661; eval:0.4602; lr:0.000500
epoch:7; metric:emoval; train:0.7183; eval:0.4227; lr:0.000500
epoch:8; metric:emoval; train:0.7197; eval:0.4039; lr:0.000500
epoch:9; metric:emoval; train:0.7542; eval:0.4695; lr:0.000500
epoch:10; metric:emoval; train:0.7837; eval:0.5062; lr:0.000500
epoch:11; metric:emoval; train:0.8137; eval:0.4271; lr:0.000500
epoch:12; metric:emoval; train:0.8298; eval:0.4926; lr:0.000500
epoch:13; metric:emoval; train:0.8243; eval:0.4592; lr:0.000500
epoch:14; metric:emoval; train:0.8508; eval:0.4296; lr:0.000500
epoch:15; metric:emoval; train:0.8638; eval:0.4816; lr:0.000500
epoch:16; metric:emoval; train:0.8693; eval:0.5127; lr:0.000500
epoch:17; metric:emoval; train:0.8609; eval:0.4786; lr:0.000500
epoch:18; metric:emoval; train:0.8823; eval:0.4560; lr:0.000500
epoch:19; metric:emoval; train:0.8896; eval:0.4950; lr:0.000500
epoch:20; metric:emoval; train:0.8980; eval:0.4471; lr:0.000500
epoch:21; metric:emoval; train:0.9065; eval:0.4121; lr:0.000500
epoch:22; metric:emoval; train:0.8967; eval:0.3948; lr:0.000500
epoch:23; metric:emoval; train:0.9203; eval:0.4589; lr:0.000500
epoch:24; metric:emoval; train:0.9241; eval:0.5078; lr:0.000500
epoch:25; metric:emoval; train:0.9203; eval:0.4934; lr:0.000500
epoch:26; metric:emoval; train:0.8926; eval:0.4717; lr:0.000500
epoch:27; metric:emoval; train:0.8973; eval:0.4727; lr:0.000250
epoch:28; metric:emoval; train:0.9448; eval:0.4821; lr:0.000250
epoch:29; metric:emoval; train:0.9587; eval:0.4673; lr:0.000250
epoch:30; metric:emoval; train:0.9651; eval:0.4703; lr:0.000250
epoch:31; metric:emoval; train:0.9665; eval:0.4664; lr:0.000250
epoch:32; metric:emoval; train:0.9654; eval:0.4739; lr:0.000250
epoch:33; metric:emoval; train:0.9693; eval:0.4765; lr:0.000250
epoch:34; metric:emoval; train:0.9691; eval:0.4705; lr:0.000250
epoch:35; metric:emoval; train:0.9682; eval:0.4401; lr:0.000250
epoch:36; metric:emoval; train:0.9701; eval:0.4699; lr:0.000250
epoch:37; metric:emoval; train:0.9705; eval:0.4682; lr:0.000250
epoch:38; metric:emoval; train:0.9716; eval:0.4615; lr:0.000125
epoch:39; metric:emoval; train:0.9744; eval:0.4711; lr:0.000125
epoch:40; metric:emoval; train:0.9743; eval:0.4719; lr:0.000125
epoch:41; metric:emoval; train:0.9773; eval:0.4793; lr:0.000125
epoch:42; metric:emoval; train:0.9765; eval:0.4593; lr:0.000125
epoch:43; metric:emoval; train:0.9783; eval:0.4617; lr:0.000125
epoch:44; metric:emoval; train:0.9787; eval:0.4697; lr:0.000125
epoch:45; metric:emoval; train:0.9788; eval:0.4760; lr:0.000125
epoch:46; metric:emoval; train:0.9783; eval:0.4710; lr:0.000125
Early stopping at epoch 46, best epoch: 16
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 15, duration: 2714.9984588623047 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3236; eval:-0.0364; lr:0.000500
epoch:2; metric:emoval; train:0.1530; eval:0.3793; lr:0.000500
epoch:3; metric:emoval; train:0.4567; eval:0.3488; lr:0.000500
epoch:4; metric:emoval; train:0.5624; eval:0.4760; lr:0.000500
epoch:5; metric:emoval; train:0.6436; eval:0.5341; lr:0.000500
epoch:6; metric:emoval; train:0.6579; eval:0.5348; lr:0.000500
epoch:7; metric:emoval; train:0.6875; eval:0.5217; lr:0.000500
epoch:8; metric:emoval; train:0.7353; eval:0.5126; lr:0.000500
epoch:9; metric:emoval; train:0.7598; eval:0.4860; lr:0.000500
epoch:10; metric:emoval; train:0.7885; eval:0.5555; lr:0.000500
epoch:11; metric:emoval; train:0.8141; eval:0.5302; lr:0.000500
epoch:12; metric:emoval; train:0.8033; eval:0.5483; lr:0.000500
epoch:13; metric:emoval; train:0.8260; eval:0.5631; lr:0.000500
epoch:14; metric:emoval; train:0.8563; eval:0.5433; lr:0.000500
epoch:15; metric:emoval; train:0.8523; eval:0.5348; lr:0.000500
epoch:16; metric:emoval; train:0.8701; eval:0.5068; lr:0.000500
epoch:17; metric:emoval; train:0.8882; eval:0.5366; lr:0.000500
epoch:18; metric:emoval; train:0.8754; eval:0.5286; lr:0.000500
epoch:19; metric:emoval; train:0.8649; eval:0.4878; lr:0.000500
epoch:20; metric:emoval; train:0.8880; eval:0.4609; lr:0.000500
epoch:21; metric:emoval; train:0.9087; eval:0.5046; lr:0.000500
epoch:22; metric:emoval; train:0.9166; eval:0.4652; lr:0.000500
epoch:23; metric:emoval; train:0.9027; eval:0.5613; lr:0.000500
epoch:24; metric:emoval; train:0.8946; eval:0.5040; lr:0.000250
epoch:25; metric:emoval; train:0.9288; eval:0.5299; lr:0.000250
epoch:26; metric:emoval; train:0.9536; eval:0.5368; lr:0.000250
epoch:27; metric:emoval; train:0.9622; eval:0.5322; lr:0.000250
epoch:28; metric:emoval; train:0.9654; eval:0.5291; lr:0.000250
epoch:29; metric:emoval; train:0.9669; eval:0.5199; lr:0.000250
epoch:30; metric:emoval; train:0.9625; eval:0.5340; lr:0.000250
epoch:31; metric:emoval; train:0.9659; eval:0.5256; lr:0.000250
epoch:32; metric:emoval; train:0.9666; eval:0.4966; lr:0.000250
epoch:33; metric:emoval; train:0.9677; eval:0.5349; lr:0.000250
epoch:34; metric:emoval; train:0.9594; eval:0.5345; lr:0.000250
epoch:35; metric:emoval; train:0.9681; eval:0.5209; lr:0.000125
epoch:36; metric:emoval; train:0.9730; eval:0.5280; lr:0.000125
epoch:37; metric:emoval; train:0.9765; eval:0.5270; lr:0.000125
epoch:38; metric:emoval; train:0.9758; eval:0.5314; lr:0.000125
epoch:39; metric:emoval; train:0.9776; eval:0.5337; lr:0.000125
epoch:40; metric:emoval; train:0.9793; eval:0.5318; lr:0.000125
epoch:41; metric:emoval; train:0.9783; eval:0.5362; lr:0.000125
epoch:42; metric:emoval; train:0.9784; eval:0.5170; lr:0.000125
epoch:43; metric:emoval; train:0.9784; eval:0.5257; lr:0.000125
Early stopping at epoch 43, best epoch: 13
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 12, duration: 2513.097573041916 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.4111; eval:-0.0377; lr:0.000500
epoch:2; metric:emoval; train:0.1295; eval:0.2989; lr:0.000500
epoch:3; metric:emoval; train:0.4315; eval:0.4562; lr:0.000500
epoch:4; metric:emoval; train:0.5570; eval:0.4773; lr:0.000500
epoch:5; metric:emoval; train:0.6379; eval:0.5167; lr:0.000500
epoch:6; metric:emoval; train:0.6628; eval:0.4502; lr:0.000500
epoch:7; metric:emoval; train:0.7190; eval:0.5222; lr:0.000500
epoch:8; metric:emoval; train:0.7485; eval:0.4916; lr:0.000500
epoch:9; metric:emoval; train:0.7870; eval:0.5358; lr:0.000500
epoch:10; metric:emoval; train:0.7717; eval:0.4557; lr:0.000500
epoch:11; metric:emoval; train:0.8163; eval:0.5005; lr:0.000500
epoch:12; metric:emoval; train:0.8539; eval:0.4564; lr:0.000500
epoch:13; metric:emoval; train:0.8636; eval:0.5408; lr:0.000500
epoch:14; metric:emoval; train:0.8359; eval:0.5405; lr:0.000500
epoch:15; metric:emoval; train:0.8646; eval:0.4893; lr:0.000500
epoch:16; metric:emoval; train:0.8534; eval:0.5130; lr:0.000500
epoch:17; metric:emoval; train:0.8901; eval:0.5315; lr:0.000500
epoch:18; metric:emoval; train:0.8916; eval:0.4393; lr:0.000500
epoch:19; metric:emoval; train:0.8827; eval:0.4833; lr:0.000500
epoch:20; metric:emoval; train:0.8905; eval:0.3973; lr:0.000500
epoch:21; metric:emoval; train:0.8993; eval:0.4879; lr:0.000500
epoch:22; metric:emoval; train:0.8632; eval:0.5371; lr:0.000500
epoch:23; metric:emoval; train:0.8724; eval:0.4709; lr:0.000500
epoch:24; metric:emoval; train:0.9017; eval:0.5148; lr:0.000250
epoch:25; metric:emoval; train:0.9398; eval:0.5499; lr:0.000250
epoch:26; metric:emoval; train:0.9534; eval:0.5211; lr:0.000250
epoch:27; metric:emoval; train:0.9573; eval:0.5118; lr:0.000250
epoch:28; metric:emoval; train:0.9567; eval:0.4955; lr:0.000250
epoch:29; metric:emoval; train:0.9621; eval:0.5373; lr:0.000250
epoch:30; metric:emoval; train:0.9595; eval:0.5076; lr:0.000250
epoch:31; metric:emoval; train:0.9663; eval:0.5303; lr:0.000250
epoch:32; metric:emoval; train:0.9660; eval:0.4799; lr:0.000250
epoch:33; metric:emoval; train:0.9631; eval:0.5072; lr:0.000250
epoch:34; metric:emoval; train:0.9603; eval:0.5104; lr:0.000250
epoch:35; metric:emoval; train:0.9639; eval:0.5105; lr:0.000250
epoch:36; metric:emoval; train:0.9654; eval:0.4983; lr:0.000125
epoch:37; metric:emoval; train:0.9713; eval:0.4881; lr:0.000125
epoch:38; metric:emoval; train:0.9734; eval:0.5062; lr:0.000125
epoch:39; metric:emoval; train:0.9752; eval:0.5012; lr:0.000125
epoch:40; metric:emoval; train:0.9754; eval:0.4958; lr:0.000125
epoch:41; metric:emoval; train:0.9733; eval:0.5006; lr:0.000125
epoch:42; metric:emoval; train:0.9750; eval:0.4900; lr:0.000125
epoch:43; metric:emoval; train:0.9761; eval:0.5057; lr:0.000125
epoch:44; metric:emoval; train:0.9769; eval:0.4577; lr:0.000125
epoch:45; metric:emoval; train:0.9759; eval:0.4873; lr:0.000125
epoch:46; metric:emoval; train:0.9783; eval:0.5126; lr:0.000125
epoch:47; metric:emoval; train:0.9762; eval:0.4974; lr:0.000063
epoch:48; metric:emoval; train:0.9783; eval:0.5107; lr:0.000063
epoch:49; metric:emoval; train:0.9799; eval:0.4875; lr:0.000063
epoch:50; metric:emoval; train:0.9804; eval:0.5165; lr:0.000063
epoch:51; metric:emoval; train:0.9810; eval:0.5032; lr:0.000063
epoch:52; metric:emoval; train:0.9806; eval:0.5160; lr:0.000063
epoch:53; metric:emoval; train:0.9813; eval:0.4965; lr:0.000063
epoch:54; metric:emoval; train:0.9819; eval:0.4868; lr:0.000063
epoch:55; metric:emoval; train:0.9824; eval:0.4861; lr:0.000063
Early stopping at epoch 55, best epoch: 25
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 24, duration: 3179.299369573593 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3030; eval:0.0306; lr:0.000500
epoch:2; metric:emoval; train:0.1990; eval:0.2611; lr:0.000500
epoch:3; metric:emoval; train:0.4558; eval:0.4093; lr:0.000500
epoch:4; metric:emoval; train:0.5700; eval:0.4833; lr:0.000500
epoch:5; metric:emoval; train:0.6429; eval:0.4961; lr:0.000500
epoch:6; metric:emoval; train:0.6816; eval:0.5172; lr:0.000500
epoch:7; metric:emoval; train:0.6938; eval:0.5504; lr:0.000500
epoch:8; metric:emoval; train:0.7594; eval:0.4738; lr:0.000500
epoch:9; metric:emoval; train:0.7710; eval:0.5267; lr:0.000500
epoch:10; metric:emoval; train:0.8092; eval:0.4673; lr:0.000500
epoch:11; metric:emoval; train:0.8293; eval:0.4668; lr:0.000500
epoch:12; metric:emoval; train:0.8291; eval:0.4921; lr:0.000500
epoch:13; metric:emoval; train:0.8348; eval:0.4664; lr:0.000500
epoch:14; metric:emoval; train:0.8340; eval:0.4618; lr:0.000500
epoch:15; metric:emoval; train:0.8266; eval:0.5031; lr:0.000500
epoch:16; metric:emoval; train:0.8526; eval:0.5011; lr:0.000500
epoch:17; metric:emoval; train:0.8687; eval:0.5320; lr:0.000500
epoch:18; metric:emoval; train:0.8635; eval:0.4909; lr:0.000250
epoch:19; metric:emoval; train:0.9111; eval:0.5466; lr:0.000250
epoch:20; metric:emoval; train:0.9381; eval:0.5251; lr:0.000250
epoch:21; metric:emoval; train:0.9466; eval:0.4995; lr:0.000250
epoch:22; metric:emoval; train:0.9386; eval:0.5353; lr:0.000250
epoch:23; metric:emoval; train:0.9460; eval:0.5026; lr:0.000250
epoch:24; metric:emoval; train:0.9457; eval:0.4864; lr:0.000250
epoch:25; metric:emoval; train:0.9462; eval:0.4951; lr:0.000250
epoch:26; metric:emoval; train:0.9548; eval:0.4840; lr:0.000250
epoch:27; metric:emoval; train:0.9509; eval:0.4986; lr:0.000250
epoch:28; metric:emoval; train:0.9589; eval:0.5102; lr:0.000250
epoch:29; metric:emoval; train:0.9612; eval:0.4872; lr:0.000125
epoch:30; metric:emoval; train:0.9664; eval:0.4845; lr:0.000125
epoch:31; metric:emoval; train:0.9714; eval:0.4899; lr:0.000125
epoch:32; metric:emoval; train:0.9719; eval:0.4778; lr:0.000125
epoch:33; metric:emoval; train:0.9709; eval:0.4650; lr:0.000125
epoch:34; metric:emoval; train:0.9730; eval:0.4923; lr:0.000125
epoch:35; metric:emoval; train:0.9715; eval:0.4863; lr:0.000125
epoch:36; metric:emoval; train:0.9740; eval:0.4866; lr:0.000125
epoch:37; metric:emoval; train:0.9750; eval:0.4768; lr:0.000125
Early stopping at epoch 37, best epoch: 7
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 6, duration: 1947.6986346244812 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7234_acc:0.7249_val:0.6963_1770133057.6027715.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7992_acc:0.8005_val:0.6620_1770133057.6027715.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7650_acc:0.7694_val:0.6260_1770133057.6027715.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.8863_acc:0.8885_val:79.7808_1770133057.6027715.npz
