====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v4', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=False, use_dynamic_kl=True, use_gated_fusion=False, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 23%|██▎       | 764/3373 [00:00<00:00, 7362.57it/s] 45%|████▍     | 1501/3373 [00:00<00:00, 6304.31it/s] 63%|██████▎   | 2140/3373 [00:00<00:00, 4606.33it/s] 78%|███████▊  | 2641/3373 [00:00<00:00, 4522.47it/s] 92%|█████████▏| 3116/3373 [00:00<00:00, 4507.98it/s]100%|██████████| 3373/3373 [00:00<00:00, 4748.25it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s] 14%|█▍        | 475/3373 [00:00<00:00, 4747.15it/s] 28%|██▊       | 950/3373 [00:00<00:00, 4524.94it/s] 42%|████▏     | 1404/3373 [00:00<00:00, 4517.18it/s] 55%|█████▌    | 1857/3373 [00:00<00:00, 4171.52it/s] 68%|██████▊   | 2278/3373 [00:00<00:00, 3889.01it/s] 79%|███████▉  | 2671/3373 [00:00<00:00, 3832.15it/s] 92%|█████████▏| 3094/3373 [00:00<00:00, 3553.20it/s]100%|██████████| 3373/3373 [00:00<00:00, 4096.15it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s] 12%|█▏        | 395/3373 [00:00<00:00, 3903.39it/s] 25%|██▍       | 835/3373 [00:00<00:00, 4193.01it/s] 41%|████▏     | 1392/3373 [00:00<00:00, 4793.90it/s] 55%|█████▌    | 1872/3373 [00:00<00:00, 4698.68it/s] 69%|██████▉   | 2343/3373 [00:00<00:00, 4614.70it/s] 90%|████████▉ | 3019/3373 [00:00<00:00, 5273.64it/s]100%|██████████| 3373/3373 [00:00<00:00, 5329.86it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 10098.82it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 10107.70it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 12942.76it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 10753.09it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 9901.81it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 6507.60it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 9093.97it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 9463.34it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 14776.47it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.0977; eval:0.2649; lr:0.000500
epoch:2; metric:emoval; train:0.2749; eval:0.3621; lr:0.000500
epoch:3; metric:emoval; train:0.4096; eval:0.4302; lr:0.000500
epoch:4; metric:emoval; train:0.4838; eval:0.5230; lr:0.000500
epoch:5; metric:emoval; train:0.5496; eval:0.4929; lr:0.000500
epoch:6; metric:emoval; train:0.5976; eval:0.5017; lr:0.000500
epoch:7; metric:emoval; train:0.6245; eval:0.5269; lr:0.000500
epoch:8; metric:emoval; train:0.6354; eval:0.5097; lr:0.000500
epoch:9; metric:emoval; train:0.6632; eval:0.4906; lr:0.000500
epoch:10; metric:emoval; train:0.6814; eval:0.5001; lr:0.000500
epoch:11; metric:emoval; train:0.7115; eval:0.5659; lr:0.000500
epoch:12; metric:emoval; train:0.7094; eval:0.5057; lr:0.000500
epoch:13; metric:emoval; train:0.7213; eval:0.5400; lr:0.000500
epoch:14; metric:emoval; train:0.7229; eval:0.5521; lr:0.000500
epoch:15; metric:emoval; train:0.7315; eval:0.5618; lr:0.000500
epoch:16; metric:emoval; train:0.7542; eval:0.5468; lr:0.000500
epoch:17; metric:emoval; train:0.7403; eval:0.5428; lr:0.000500
epoch:18; metric:emoval; train:0.7763; eval:0.5316; lr:0.000500
epoch:19; metric:emoval; train:0.7612; eval:0.5048; lr:0.000500
epoch:20; metric:emoval; train:0.7690; eval:0.5462; lr:0.000500
epoch:21; metric:emoval; train:0.7699; eval:0.5021; lr:0.000500
epoch:22; metric:emoval; train:0.7541; eval:0.5537; lr:0.000250
epoch:23; metric:emoval; train:0.8002; eval:0.5462; lr:0.000250
epoch:24; metric:emoval; train:0.8079; eval:0.5378; lr:0.000250
epoch:25; metric:emoval; train:0.8194; eval:0.5730; lr:0.000250
epoch:26; metric:emoval; train:0.8253; eval:0.5445; lr:0.000250
epoch:27; metric:emoval; train:0.8308; eval:0.5548; lr:0.000250
epoch:28; metric:emoval; train:0.8084; eval:0.5863; lr:0.000250
epoch:29; metric:emoval; train:0.8219; eval:0.5885; lr:0.000250
epoch:30; metric:emoval; train:0.8005; eval:0.5470; lr:0.000250
epoch:31; metric:emoval; train:0.8129; eval:0.5055; lr:0.000250
epoch:32; metric:emoval; train:0.8186; eval:0.5753; lr:0.000250
epoch:33; metric:emoval; train:0.8142; eval:0.5440; lr:0.000250
epoch:34; metric:emoval; train:0.8044; eval:0.5414; lr:0.000250
epoch:35; metric:emoval; train:0.8063; eval:0.5551; lr:0.000250
epoch:36; metric:emoval; train:0.7913; eval:0.5346; lr:0.000250
epoch:37; metric:emoval; train:0.7838; eval:0.5511; lr:0.000250
epoch:38; metric:emoval; train:0.7898; eval:0.5810; lr:0.000250
epoch:39; metric:emoval; train:0.7977; eval:0.5405; lr:0.000250
epoch:40; metric:emoval; train:0.7813; eval:0.5285; lr:0.000125
epoch:41; metric:emoval; train:0.8075; eval:0.5290; lr:0.000125
epoch:42; metric:emoval; train:0.8130; eval:0.5407; lr:0.000125
epoch:43; metric:emoval; train:0.8162; eval:0.5659; lr:0.000125
epoch:44; metric:emoval; train:0.8395; eval:0.5631; lr:0.000125
epoch:45; metric:emoval; train:0.8265; eval:0.5547; lr:0.000125
epoch:46; metric:emoval; train:0.8286; eval:0.5908; lr:0.000125
epoch:47; metric:emoval; train:0.8265; eval:0.5480; lr:0.000125
epoch:48; metric:emoval; train:0.8292; eval:0.5731; lr:0.000125
epoch:49; metric:emoval; train:0.8228; eval:0.5832; lr:0.000125
epoch:50; metric:emoval; train:0.8394; eval:0.5706; lr:0.000125
epoch:51; metric:emoval; train:0.8130; eval:0.5682; lr:0.000125
epoch:52; metric:emoval; train:0.8333; eval:0.5905; lr:0.000125
epoch:53; metric:emoval; train:0.8276; eval:0.5635; lr:0.000125
epoch:54; metric:emoval; train:0.8282; eval:0.5695; lr:0.000125
epoch:55; metric:emoval; train:0.8458; eval:0.5484; lr:0.000125
epoch:56; metric:emoval; train:0.8298; eval:0.5853; lr:0.000125
epoch:57; metric:emoval; train:0.8273; eval:0.5506; lr:0.000063
epoch:58; metric:emoval; train:0.8404; eval:0.5783; lr:0.000063
epoch:59; metric:emoval; train:0.8455; eval:0.5655; lr:0.000063
epoch:60; metric:emoval; train:0.8682; eval:0.5855; lr:0.000063
epoch:61; metric:emoval; train:0.8641; eval:0.5724; lr:0.000063
epoch:62; metric:emoval; train:0.8464; eval:0.5692; lr:0.000063
epoch:63; metric:emoval; train:0.8625; eval:0.5677; lr:0.000063
epoch:64; metric:emoval; train:0.8564; eval:0.5831; lr:0.000063
epoch:65; metric:emoval; train:0.8585; eval:0.5633; lr:0.000063
epoch:66; metric:emoval; train:0.8620; eval:0.5662; lr:0.000063
epoch:67; metric:emoval; train:0.8615; eval:0.5596; lr:0.000063
epoch:68; metric:emoval; train:0.8492; eval:0.5591; lr:0.000031
epoch:69; metric:emoval; train:0.8601; eval:0.5639; lr:0.000031
epoch:70; metric:emoval; train:0.8696; eval:0.5567; lr:0.000031
epoch:71; metric:emoval; train:0.8657; eval:0.5813; lr:0.000031
epoch:72; metric:emoval; train:0.8609; eval:0.5696; lr:0.000031
epoch:73; metric:emoval; train:0.8682; eval:0.5795; lr:0.000031
epoch:74; metric:emoval; train:0.8735; eval:0.5854; lr:0.000031
epoch:75; metric:emoval; train:0.8752; eval:0.5731; lr:0.000031
epoch:76; metric:emoval; train:0.8785; eval:0.5823; lr:0.000031
Early stopping at epoch 76, best epoch: 46
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 45, duration: 2741.543352365494 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1109; eval:0.1273; lr:0.000500
epoch:2; metric:emoval; train:0.2760; eval:0.4926; lr:0.000500
epoch:3; metric:emoval; train:0.3889; eval:0.5296; lr:0.000500
epoch:4; metric:emoval; train:0.4949; eval:0.5282; lr:0.000500
epoch:5; metric:emoval; train:0.5361; eval:0.5081; lr:0.000500
epoch:6; metric:emoval; train:0.5852; eval:0.5261; lr:0.000500
epoch:7; metric:emoval; train:0.6124; eval:0.5544; lr:0.000500
epoch:8; metric:emoval; train:0.6488; eval:0.5171; lr:0.000500
epoch:9; metric:emoval; train:0.6389; eval:0.5134; lr:0.000500
epoch:10; metric:emoval; train:0.6672; eval:0.4446; lr:0.000500
epoch:11; metric:emoval; train:0.6907; eval:0.5792; lr:0.000500
epoch:12; metric:emoval; train:0.7007; eval:0.5800; lr:0.000500
epoch:13; metric:emoval; train:0.7030; eval:0.5059; lr:0.000500
epoch:14; metric:emoval; train:0.7254; eval:0.5195; lr:0.000500
epoch:15; metric:emoval; train:0.7328; eval:0.5581; lr:0.000500
epoch:16; metric:emoval; train:0.7452; eval:0.5801; lr:0.000500
epoch:17; metric:emoval; train:0.7436; eval:0.5925; lr:0.000500
epoch:18; metric:emoval; train:0.7577; eval:0.5882; lr:0.000500
epoch:19; metric:emoval; train:0.7447; eval:0.4953; lr:0.000500
epoch:20; metric:emoval; train:0.7534; eval:0.6078; lr:0.000500
epoch:21; metric:emoval; train:0.7612; eval:0.4706; lr:0.000500
epoch:22; metric:emoval; train:0.7626; eval:0.6145; lr:0.000500
epoch:23; metric:emoval; train:0.7633; eval:0.5473; lr:0.000500
epoch:24; metric:emoval; train:0.7759; eval:0.5473; lr:0.000500
epoch:25; metric:emoval; train:0.7355; eval:0.5712; lr:0.000500
epoch:26; metric:emoval; train:0.7416; eval:0.6136; lr:0.000500
epoch:27; metric:emoval; train:0.7654; eval:0.6275; lr:0.000500
epoch:28; metric:emoval; train:0.7560; eval:0.5895; lr:0.000500
epoch:29; metric:emoval; train:0.7465; eval:0.6055; lr:0.000500
epoch:30; metric:emoval; train:0.7531; eval:0.5770; lr:0.000500
epoch:31; metric:emoval; train:0.7676; eval:0.5231; lr:0.000500
epoch:32; metric:emoval; train:0.7248; eval:0.5691; lr:0.000500
epoch:33; metric:emoval; train:0.7481; eval:0.5671; lr:0.000500
epoch:34; metric:emoval; train:0.7268; eval:0.5462; lr:0.000500
epoch:35; metric:emoval; train:0.7547; eval:0.5731; lr:0.000500
epoch:36; metric:emoval; train:0.7434; eval:0.5876; lr:0.000500
epoch:37; metric:emoval; train:0.7507; eval:0.5726; lr:0.000500
epoch:38; metric:emoval; train:0.7378; eval:0.5622; lr:0.000250
epoch:39; metric:emoval; train:0.7773; eval:0.6137; lr:0.000250
epoch:40; metric:emoval; train:0.7993; eval:0.5921; lr:0.000250
epoch:41; metric:emoval; train:0.7826; eval:0.6155; lr:0.000250
epoch:42; metric:emoval; train:0.8008; eval:0.6096; lr:0.000250
epoch:43; metric:emoval; train:0.7974; eval:0.5820; lr:0.000250
epoch:44; metric:emoval; train:0.7793; eval:0.5935; lr:0.000250
epoch:45; metric:emoval; train:0.7750; eval:0.5980; lr:0.000250
epoch:46; metric:emoval; train:0.7949; eval:0.5812; lr:0.000250
epoch:47; metric:emoval; train:0.7928; eval:0.5958; lr:0.000250
epoch:48; metric:emoval; train:0.8012; eval:0.5978; lr:0.000250
epoch:49; metric:emoval; train:0.8186; eval:0.5963; lr:0.000125
epoch:50; metric:emoval; train:0.8204; eval:0.6184; lr:0.000125
epoch:51; metric:emoval; train:0.8342; eval:0.6056; lr:0.000125
epoch:52; metric:emoval; train:0.8454; eval:0.5890; lr:0.000125
epoch:53; metric:emoval; train:0.8405; eval:0.5971; lr:0.000125
epoch:54; metric:emoval; train:0.8334; eval:0.5960; lr:0.000125
epoch:55; metric:emoval; train:0.8304; eval:0.6024; lr:0.000125
epoch:56; metric:emoval; train:0.8477; eval:0.6039; lr:0.000125
epoch:57; metric:emoval; train:0.8323; eval:0.5887; lr:0.000125
Early stopping at epoch 57, best epoch: 27
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 26, duration: 3414.699717283249 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1108; eval:0.3046; lr:0.000500
epoch:2; metric:emoval; train:0.2946; eval:0.1109; lr:0.000500
epoch:3; metric:emoval; train:0.4109; eval:0.5265; lr:0.000500
epoch:4; metric:emoval; train:0.5013; eval:0.5241; lr:0.000500
epoch:5; metric:emoval; train:0.5334; eval:0.4875; lr:0.000500
epoch:6; metric:emoval; train:0.5759; eval:0.5269; lr:0.000500
epoch:7; metric:emoval; train:0.5991; eval:0.5127; lr:0.000500
epoch:8; metric:emoval; train:0.6377; eval:0.5481; lr:0.000500
epoch:9; metric:emoval; train:0.6688; eval:0.4858; lr:0.000500
epoch:10; metric:emoval; train:0.6849; eval:0.5760; lr:0.000500
epoch:11; metric:emoval; train:0.7039; eval:0.5568; lr:0.000500
epoch:12; metric:emoval; train:0.7104; eval:0.5556; lr:0.000500
epoch:13; metric:emoval; train:0.7141; eval:0.5479; lr:0.000500
epoch:14; metric:emoval; train:0.7315; eval:0.5616; lr:0.000500
epoch:15; metric:emoval; train:0.7363; eval:0.5575; lr:0.000500
epoch:16; metric:emoval; train:0.7468; eval:0.4792; lr:0.000500
epoch:17; metric:emoval; train:0.7419; eval:0.5320; lr:0.000500
epoch:18; metric:emoval; train:0.7820; eval:0.5786; lr:0.000500
epoch:19; metric:emoval; train:0.7430; eval:0.5527; lr:0.000500
epoch:20; metric:emoval; train:0.7741; eval:0.5689; lr:0.000500
epoch:21; metric:emoval; train:0.7664; eval:0.5678; lr:0.000500
epoch:22; metric:emoval; train:0.7760; eval:0.5347; lr:0.000500
epoch:23; metric:emoval; train:0.7757; eval:0.4702; lr:0.000500
epoch:24; metric:emoval; train:0.7614; eval:0.5720; lr:0.000500
epoch:25; metric:emoval; train:0.7630; eval:0.5973; lr:0.000500
epoch:26; metric:emoval; train:0.7637; eval:0.5402; lr:0.000500
epoch:27; metric:emoval; train:0.7721; eval:0.5187; lr:0.000500
epoch:28; metric:emoval; train:0.7463; eval:0.5709; lr:0.000500
epoch:29; metric:emoval; train:0.7670; eval:0.5834; lr:0.000500
epoch:30; metric:emoval; train:0.7607; eval:0.5446; lr:0.000500
epoch:31; metric:emoval; train:0.7398; eval:0.5686; lr:0.000500
epoch:32; metric:emoval; train:0.7606; eval:0.5907; lr:0.000500
epoch:33; metric:emoval; train:0.7606; eval:0.5778; lr:0.000500
epoch:34; metric:emoval; train:0.7602; eval:0.5598; lr:0.000500
epoch:35; metric:emoval; train:0.7670; eval:0.5713; lr:0.000500
epoch:36; metric:emoval; train:0.7382; eval:0.4719; lr:0.000250
epoch:37; metric:emoval; train:0.7960; eval:0.5663; lr:0.000250
epoch:38; metric:emoval; train:0.7847; eval:0.5624; lr:0.000250
epoch:39; metric:emoval; train:0.8154; eval:0.6017; lr:0.000250
epoch:40; metric:emoval; train:0.7769; eval:0.5773; lr:0.000250
epoch:41; metric:emoval; train:0.8016; eval:0.5781; lr:0.000250
epoch:42; metric:emoval; train:0.7978; eval:0.5975; lr:0.000250
epoch:43; metric:emoval; train:0.7985; eval:0.5796; lr:0.000250
epoch:44; metric:emoval; train:0.7958; eval:0.6135; lr:0.000250
epoch:45; metric:emoval; train:0.8031; eval:0.5997; lr:0.000250
epoch:46; metric:emoval; train:0.8004; eval:0.5613; lr:0.000250
epoch:47; metric:emoval; train:0.7945; eval:0.5748; lr:0.000250
epoch:48; metric:emoval; train:0.8112; eval:0.5894; lr:0.000250
epoch:49; metric:emoval; train:0.7933; eval:0.5576; lr:0.000250
epoch:50; metric:emoval; train:0.8170; eval:0.6022; lr:0.000250
epoch:51; metric:emoval; train:0.8009; eval:0.5702; lr:0.000250
epoch:52; metric:emoval; train:0.7893; eval:0.5726; lr:0.000250
epoch:53; metric:emoval; train:0.8125; eval:0.6000; lr:0.000250
epoch:54; metric:emoval; train:0.8076; eval:0.5586; lr:0.000250
epoch:55; metric:emoval; train:0.8087; eval:0.5957; lr:0.000125
epoch:56; metric:emoval; train:0.8413; eval:0.6073; lr:0.000125
epoch:57; metric:emoval; train:0.8360; eval:0.5820; lr:0.000125
epoch:58; metric:emoval; train:0.8369; eval:0.5917; lr:0.000125
epoch:59; metric:emoval; train:0.8378; eval:0.5999; lr:0.000125
epoch:60; metric:emoval; train:0.8512; eval:0.5938; lr:0.000125
epoch:61; metric:emoval; train:0.8520; eval:0.5856; lr:0.000125
epoch:62; metric:emoval; train:0.8334; eval:0.5829; lr:0.000125
epoch:63; metric:emoval; train:0.8471; eval:0.6025; lr:0.000125
epoch:64; metric:emoval; train:0.8409; eval:0.6046; lr:0.000125
epoch:65; metric:emoval; train:0.8448; eval:0.5939; lr:0.000125
epoch:66; metric:emoval; train:0.8526; eval:0.6028; lr:0.000063
epoch:67; metric:emoval; train:0.8638; eval:0.6019; lr:0.000063
epoch:68; metric:emoval; train:0.8534; eval:0.5938; lr:0.000063
epoch:69; metric:emoval; train:0.8795; eval:0.5972; lr:0.000063
epoch:70; metric:emoval; train:0.8701; eval:0.5856; lr:0.000063
epoch:71; metric:emoval; train:0.8649; eval:0.5883; lr:0.000063
epoch:72; metric:emoval; train:0.8628; eval:0.5802; lr:0.000063
epoch:73; metric:emoval; train:0.8748; eval:0.6045; lr:0.000063
epoch:74; metric:emoval; train:0.8700; eval:0.6133; lr:0.000063
Early stopping at epoch 74, best epoch: 44
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 43, duration: 4414.601316690445 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.0782; eval:0.3174; lr:0.000500
epoch:2; metric:emoval; train:0.2718; eval:0.3887; lr:0.000500
epoch:3; metric:emoval; train:0.4076; eval:0.4868; lr:0.000500
epoch:4; metric:emoval; train:0.4819; eval:0.5036; lr:0.000500
epoch:5; metric:emoval; train:0.5319; eval:0.5456; lr:0.000500
epoch:6; metric:emoval; train:0.5632; eval:0.5346; lr:0.000500
epoch:7; metric:emoval; train:0.5987; eval:0.5411; lr:0.000500
epoch:8; metric:emoval; train:0.6480; eval:0.5458; lr:0.000500
epoch:9; metric:emoval; train:0.6487; eval:0.5584; lr:0.000500
epoch:10; metric:emoval; train:0.6725; eval:0.5895; lr:0.000500
epoch:11; metric:emoval; train:0.6880; eval:0.5890; lr:0.000500
epoch:12; metric:emoval; train:0.6851; eval:0.5927; lr:0.000500
epoch:13; metric:emoval; train:0.7186; eval:0.5824; lr:0.000500
epoch:14; metric:emoval; train:0.7144; eval:0.5655; lr:0.000500
epoch:15; metric:emoval; train:0.7373; eval:0.6033; lr:0.000500
epoch:16; metric:emoval; train:0.7319; eval:0.6156; lr:0.000500
epoch:17; metric:emoval; train:0.7557; eval:0.5725; lr:0.000500
epoch:18; metric:emoval; train:0.7566; eval:0.5735; lr:0.000500
epoch:19; metric:emoval; train:0.7630; eval:0.5876; lr:0.000500
epoch:20; metric:emoval; train:0.7592; eval:0.4174; lr:0.000500
epoch:21; metric:emoval; train:0.7440; eval:0.5728; lr:0.000500
epoch:22; metric:emoval; train:0.7624; eval:0.6014; lr:0.000500
epoch:23; metric:emoval; train:0.7700; eval:0.5931; lr:0.000500
epoch:24; metric:emoval; train:0.7552; eval:0.5722; lr:0.000500
epoch:25; metric:emoval; train:0.7695; eval:0.5800; lr:0.000500
epoch:26; metric:emoval; train:0.7560; eval:0.5748; lr:0.000500
epoch:27; metric:emoval; train:0.7650; eval:0.4985; lr:0.000250
epoch:28; metric:emoval; train:0.7928; eval:0.6233; lr:0.000250
epoch:29; metric:emoval; train:0.8125; eval:0.5767; lr:0.000250
epoch:30; metric:emoval; train:0.8029; eval:0.6102; lr:0.000250
epoch:31; metric:emoval; train:0.8119; eval:0.5983; lr:0.000250
epoch:32; metric:emoval; train:0.8093; eval:0.5947; lr:0.000250
epoch:33; metric:emoval; train:0.7870; eval:0.6084; lr:0.000250
epoch:34; metric:emoval; train:0.7962; eval:0.5901; lr:0.000250
epoch:35; metric:emoval; train:0.7984; eval:0.6147; lr:0.000250
epoch:36; metric:emoval; train:0.8210; eval:0.5882; lr:0.000250
epoch:37; metric:emoval; train:0.7989; eval:0.6047; lr:0.000250
epoch:38; metric:emoval; train:0.7733; eval:0.5237; lr:0.000250
epoch:39; metric:emoval; train:0.7943; eval:0.6149; lr:0.000125
epoch:40; metric:emoval; train:0.8098; eval:0.6129; lr:0.000125
epoch:41; metric:emoval; train:0.8174; eval:0.6091; lr:0.000125
epoch:42; metric:emoval; train:0.8221; eval:0.5787; lr:0.000125
epoch:43; metric:emoval; train:0.8166; eval:0.6095; lr:0.000125
epoch:44; metric:emoval; train:0.8212; eval:0.5875; lr:0.000125
epoch:45; metric:emoval; train:0.8179; eval:0.5863; lr:0.000125
epoch:46; metric:emoval; train:0.8327; eval:0.5906; lr:0.000125
epoch:47; metric:emoval; train:0.8394; eval:0.6139; lr:0.000125
epoch:48; metric:emoval; train:0.8308; eval:0.6315; lr:0.000125
epoch:49; metric:emoval; train:0.8245; eval:0.6037; lr:0.000125
epoch:50; metric:emoval; train:0.8218; eval:0.6044; lr:0.000125
epoch:51; metric:emoval; train:0.8175; eval:0.6125; lr:0.000125
epoch:52; metric:emoval; train:0.8301; eval:0.6136; lr:0.000125
epoch:53; metric:emoval; train:0.8293; eval:0.6163; lr:0.000125
epoch:54; metric:emoval; train:0.8378; eval:0.5922; lr:0.000125
epoch:55; metric:emoval; train:0.8407; eval:0.5970; lr:0.000125
epoch:56; metric:emoval; train:0.8319; eval:0.6033; lr:0.000125
epoch:57; metric:emoval; train:0.8346; eval:0.6109; lr:0.000125
epoch:58; metric:emoval; train:0.8412; eval:0.6152; lr:0.000125
epoch:59; metric:emoval; train:0.8383; eval:0.5919; lr:0.000063
epoch:60; metric:emoval; train:0.8471; eval:0.6375; lr:0.000063
epoch:61; metric:emoval; train:0.8407; eval:0.6215; lr:0.000063
epoch:62; metric:emoval; train:0.8519; eval:0.6206; lr:0.000063
epoch:63; metric:emoval; train:0.8509; eval:0.6020; lr:0.000063
epoch:64; metric:emoval; train:0.8621; eval:0.6044; lr:0.000063
epoch:65; metric:emoval; train:0.8469; eval:0.6427; lr:0.000063
epoch:66; metric:emoval; train:0.8659; eval:0.6284; lr:0.000063
epoch:67; metric:emoval; train:0.8616; eval:0.6034; lr:0.000063
epoch:68; metric:emoval; train:0.8473; eval:0.6283; lr:0.000063
epoch:69; metric:emoval; train:0.8580; eval:0.6049; lr:0.000063
epoch:70; metric:emoval; train:0.8609; eval:0.6153; lr:0.000063
epoch:71; metric:emoval; train:0.8683; eval:0.6225; lr:0.000063
epoch:72; metric:emoval; train:0.8596; eval:0.6101; lr:0.000063
epoch:73; metric:emoval; train:0.8586; eval:0.6113; lr:0.000063
epoch:74; metric:emoval; train:0.8618; eval:0.6283; lr:0.000063
epoch:75; metric:emoval; train:0.8572; eval:0.6164; lr:0.000063
epoch:76; metric:emoval; train:0.8707; eval:0.6132; lr:0.000031
epoch:77; metric:emoval; train:0.8694; eval:0.6251; lr:0.000031
epoch:78; metric:emoval; train:0.8652; eval:0.6173; lr:0.000031
epoch:79; metric:emoval; train:0.8723; eval:0.6263; lr:0.000031
epoch:80; metric:emoval; train:0.8691; eval:0.6266; lr:0.000031
epoch:81; metric:emoval; train:0.8747; eval:0.6286; lr:0.000031
epoch:82; metric:emoval; train:0.8686; eval:0.6253; lr:0.000031
epoch:83; metric:emoval; train:0.8767; eval:0.6298; lr:0.000031
epoch:84; metric:emoval; train:0.8681; eval:0.6264; lr:0.000031
epoch:85; metric:emoval; train:0.8674; eval:0.6304; lr:0.000031
epoch:86; metric:emoval; train:0.8747; eval:0.6245; lr:0.000031
epoch:87; metric:emoval; train:0.8731; eval:0.6179; lr:0.000016
epoch:88; metric:emoval; train:0.8765; eval:0.6158; lr:0.000016
epoch:89; metric:emoval; train:0.8797; eval:0.6202; lr:0.000016
epoch:90; metric:emoval; train:0.8736; eval:0.6269; lr:0.000016
epoch:91; metric:emoval; train:0.8798; eval:0.6226; lr:0.000016
epoch:92; metric:emoval; train:0.8780; eval:0.6275; lr:0.000016
epoch:93; metric:emoval; train:0.8702; eval:0.6217; lr:0.000016
epoch:94; metric:emoval; train:0.8806; eval:0.6137; lr:0.000016
epoch:95; metric:emoval; train:0.8908; eval:0.6247; lr:0.000016
Early stopping at epoch 95, best epoch: 65
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 64, duration: 2977.6930780410767 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1052; eval:0.2637; lr:0.000500
epoch:2; metric:emoval; train:0.3102; eval:0.4312; lr:0.000500
epoch:3; metric:emoval; train:0.4188; eval:0.4498; lr:0.000500
epoch:4; metric:emoval; train:0.4825; eval:0.4785; lr:0.000500
epoch:5; metric:emoval; train:0.5614; eval:0.4716; lr:0.000500
epoch:6; metric:emoval; train:0.5976; eval:0.5337; lr:0.000500
epoch:7; metric:emoval; train:0.6401; eval:0.5077; lr:0.000500
epoch:8; metric:emoval; train:0.6388; eval:0.5258; lr:0.000500
epoch:9; metric:emoval; train:0.6664; eval:0.5380; lr:0.000500
epoch:10; metric:emoval; train:0.6740; eval:0.5334; lr:0.000500
epoch:11; metric:emoval; train:0.6978; eval:0.5272; lr:0.000500
epoch:12; metric:emoval; train:0.7190; eval:0.5663; lr:0.000500
epoch:13; metric:emoval; train:0.7253; eval:0.5291; lr:0.000500
epoch:14; metric:emoval; train:0.7411; eval:0.5483; lr:0.000500
epoch:15; metric:emoval; train:0.7341; eval:0.4312; lr:0.000500
epoch:16; metric:emoval; train:0.7514; eval:0.5512; lr:0.000500
epoch:17; metric:emoval; train:0.7520; eval:0.5474; lr:0.000500
epoch:18; metric:emoval; train:0.7699; eval:0.5624; lr:0.000500
epoch:19; metric:emoval; train:0.7652; eval:0.5743; lr:0.000500
epoch:20; metric:emoval; train:0.7695; eval:0.5337; lr:0.000500
epoch:21; metric:emoval; train:0.7648; eval:0.5370; lr:0.000500
epoch:22; metric:emoval; train:0.7582; eval:0.5339; lr:0.000500
epoch:23; metric:emoval; train:0.7735; eval:0.4966; lr:0.000500
epoch:24; metric:emoval; train:0.7693; eval:0.5521; lr:0.000500
epoch:25; metric:emoval; train:0.7724; eval:0.5301; lr:0.000500
epoch:26; metric:emoval; train:0.7859; eval:0.4528; lr:0.000500
epoch:27; metric:emoval; train:0.7635; eval:0.5460; lr:0.000500
epoch:28; metric:emoval; train:0.7862; eval:0.5337; lr:0.000500
epoch:29; metric:emoval; train:0.7614; eval:0.5259; lr:0.000500
epoch:30; metric:emoval; train:0.7384; eval:0.5540; lr:0.000250
epoch:31; metric:emoval; train:0.8008; eval:0.5469; lr:0.000250
epoch:32; metric:emoval; train:0.8214; eval:0.6043; lr:0.000250
epoch:33; metric:emoval; train:0.8201; eval:0.5660; lr:0.000250
epoch:34; metric:emoval; train:0.7965; eval:0.5423; lr:0.000250
epoch:35; metric:emoval; train:0.8157; eval:0.5777; lr:0.000250
epoch:36; metric:emoval; train:0.8070; eval:0.5449; lr:0.000250
epoch:37; metric:emoval; train:0.7931; eval:0.5939; lr:0.000250
epoch:38; metric:emoval; train:0.8062; eval:0.5673; lr:0.000250
epoch:39; metric:emoval; train:0.7851; eval:0.5178; lr:0.000250
epoch:40; metric:emoval; train:0.7950; eval:0.5829; lr:0.000250
epoch:41; metric:emoval; train:0.8007; eval:0.5397; lr:0.000250
epoch:42; metric:emoval; train:0.8073; eval:0.5864; lr:0.000250
epoch:43; metric:emoval; train:0.7900; eval:0.5449; lr:0.000125
epoch:44; metric:emoval; train:0.8281; eval:0.5555; lr:0.000125
epoch:45; metric:emoval; train:0.8320; eval:0.5758; lr:0.000125
epoch:46; metric:emoval; train:0.8444; eval:0.5950; lr:0.000125
epoch:47; metric:emoval; train:0.8473; eval:0.5802; lr:0.000125
epoch:48; metric:emoval; train:0.8388; eval:0.5652; lr:0.000125
epoch:49; metric:emoval; train:0.8242; eval:0.5814; lr:0.000125
epoch:50; metric:emoval; train:0.8305; eval:0.5886; lr:0.000125
epoch:51; metric:emoval; train:0.8345; eval:0.5667; lr:0.000125
epoch:52; metric:emoval; train:0.8386; eval:0.6040; lr:0.000125
epoch:53; metric:emoval; train:0.8398; eval:0.5802; lr:0.000125
epoch:54; metric:emoval; train:0.8285; eval:0.5462; lr:0.000063
epoch:55; metric:emoval; train:0.8496; eval:0.5921; lr:0.000063
epoch:56; metric:emoval; train:0.8546; eval:0.5730; lr:0.000063
epoch:57; metric:emoval; train:0.8513; eval:0.5673; lr:0.000063
epoch:58; metric:emoval; train:0.8578; eval:0.5759; lr:0.000063
epoch:59; metric:emoval; train:0.8606; eval:0.5970; lr:0.000063
epoch:60; metric:emoval; train:0.8649; eval:0.5718; lr:0.000063
epoch:61; metric:emoval; train:0.8607; eval:0.5966; lr:0.000063
epoch:62; metric:emoval; train:0.8569; eval:0.5859; lr:0.000063
Early stopping at epoch 62, best epoch: 32
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 31, duration: 474.0158517360687 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.7618_acc:0.7625_val:0.5841_1770137570.297805.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.8223_acc:0.8224_val:0.6341_1770137570.297805.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.7527_acc:0.7549_val:0.6235_1770137570.297805.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.8879_acc:0.8885_val:80.5621_1770137570.297805.npz
