====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v4', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=False, use_dynamic_kl=True, use_gated_fusion=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s]  3%|▎         | 97/3373 [00:00<00:09, 333.65it/s]  7%|▋         | 232/3373 [00:00<00:04, 660.64it/s] 10%|█         | 339/3373 [00:00<00:03, 786.99it/s] 13%|█▎        | 437/3373 [00:00<00:03, 830.24it/s] 16%|█▌        | 533/3373 [00:00<00:05, 543.27it/s] 18%|█▊        | 611/3373 [00:01<00:04, 593.19it/s] 21%|██        | 716/3373 [00:01<00:03, 699.28it/s] 24%|██▎       | 801/3373 [00:01<00:05, 491.45it/s] 26%|██▌       | 875/3373 [00:01<00:04, 539.21it/s] 28%|██▊       | 944/3373 [00:01<00:05, 469.02it/s] 32%|███▏      | 1092/3373 [00:01<00:03, 571.66it/s] 36%|███▌      | 1198/3373 [00:01<00:03, 667.58it/s] 39%|███▊      | 1305/3373 [00:02<00:02, 754.80it/s] 41%|████      | 1391/3373 [00:02<00:02, 768.34it/s] 46%|████▌     | 1540/3373 [00:02<00:01, 949.43it/s] 49%|████▊     | 1644/3373 [00:02<00:01, 970.59it/s] 52%|█████▏    | 1749/3373 [00:02<00:01, 990.85it/s] 55%|█████▍    | 1854/3373 [00:02<00:01, 1004.84it/s] 58%|█████▊    | 1971/3373 [00:02<00:01, 1051.51it/s] 62%|██████▏   | 2079/3373 [00:02<00:01, 1055.37it/s] 65%|██████▌   | 2198/3373 [00:02<00:01, 1093.02it/s] 68%|██████▊   | 2309/3373 [00:03<00:01, 854.28it/s]  72%|███████▏  | 2435/3373 [00:03<00:00, 954.06it/s] 76%|███████▌  | 2552/3373 [00:03<00:00, 1009.86it/s] 80%|███████▉  | 2684/3373 [00:03<00:00, 1092.90it/s] 84%|████████▍ | 2840/3373 [00:03<00:00, 1222.87it/s] 88%|████████▊ | 2968/3373 [00:03<00:00, 981.67it/s]  91%|█████████▏| 3078/3373 [00:03<00:00, 989.22it/s] 97%|█████████▋| 3287/3373 [00:03<00:00, 1034.04it/s]100%|██████████| 3373/3373 [00:03<00:00, 844.13it/s] 
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s]  2%|▏         | 57/3373 [00:00<00:05, 569.89it/s]  3%|▎         | 114/3373 [00:00<00:05, 565.29it/s]  5%|▌         | 171/3373 [00:00<00:08, 390.67it/s]  6%|▋         | 215/3373 [00:00<00:07, 404.30it/s]  9%|▊         | 292/3373 [00:00<00:05, 515.58it/s] 11%|█▏        | 386/3373 [00:00<00:04, 642.88it/s] 14%|█▎        | 461/3373 [00:00<00:04, 674.68it/s] 16%|█▌        | 532/3373 [00:01<00:05, 526.69it/s] 18%|█▊        | 595/3373 [00:01<00:05, 551.45it/s] 20%|█▉        | 667/3373 [00:01<00:04, 595.23it/s] 22%|██▏       | 731/3373 [00:01<00:05, 486.71it/s] 24%|██▎       | 793/3373 [00:01<00:04, 518.22it/s] 25%|██▌       | 852/3373 [00:01<00:04, 536.18it/s] 27%|██▋       | 910/3373 [00:01<00:04, 546.13it/s] 29%|██▊       | 968/3373 [00:01<00:05, 437.53it/s] 30%|███       | 1017/3373 [00:02<00:05, 443.36it/s] 32%|███▏      | 1077/3373 [00:02<00:05, 393.05it/s] 33%|███▎      | 1121/3373 [00:02<00:05, 402.25it/s] 35%|███▍      | 1165/3373 [00:02<00:05, 411.14it/s] 36%|███▌      | 1219/3373 [00:02<00:04, 443.99it/s] 38%|███▊      | 1266/3373 [00:02<00:04, 443.11it/s] 40%|████      | 1353/3373 [00:02<00:03, 559.84it/s] 42%|████▏     | 1412/3373 [00:02<00:03, 563.79it/s] 44%|████▎     | 1470/3373 [00:02<00:04, 450.73it/s] 46%|████▌     | 1537/3373 [00:03<00:03, 504.16it/s] 47%|████▋     | 1592/3373 [00:03<00:04, 409.17it/s] 49%|████▊     | 1639/3373 [00:03<00:05, 292.51it/s] 52%|█████▏    | 1741/3373 [00:03<00:03, 425.32it/s] 54%|█████▍    | 1823/3373 [00:03<00:03, 506.69it/s] 56%|█████▌    | 1888/3373 [00:03<00:02, 536.16it/s] 58%|█████▊    | 1952/3373 [00:04<00:03, 378.70it/s] 59%|█████▉    | 2004/3373 [00:04<00:03, 399.61it/s] 61%|██████▏   | 2066/3373 [00:04<00:02, 445.60it/s] 63%|██████▎   | 2120/3373 [00:04<00:02, 466.06it/s] 64%|██████▍   | 2174/3373 [00:04<00:03, 391.40it/s] 66%|██████▌   | 2229/3373 [00:04<00:02, 425.47it/s] 68%|██████▊   | 2282/3373 [00:04<00:02, 450.33it/s] 69%|██████▉   | 2332/3373 [00:05<00:02, 461.68it/s] 71%|███████   | 2395/3373 [00:05<00:01, 506.37it/s] 73%|███████▎  | 2452/3373 [00:05<00:01, 522.54it/s] 75%|███████▍  | 2513/3373 [00:05<00:01, 547.15it/s] 76%|███████▋  | 2578/3373 [00:05<00:01, 459.16it/s] 79%|███████▉  | 2667/3373 [00:05<00:01, 560.80it/s] 81%|████████  | 2728/3373 [00:05<00:01, 567.95it/s] 83%|████████▎ | 2811/3373 [00:05<00:00, 638.16it/s] 85%|████████▌ | 2878/3373 [00:05<00:00, 644.97it/s] 87%|████████▋ | 2945/3373 [00:06<00:00, 647.59it/s] 89%|████████▉ | 3012/3373 [00:06<00:00, 516.08it/s] 91%|█████████ | 3069/3373 [00:06<00:00, 427.22it/s] 94%|█████████▍| 3177/3373 [00:06<00:00, 570.10it/s] 96%|█████████▌| 3246/3373 [00:06<00:00, 596.11it/s] 99%|█████████▉| 3332/3373 [00:06<00:00, 661.96it/s]100%|██████████| 3373/3373 [00:06<00:00, 503.20it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s]  0%|          | 4/3373 [00:00<02:43, 20.59it/s]  1%|          | 36/3373 [00:00<00:22, 147.19it/s]  4%|▍         | 136/3373 [00:00<00:06, 466.96it/s]  6%|▌         | 192/3373 [00:00<00:08, 375.28it/s]  7%|▋         | 238/3373 [00:00<00:07, 397.70it/s] 10%|▉         | 333/3373 [00:00<00:05, 547.27it/s] 12%|█▏        | 395/3373 [00:01<00:08, 362.25it/s] 15%|█▍        | 497/3373 [00:01<00:05, 497.26it/s] 18%|█▊        | 596/3373 [00:01<00:04, 608.30it/s] 20%|██        | 680/3373 [00:01<00:04, 660.83it/s] 22%|██▏       | 758/3373 [00:01<00:04, 550.26it/s] 25%|██▌       | 852/3373 [00:01<00:03, 638.39it/s] 27%|██▋       | 927/3373 [00:01<00:04, 539.75it/s] 29%|██▉       | 991/3373 [00:02<00:05, 452.98it/s] 31%|███       | 1049/3373 [00:02<00:05, 403.27it/s] 33%|███▎      | 1099/3373 [00:02<00:05, 420.35it/s] 35%|███▌      | 1190/3373 [00:02<00:04, 525.85it/s] 37%|███▋      | 1251/3373 [00:02<00:05, 374.22it/s] 39%|███▊      | 1300/3373 [00:02<00:06, 329.60it/s] 41%|████▏     | 1394/3373 [00:03<00:05, 377.60it/s] 43%|████▎     | 1458/3373 [00:03<00:04, 418.92it/s] 45%|████▌     | 1533/3373 [00:03<00:04, 410.05it/s] 47%|████▋     | 1585/3373 [00:03<00:04, 428.05it/s] 48%|████▊     | 1632/3373 [00:03<00:04, 360.11it/s] 50%|████▉     | 1684/3373 [00:03<00:04, 389.12it/s] 53%|█████▎    | 1777/3373 [00:03<00:03, 509.63it/s] 54%|█████▍    | 1835/3373 [00:04<00:04, 308.13it/s] 58%|█████▊    | 1950/3373 [00:04<00:03, 450.00it/s] 60%|█████▉    | 2016/3373 [00:04<00:02, 488.69it/s] 62%|██████▏   | 2082/3373 [00:04<00:02, 434.14it/s] 63%|██████▎   | 2138/3373 [00:04<00:02, 456.35it/s] 65%|██████▌   | 2194/3373 [00:05<00:02, 393.38it/s] 66%|██████▋   | 2242/3373 [00:05<00:02, 406.11it/s] 68%|██████▊   | 2289/3373 [00:05<00:03, 347.42it/s] 69%|██████▉   | 2329/3373 [00:05<00:03, 296.16it/s] 71%|███████▏  | 2407/3373 [00:05<00:02, 389.77it/s] 73%|███████▎  | 2454/3373 [00:05<00:02, 336.09it/s] 75%|███████▍  | 2526/3373 [00:05<00:02, 415.10it/s] 77%|███████▋  | 2591/3373 [00:06<00:02, 377.26it/s] 79%|███████▉  | 2679/3373 [00:06<00:01, 479.82it/s] 81%|████████  | 2736/3373 [00:06<00:01, 408.89it/s] 84%|████████▍ | 2829/3373 [00:06<00:01, 433.48it/s] 85%|████████▌ | 2878/3373 [00:06<00:01, 434.98it/s] 87%|████████▋ | 2926/3373 [00:06<00:01, 372.12it/s] 88%|████████▊ | 2967/3373 [00:07<00:01, 375.95it/s] 90%|████████▉ | 3023/3373 [00:07<00:01, 344.09it/s] 93%|█████████▎| 3129/3373 [00:07<00:00, 407.99it/s] 95%|█████████▍| 3203/3373 [00:07<00:00, 392.82it/s] 99%|█████████▉| 3343/3373 [00:07<00:00, 582.52it/s]100%|██████████| 3373/3373 [00:07<00:00, 432.58it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]  0%|          | 1/411 [00:00<02:00,  3.39it/s] 16%|█▌        | 65/411 [00:00<00:02, 159.50it/s] 50%|████▉     | 204/411 [00:00<00:00, 470.12it/s] 67%|██████▋   | 274/411 [00:00<00:00, 428.45it/s] 97%|█████████▋| 399/411 [00:00<00:00, 621.53it/s]100%|██████████| 411/411 [00:00<00:00, 458.99it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]  0%|          | 1/411 [00:00<01:21,  5.05it/s] 12%|█▏        | 49/411 [00:00<00:02, 140.60it/s] 31%|███       | 127/411 [00:00<00:00, 326.69it/s] 54%|█████▍    | 223/411 [00:00<00:00, 512.18it/s] 76%|███████▌  | 312/411 [00:00<00:00, 619.00it/s] 95%|█████████▌| 391/411 [00:00<00:00, 526.62it/s]100%|██████████| 411/411 [00:00<00:00, 454.30it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s] 36%|███▋      | 150/411 [00:00<00:00, 1497.51it/s] 74%|███████▍  | 305/411 [00:00<00:00, 1520.83it/s]100%|██████████| 411/411 [00:00<00:00, 1397.75it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s] 20%|██        | 83/412 [00:00<00:00, 829.81it/s] 61%|██████    | 251/412 [00:00<00:00, 874.64it/s]100%|██████████| 412/412 [00:00<00:00, 1376.08it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]  6%|▋         | 26/412 [00:00<00:01, 256.36it/s] 23%|██▎       | 93/412 [00:00<00:00, 322.91it/s] 38%|███▊      | 158/412 [00:00<00:00, 436.76it/s] 64%|██████▍   | 263/412 [00:00<00:00, 640.64it/s] 81%|████████  | 333/412 [00:00<00:00, 485.81it/s]100%|██████████| 412/412 [00:00<00:00, 516.57it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]  1%|          | 3/412 [00:00<00:13, 29.94it/s] 45%|████▌     | 186/412 [00:00<00:00, 705.36it/s] 59%|█████▉    | 244/412 [00:00<00:00, 648.17it/s] 85%|████████▍ | 350/412 [00:00<00:00, 597.84it/s]100%|██████████| 412/412 [00:00<00:00, 594.84it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]  0%|          | 1/834 [00:00<01:24,  9.83it/s] 10%|▉         | 82/834 [00:00<00:01, 476.35it/s] 17%|█▋        | 142/834 [00:00<00:01, 374.63it/s] 26%|██▋       | 220/834 [00:00<00:01, 500.80it/s] 37%|███▋      | 308/834 [00:00<00:00, 615.89it/s] 46%|████▌     | 381/834 [00:00<00:00, 647.46it/s] 56%|█████▌    | 464/834 [00:00<00:00, 700.61it/s] 71%|███████   | 591/834 [00:00<00:00, 871.65it/s] 84%|████████▍ | 701/834 [00:01<00:00, 934.66it/s]100%|██████████| 834/834 [00:01<00:00, 758.39it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]  3%|▎         | 23/834 [00:00<00:03, 229.33it/s]  6%|▌         | 46/834 [00:00<00:03, 222.59it/s] 13%|█▎        | 107/834 [00:00<00:02, 284.39it/s] 23%|██▎       | 193/834 [00:00<00:01, 465.66it/s] 31%|███▏      | 262/834 [00:00<00:01, 535.18it/s] 39%|███▉      | 328/834 [00:00<00:00, 567.97it/s] 51%|█████     | 426/834 [00:00<00:00, 691.77it/s] 61%|██████▏   | 511/834 [00:00<00:00, 739.40it/s] 71%|███████   | 588/834 [00:00<00:00, 743.17it/s] 80%|███████▉  | 664/834 [00:01<00:00, 736.96it/s] 89%|████████▊ | 739/834 [00:01<00:00, 586.04it/s] 97%|█████████▋| 810/834 [00:01<00:00, 617.07it/s]100%|██████████| 834/834 [00:01<00:00, 596.97it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]  7%|▋         | 62/834 [00:00<00:01, 616.63it/s] 24%|██▍       | 202/834 [00:00<00:00, 1073.45it/s] 38%|███▊      | 314/834 [00:00<00:00, 1090.28it/s] 51%|█████     | 424/834 [00:00<00:00, 793.12it/s]  75%|███████▍  | 625/834 [00:00<00:00, 1143.90it/s] 90%|█████████ | 754/834 [00:00<00:00, 1175.80it/s]100%|██████████| 834/834 [00:00<00:00, 1178.22it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.0971; eval:0.3774; lr:0.000500
epoch:2; metric:emoval; train:0.3003; eval:0.3228; lr:0.000500
epoch:3; metric:emoval; train:0.4169; eval:0.5309; lr:0.000500
epoch:4; metric:emoval; train:0.4925; eval:0.5546; lr:0.000500
epoch:5; metric:emoval; train:0.5486; eval:0.5450; lr:0.000500
epoch:6; metric:emoval; train:0.5975; eval:0.5055; lr:0.000500
epoch:7; metric:emoval; train:0.6340; eval:0.5525; lr:0.000500
epoch:8; metric:emoval; train:0.6734; eval:0.5469; lr:0.000500
epoch:9; metric:emoval; train:0.6738; eval:0.5864; lr:0.000500
epoch:10; metric:emoval; train:0.6908; eval:0.5829; lr:0.000500
epoch:11; metric:emoval; train:0.7081; eval:0.5767; lr:0.000500
epoch:12; metric:emoval; train:0.7128; eval:0.5830; lr:0.000500
epoch:13; metric:emoval; train:0.7259; eval:0.5685; lr:0.000500
epoch:14; metric:emoval; train:0.7475; eval:0.6077; lr:0.000500
epoch:15; metric:emoval; train:0.7446; eval:0.5818; lr:0.000500
epoch:16; metric:emoval; train:0.7534; eval:0.6010; lr:0.000500
epoch:17; metric:emoval; train:0.7603; eval:0.5240; lr:0.000500
epoch:18; metric:emoval; train:0.7655; eval:0.6002; lr:0.000500
epoch:19; metric:emoval; train:0.7553; eval:0.5893; lr:0.000500
epoch:20; metric:emoval; train:0.7689; eval:0.5622; lr:0.000500
epoch:21; metric:emoval; train:0.7788; eval:0.5744; lr:0.000500
epoch:22; metric:emoval; train:0.7820; eval:0.5608; lr:0.000500
epoch:23; metric:emoval; train:0.7830; eval:0.5710; lr:0.000500
epoch:24; metric:emoval; train:0.7654; eval:0.5821; lr:0.000500
epoch:25; metric:emoval; train:0.7695; eval:0.5631; lr:0.000250
epoch:26; metric:emoval; train:0.8186; eval:0.5548; lr:0.000250
epoch:27; metric:emoval; train:0.7961; eval:0.6046; lr:0.000250
epoch:28; metric:emoval; train:0.8188; eval:0.5973; lr:0.000250
epoch:29; metric:emoval; train:0.8184; eval:0.6139; lr:0.000250
epoch:30; metric:emoval; train:0.8033; eval:0.5646; lr:0.000250
epoch:31; metric:emoval; train:0.8146; eval:0.5980; lr:0.000250
epoch:32; metric:emoval; train:0.7994; eval:0.5694; lr:0.000250
epoch:33; metric:emoval; train:0.7926; eval:0.6056; lr:0.000250
epoch:34; metric:emoval; train:0.8006; eval:0.6010; lr:0.000250
epoch:35; metric:emoval; train:0.7961; eval:0.6121; lr:0.000250
epoch:36; metric:emoval; train:0.8094; eval:0.5766; lr:0.000250
epoch:37; metric:emoval; train:0.7805; eval:0.6230; lr:0.000250
epoch:38; metric:emoval; train:0.7971; eval:0.5896; lr:0.000250
epoch:39; metric:emoval; train:0.7961; eval:0.5989; lr:0.000250
epoch:40; metric:emoval; train:0.7889; eval:0.5956; lr:0.000250
epoch:41; metric:emoval; train:0.8030; eval:0.6134; lr:0.000250
epoch:42; metric:emoval; train:0.8072; eval:0.6187; lr:0.000250
epoch:43; metric:emoval; train:0.7856; eval:0.6130; lr:0.000250
epoch:44; metric:emoval; train:0.7821; eval:0.5863; lr:0.000250
epoch:45; metric:emoval; train:0.7910; eval:0.5978; lr:0.000250
epoch:46; metric:emoval; train:0.8071; eval:0.5802; lr:0.000250
epoch:47; metric:emoval; train:0.8090; eval:0.6156; lr:0.000250
epoch:48; metric:emoval; train:0.7984; eval:0.5748; lr:0.000125
epoch:49; metric:emoval; train:0.8273; eval:0.6037; lr:0.000125
epoch:50; metric:emoval; train:0.8270; eval:0.6204; lr:0.000125
epoch:51; metric:emoval; train:0.8484; eval:0.6141; lr:0.000125
epoch:52; metric:emoval; train:0.8361; eval:0.6119; lr:0.000125
epoch:53; metric:emoval; train:0.8325; eval:0.6082; lr:0.000125
epoch:54; metric:emoval; train:0.8338; eval:0.6239; lr:0.000125
epoch:55; metric:emoval; train:0.8389; eval:0.6134; lr:0.000125
epoch:56; metric:emoval; train:0.8364; eval:0.6194; lr:0.000125
epoch:57; metric:emoval; train:0.8354; eval:0.6118; lr:0.000125
epoch:58; metric:emoval; train:0.8386; eval:0.6123; lr:0.000125
epoch:59; metric:emoval; train:0.8288; eval:0.5899; lr:0.000125
epoch:60; metric:emoval; train:0.8435; eval:0.6085; lr:0.000125
epoch:61; metric:emoval; train:0.8390; eval:0.6208; lr:0.000125
epoch:62; metric:emoval; train:0.8538; eval:0.6098; lr:0.000125
epoch:63; metric:emoval; train:0.8366; eval:0.6033; lr:0.000125
epoch:64; metric:emoval; train:0.8340; eval:0.6198; lr:0.000125
epoch:65; metric:emoval; train:0.8471; eval:0.6242; lr:0.000125
epoch:66; metric:emoval; train:0.8553; eval:0.6045; lr:0.000125
epoch:67; metric:emoval; train:0.8596; eval:0.6136; lr:0.000125
epoch:68; metric:emoval; train:0.8707; eval:0.6142; lr:0.000125
epoch:69; metric:emoval; train:0.8366; eval:0.6158; lr:0.000125
epoch:70; metric:emoval; train:0.8539; eval:0.6054; lr:0.000125
epoch:71; metric:emoval; train:0.8542; eval:0.5788; lr:0.000125
epoch:72; metric:emoval; train:0.8395; eval:0.6249; lr:0.000125
epoch:73; metric:emoval; train:0.8560; eval:0.6047; lr:0.000125
epoch:74; metric:emoval; train:0.8596; eval:0.6273; lr:0.000125
epoch:75; metric:emoval; train:0.8671; eval:0.6238; lr:0.000125
epoch:76; metric:emoval; train:0.8385; eval:0.6256; lr:0.000125
epoch:77; metric:emoval; train:0.8531; eval:0.6176; lr:0.000125
epoch:78; metric:emoval; train:0.8512; eval:0.5910; lr:0.000125
epoch:79; metric:emoval; train:0.8447; eval:0.6122; lr:0.000125
epoch:80; metric:emoval; train:0.8533; eval:0.6118; lr:0.000125
epoch:81; metric:emoval; train:0.8434; eval:0.5758; lr:0.000125
epoch:82; metric:emoval; train:0.8558; eval:0.6067; lr:0.000125
epoch:83; metric:emoval; train:0.8450; eval:0.5846; lr:0.000125
epoch:84; metric:emoval; train:0.8493; eval:0.6091; lr:0.000125
epoch:85; metric:emoval; train:0.8521; eval:0.6250; lr:0.000063
epoch:86; metric:emoval; train:0.8671; eval:0.6246; lr:0.000063
epoch:87; metric:emoval; train:0.8666; eval:0.6271; lr:0.000063
epoch:88; metric:emoval; train:0.8669; eval:0.6230; lr:0.000063
epoch:89; metric:emoval; train:0.8656; eval:0.6234; lr:0.000063
epoch:90; metric:emoval; train:0.8759; eval:0.6230; lr:0.000063
epoch:91; metric:emoval; train:0.8798; eval:0.6134; lr:0.000063
epoch:92; metric:emoval; train:0.8636; eval:0.6212; lr:0.000063
epoch:93; metric:emoval; train:0.8726; eval:0.6019; lr:0.000063
epoch:94; metric:emoval; train:0.8734; eval:0.6213; lr:0.000063
epoch:95; metric:emoval; train:0.8710; eval:0.6223; lr:0.000063
epoch:96; metric:emoval; train:0.8830; eval:0.6254; lr:0.000031
epoch:97; metric:emoval; train:0.8745; eval:0.6215; lr:0.000031
epoch:98; metric:emoval; train:0.8783; eval:0.6239; lr:0.000031
epoch:99; metric:emoval; train:0.8829; eval:0.6258; lr:0.000031
epoch:100; metric:emoval; train:0.8887; eval:0.6272; lr:0.000031
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 73, duration: 6231.389951705933 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1049; eval:0.3817; lr:0.000500
epoch:2; metric:emoval; train:0.2837; eval:0.4723; lr:0.000500
epoch:3; metric:emoval; train:0.4273; eval:0.5129; lr:0.000500
epoch:4; metric:emoval; train:0.4932; eval:0.5003; lr:0.000500
epoch:5; metric:emoval; train:0.5759; eval:0.5451; lr:0.000500
epoch:6; metric:emoval; train:0.6045; eval:0.5272; lr:0.000500
epoch:7; metric:emoval; train:0.6279; eval:0.4732; lr:0.000500
epoch:8; metric:emoval; train:0.6626; eval:0.4992; lr:0.000500
epoch:9; metric:emoval; train:0.6933; eval:0.5643; lr:0.000500
epoch:10; metric:emoval; train:0.6846; eval:0.5601; lr:0.000500
epoch:11; metric:emoval; train:0.7125; eval:0.5764; lr:0.000500
epoch:12; metric:emoval; train:0.7323; eval:0.5097; lr:0.000500
epoch:13; metric:emoval; train:0.7258; eval:0.5671; lr:0.000500
epoch:14; metric:emoval; train:0.7456; eval:0.5923; lr:0.000500
epoch:15; metric:emoval; train:0.7332; eval:0.5352; lr:0.000500
epoch:16; metric:emoval; train:0.7635; eval:0.5544; lr:0.000500
epoch:17; metric:emoval; train:0.7686; eval:0.5597; lr:0.000500
epoch:18; metric:emoval; train:0.7712; eval:0.5482; lr:0.000500
epoch:19; metric:emoval; train:0.7787; eval:0.5591; lr:0.000500
epoch:20; metric:emoval; train:0.7801; eval:0.5888; lr:0.000500
epoch:21; metric:emoval; train:0.7764; eval:0.5865; lr:0.000500
epoch:22; metric:emoval; train:0.7806; eval:0.5023; lr:0.000500
epoch:23; metric:emoval; train:0.7815; eval:0.4952; lr:0.000500
epoch:24; metric:emoval; train:0.7828; eval:0.5910; lr:0.000500
epoch:25; metric:emoval; train:0.7838; eval:0.5778; lr:0.000250
epoch:26; metric:emoval; train:0.8086; eval:0.5858; lr:0.000250
epoch:27; metric:emoval; train:0.8227; eval:0.5873; lr:0.000250
epoch:28; metric:emoval; train:0.8336; eval:0.5980; lr:0.000250
epoch:29; metric:emoval; train:0.8339; eval:0.5837; lr:0.000250
epoch:30; metric:emoval; train:0.8415; eval:0.5897; lr:0.000250
epoch:31; metric:emoval; train:0.8224; eval:0.5965; lr:0.000250
epoch:32; metric:emoval; train:0.8231; eval:0.5309; lr:0.000250
epoch:33; metric:emoval; train:0.8304; eval:0.5762; lr:0.000250
epoch:34; metric:emoval; train:0.8164; eval:0.5137; lr:0.000250
epoch:35; metric:emoval; train:0.8135; eval:0.5344; lr:0.000250
epoch:36; metric:emoval; train:0.8201; eval:0.5389; lr:0.000250
epoch:37; metric:emoval; train:0.8063; eval:0.5898; lr:0.000250
epoch:38; metric:emoval; train:0.8276; eval:0.5758; lr:0.000250
epoch:39; metric:emoval; train:0.8088; eval:0.5645; lr:0.000125
epoch:40; metric:emoval; train:0.8376; eval:0.5826; lr:0.000125
epoch:41; metric:emoval; train:0.8218; eval:0.5754; lr:0.000125
epoch:42; metric:emoval; train:0.8320; eval:0.5439; lr:0.000125
epoch:43; metric:emoval; train:0.8352; eval:0.5739; lr:0.000125
epoch:44; metric:emoval; train:0.8379; eval:0.5743; lr:0.000125
epoch:45; metric:emoval; train:0.8428; eval:0.5857; lr:0.000125
epoch:46; metric:emoval; train:0.8461; eval:0.5849; lr:0.000125
epoch:47; metric:emoval; train:0.8241; eval:0.5767; lr:0.000125
epoch:48; metric:emoval; train:0.8409; eval:0.5877; lr:0.000125
epoch:49; metric:emoval; train:0.8393; eval:0.5943; lr:0.000125
epoch:50; metric:emoval; train:0.8514; eval:0.5536; lr:0.000063
epoch:51; metric:emoval; train:0.8604; eval:0.5789; lr:0.000063
epoch:52; metric:emoval; train:0.8581; eval:0.5914; lr:0.000063
epoch:53; metric:emoval; train:0.8507; eval:0.5637; lr:0.000063
epoch:54; metric:emoval; train:0.8633; eval:0.5919; lr:0.000063
epoch:55; metric:emoval; train:0.8532; eval:0.5918; lr:0.000063
epoch:56; metric:emoval; train:0.8722; eval:0.5863; lr:0.000063
epoch:57; metric:emoval; train:0.8637; eval:0.5526; lr:0.000063
epoch:58; metric:emoval; train:0.8589; eval:0.5792; lr:0.000063
Early stopping at epoch 58, best epoch: 28
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 27, duration: 3390.8025875091553 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1277; eval:0.3870; lr:0.000500
epoch:2; metric:emoval; train:0.2829; eval:0.4095; lr:0.000500
epoch:3; metric:emoval; train:0.3865; eval:0.5624; lr:0.000500
epoch:4; metric:emoval; train:0.4838; eval:0.5466; lr:0.000500
epoch:5; metric:emoval; train:0.5543; eval:0.5720; lr:0.000500
epoch:6; metric:emoval; train:0.5837; eval:0.5865; lr:0.000500
epoch:7; metric:emoval; train:0.6191; eval:0.6011; lr:0.000500
epoch:8; metric:emoval; train:0.6702; eval:0.5880; lr:0.000500
epoch:9; metric:emoval; train:0.6716; eval:0.5309; lr:0.000500
epoch:10; metric:emoval; train:0.6744; eval:0.5756; lr:0.000500
epoch:11; metric:emoval; train:0.7024; eval:0.5606; lr:0.000500
epoch:12; metric:emoval; train:0.7151; eval:0.5974; lr:0.000500
epoch:13; metric:emoval; train:0.7110; eval:0.5549; lr:0.000500
epoch:14; metric:emoval; train:0.7364; eval:0.6041; lr:0.000500
epoch:15; metric:emoval; train:0.7404; eval:0.6058; lr:0.000500
epoch:16; metric:emoval; train:0.7443; eval:0.5321; lr:0.000500
epoch:17; metric:emoval; train:0.7585; eval:0.5804; lr:0.000500
epoch:18; metric:emoval; train:0.7586; eval:0.5768; lr:0.000500
epoch:19; metric:emoval; train:0.7508; eval:0.5767; lr:0.000500
epoch:20; metric:emoval; train:0.7655; eval:0.5749; lr:0.000500
epoch:21; metric:emoval; train:0.7656; eval:0.5773; lr:0.000500
epoch:22; metric:emoval; train:0.7668; eval:0.5635; lr:0.000500
epoch:23; metric:emoval; train:0.7539; eval:0.6030; lr:0.000500
epoch:24; metric:emoval; train:0.7863; eval:0.6024; lr:0.000500
epoch:25; metric:emoval; train:0.7702; eval:0.5965; lr:0.000500
epoch:26; metric:emoval; train:0.7810; eval:0.5212; lr:0.000250
epoch:27; metric:emoval; train:0.8155; eval:0.6183; lr:0.000250
epoch:28; metric:emoval; train:0.8265; eval:0.5912; lr:0.000250
epoch:29; metric:emoval; train:0.8197; eval:0.5787; lr:0.000250
epoch:30; metric:emoval; train:0.8204; eval:0.6014; lr:0.000250
epoch:31; metric:emoval; train:0.8280; eval:0.5998; lr:0.000250
epoch:32; metric:emoval; train:0.8035; eval:0.5848; lr:0.000250
epoch:33; metric:emoval; train:0.8276; eval:0.6218; lr:0.000250
epoch:34; metric:emoval; train:0.8224; eval:0.5955; lr:0.000250
epoch:35; metric:emoval; train:0.7900; eval:0.6007; lr:0.000250
epoch:36; metric:emoval; train:0.8046; eval:0.6230; lr:0.000250
epoch:37; metric:emoval; train:0.8070; eval:0.6031; lr:0.000250
epoch:38; metric:emoval; train:0.8030; eval:0.6178; lr:0.000250
epoch:39; metric:emoval; train:0.8008; eval:0.6419; lr:0.000250
epoch:40; metric:emoval; train:0.7857; eval:0.5824; lr:0.000250
epoch:41; metric:emoval; train:0.8046; eval:0.5711; lr:0.000250
epoch:42; metric:emoval; train:0.8094; eval:0.5771; lr:0.000250
epoch:43; metric:emoval; train:0.7915; eval:0.5977; lr:0.000250
epoch:44; metric:emoval; train:0.8017; eval:0.5972; lr:0.000250
epoch:45; metric:emoval; train:0.7823; eval:0.5802; lr:0.000250
epoch:46; metric:emoval; train:0.8195; eval:0.6144; lr:0.000250
epoch:47; metric:emoval; train:0.7932; eval:0.5691; lr:0.000250
epoch:48; metric:emoval; train:0.7943; eval:0.6213; lr:0.000250
epoch:49; metric:emoval; train:0.8034; eval:0.5898; lr:0.000250
epoch:50; metric:emoval; train:0.7986; eval:0.6013; lr:0.000125
epoch:51; metric:emoval; train:0.8342; eval:0.6072; lr:0.000125
epoch:52; metric:emoval; train:0.8454; eval:0.6279; lr:0.000125
epoch:53; metric:emoval; train:0.8351; eval:0.6172; lr:0.000125
epoch:54; metric:emoval; train:0.8297; eval:0.6228; lr:0.000125
epoch:55; metric:emoval; train:0.8396; eval:0.6044; lr:0.000125
epoch:56; metric:emoval; train:0.8456; eval:0.6122; lr:0.000125
epoch:57; metric:emoval; train:0.8463; eval:0.6139; lr:0.000125
epoch:58; metric:emoval; train:0.8514; eval:0.6111; lr:0.000125
epoch:59; metric:emoval; train:0.8376; eval:0.6254; lr:0.000125
epoch:60; metric:emoval; train:0.8438; eval:0.6178; lr:0.000125
epoch:61; metric:emoval; train:0.8588; eval:0.6000; lr:0.000063
epoch:62; metric:emoval; train:0.8610; eval:0.6130; lr:0.000063
epoch:63; metric:emoval; train:0.8507; eval:0.6063; lr:0.000063
epoch:64; metric:emoval; train:0.8659; eval:0.6130; lr:0.000063
epoch:65; metric:emoval; train:0.8685; eval:0.6260; lr:0.000063
epoch:66; metric:emoval; train:0.8784; eval:0.6166; lr:0.000063
epoch:67; metric:emoval; train:0.8731; eval:0.6169; lr:0.000063
epoch:68; metric:emoval; train:0.8649; eval:0.6160; lr:0.000063
epoch:69; metric:emoval; train:0.8744; eval:0.6147; lr:0.000063
Early stopping at epoch 69, best epoch: 39
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 38, duration: 2778.5971155166626 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1309; eval:0.3202; lr:0.000500
epoch:2; metric:emoval; train:0.3228; eval:0.4283; lr:0.000500
epoch:3; metric:emoval; train:0.4443; eval:0.4763; lr:0.000500
epoch:4; metric:emoval; train:0.5122; eval:0.5025; lr:0.000500
epoch:5; metric:emoval; train:0.5528; eval:0.5278; lr:0.000500
epoch:6; metric:emoval; train:0.5778; eval:0.4544; lr:0.000500
epoch:7; metric:emoval; train:0.6307; eval:0.5710; lr:0.000500
epoch:8; metric:emoval; train:0.6424; eval:0.5242; lr:0.000500
epoch:9; metric:emoval; train:0.6750; eval:0.5693; lr:0.000500
epoch:10; metric:emoval; train:0.6807; eval:0.5825; lr:0.000500
epoch:11; metric:emoval; train:0.7134; eval:0.5267; lr:0.000500
epoch:12; metric:emoval; train:0.7246; eval:0.5841; lr:0.000500
epoch:13; metric:emoval; train:0.7267; eval:0.5529; lr:0.000500
epoch:14; metric:emoval; train:0.7355; eval:0.5412; lr:0.000500
epoch:15; metric:emoval; train:0.7609; eval:0.5656; lr:0.000500
epoch:16; metric:emoval; train:0.7415; eval:0.5401; lr:0.000500
epoch:17; metric:emoval; train:0.7466; eval:0.5463; lr:0.000500
epoch:18; metric:emoval; train:0.7594; eval:0.4923; lr:0.000500
epoch:19; metric:emoval; train:0.7513; eval:0.5442; lr:0.000500
epoch:20; metric:emoval; train:0.7809; eval:0.5395; lr:0.000500
epoch:21; metric:emoval; train:0.7788; eval:0.5356; lr:0.000500
epoch:22; metric:emoval; train:0.7434; eval:0.5722; lr:0.000500
epoch:23; metric:emoval; train:0.7758; eval:0.5268; lr:0.000250
epoch:24; metric:emoval; train:0.8284; eval:0.5897; lr:0.000250
epoch:25; metric:emoval; train:0.8258; eval:0.6071; lr:0.000250
epoch:26; metric:emoval; train:0.8360; eval:0.5483; lr:0.000250
epoch:27; metric:emoval; train:0.8240; eval:0.6001; lr:0.000250
epoch:28; metric:emoval; train:0.8264; eval:0.5725; lr:0.000250
epoch:29; metric:emoval; train:0.8368; eval:0.5586; lr:0.000250
epoch:30; metric:emoval; train:0.8275; eval:0.5959; lr:0.000250
epoch:31; metric:emoval; train:0.8049; eval:0.5798; lr:0.000250
epoch:32; metric:emoval; train:0.8244; eval:0.6066; lr:0.000250
epoch:33; metric:emoval; train:0.8164; eval:0.5937; lr:0.000250
epoch:34; metric:emoval; train:0.8034; eval:0.5424; lr:0.000250
epoch:35; metric:emoval; train:0.8157; eval:0.5517; lr:0.000250
epoch:36; metric:emoval; train:0.8129; eval:0.5768; lr:0.000125
epoch:37; metric:emoval; train:0.8336; eval:0.6122; lr:0.000125
epoch:38; metric:emoval; train:0.8292; eval:0.5859; lr:0.000125
epoch:39; metric:emoval; train:0.8310; eval:0.5639; lr:0.000125
epoch:40; metric:emoval; train:0.8283; eval:0.6063; lr:0.000125
epoch:41; metric:emoval; train:0.8310; eval:0.5888; lr:0.000125
epoch:42; metric:emoval; train:0.8312; eval:0.5664; lr:0.000125
epoch:43; metric:emoval; train:0.8261; eval:0.5719; lr:0.000125
epoch:44; metric:emoval; train:0.8237; eval:0.5724; lr:0.000125
epoch:45; metric:emoval; train:0.8497; eval:0.5949; lr:0.000125
epoch:46; metric:emoval; train:0.8310; eval:0.6022; lr:0.000125
epoch:47; metric:emoval; train:0.8374; eval:0.5439; lr:0.000125
epoch:48; metric:emoval; train:0.8282; eval:0.6071; lr:0.000063
epoch:49; metric:emoval; train:0.8446; eval:0.6028; lr:0.000063
epoch:50; metric:emoval; train:0.8508; eval:0.6036; lr:0.000063
epoch:51; metric:emoval; train:0.8549; eval:0.6022; lr:0.000063
epoch:52; metric:emoval; train:0.8585; eval:0.6082; lr:0.000063
epoch:53; metric:emoval; train:0.8503; eval:0.6082; lr:0.000063
epoch:54; metric:emoval; train:0.8725; eval:0.6053; lr:0.000063
epoch:55; metric:emoval; train:0.8623; eval:0.6051; lr:0.000063
epoch:56; metric:emoval; train:0.8686; eval:0.5944; lr:0.000063
epoch:57; metric:emoval; train:0.8552; eval:0.5578; lr:0.000063
epoch:58; metric:emoval; train:0.8628; eval:0.5921; lr:0.000063
epoch:59; metric:emoval; train:0.8615; eval:0.5917; lr:0.000031
epoch:60; metric:emoval; train:0.8631; eval:0.5921; lr:0.000031
epoch:61; metric:emoval; train:0.8774; eval:0.5843; lr:0.000031
epoch:62; metric:emoval; train:0.8737; eval:0.6008; lr:0.000031
epoch:63; metric:emoval; train:0.8734; eval:0.5962; lr:0.000031
epoch:64; metric:emoval; train:0.8773; eval:0.5958; lr:0.000031
epoch:65; metric:emoval; train:0.8810; eval:0.6024; lr:0.000031
epoch:66; metric:emoval; train:0.8715; eval:0.5879; lr:0.000031
epoch:67; metric:emoval; train:0.8604; eval:0.5822; lr:0.000031
Early stopping at epoch 67, best epoch: 37
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 36, duration: 878.2238118648529 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1126; eval:0.2693; lr:0.000500
epoch:2; metric:emoval; train:0.3148; eval:0.4301; lr:0.000500
epoch:3; metric:emoval; train:0.4382; eval:0.4635; lr:0.000500
epoch:4; metric:emoval; train:0.4992; eval:0.4899; lr:0.000500
epoch:5; metric:emoval; train:0.5620; eval:0.4672; lr:0.000500
epoch:6; metric:emoval; train:0.5927; eval:0.4908; lr:0.000500
epoch:7; metric:emoval; train:0.6395; eval:0.4659; lr:0.000500
epoch:8; metric:emoval; train:0.6692; eval:0.4699; lr:0.000500
epoch:9; metric:emoval; train:0.6862; eval:0.5122; lr:0.000500
epoch:10; metric:emoval; train:0.7097; eval:0.5457; lr:0.000500
epoch:11; metric:emoval; train:0.7161; eval:0.3989; lr:0.000500
epoch:12; metric:emoval; train:0.7235; eval:0.5296; lr:0.000500
epoch:13; metric:emoval; train:0.7351; eval:0.5244; lr:0.000500
epoch:14; metric:emoval; train:0.7440; eval:0.3923; lr:0.000500
epoch:15; metric:emoval; train:0.7433; eval:0.5282; lr:0.000500
epoch:16; metric:emoval; train:0.7647; eval:0.5201; lr:0.000500
epoch:17; metric:emoval; train:0.7710; eval:0.4359; lr:0.000500
epoch:18; metric:emoval; train:0.7692; eval:0.5370; lr:0.000500
epoch:19; metric:emoval; train:0.7671; eval:0.5231; lr:0.000500
epoch:20; metric:emoval; train:0.7558; eval:0.5143; lr:0.000500
epoch:21; metric:emoval; train:0.7795; eval:0.5504; lr:0.000500
epoch:22; metric:emoval; train:0.7777; eval:0.4930; lr:0.000500
epoch:23; metric:emoval; train:0.7853; eval:0.4750; lr:0.000500
epoch:24; metric:emoval; train:0.7852; eval:0.5485; lr:0.000500
epoch:25; metric:emoval; train:0.7920; eval:0.5309; lr:0.000500
epoch:26; metric:emoval; train:0.7686; eval:0.5242; lr:0.000500
epoch:27; metric:emoval; train:0.7753; eval:0.4500; lr:0.000500
epoch:28; metric:emoval; train:0.7512; eval:0.4642; lr:0.000500
epoch:29; metric:emoval; train:0.7698; eval:0.4773; lr:0.000500
epoch:30; metric:emoval; train:0.7527; eval:0.4807; lr:0.000500
epoch:31; metric:emoval; train:0.7612; eval:0.5343; lr:0.000500
epoch:32; metric:emoval; train:0.7577; eval:0.5315; lr:0.000250
epoch:33; metric:emoval; train:0.7966; eval:0.5143; lr:0.000250
epoch:34; metric:emoval; train:0.8133; eval:0.5444; lr:0.000250
epoch:35; metric:emoval; train:0.8270; eval:0.5432; lr:0.000250
epoch:36; metric:emoval; train:0.8212; eval:0.5181; lr:0.000250
epoch:37; metric:emoval; train:0.8112; eval:0.5542; lr:0.000250
epoch:38; metric:emoval; train:0.8091; eval:0.5168; lr:0.000250
epoch:39; metric:emoval; train:0.8248; eval:0.5236; lr:0.000250
epoch:40; metric:emoval; train:0.7892; eval:0.5380; lr:0.000250
epoch:41; metric:emoval; train:0.8105; eval:0.5482; lr:0.000250
epoch:42; metric:emoval; train:0.8075; eval:0.5294; lr:0.000250
epoch:43; metric:emoval; train:0.8145; eval:0.5290; lr:0.000250
epoch:44; metric:emoval; train:0.7776; eval:0.5242; lr:0.000250
epoch:45; metric:emoval; train:0.8075; eval:0.5203; lr:0.000250
epoch:46; metric:emoval; train:0.8092; eval:0.5237; lr:0.000250
epoch:47; metric:emoval; train:0.8155; eval:0.5232; lr:0.000250
epoch:48; metric:emoval; train:0.7984; eval:0.4674; lr:0.000125
epoch:49; metric:emoval; train:0.8232; eval:0.5343; lr:0.000125
epoch:50; metric:emoval; train:0.8412; eval:0.5576; lr:0.000125
epoch:51; metric:emoval; train:0.8448; eval:0.5442; lr:0.000125
epoch:52; metric:emoval; train:0.8406; eval:0.5738; lr:0.000125
epoch:53; metric:emoval; train:0.8576; eval:0.5840; lr:0.000125
epoch:54; metric:emoval; train:0.8382; eval:0.5553; lr:0.000125
epoch:55; metric:emoval; train:0.8388; eval:0.5623; lr:0.000125
epoch:56; metric:emoval; train:0.8534; eval:0.5563; lr:0.000125
epoch:57; metric:emoval; train:0.8404; eval:0.5391; lr:0.000125
epoch:58; metric:emoval; train:0.8510; eval:0.5516; lr:0.000125
epoch:59; metric:emoval; train:0.8535; eval:0.5127; lr:0.000125
epoch:60; metric:emoval; train:0.8403; eval:0.5524; lr:0.000125
epoch:61; metric:emoval; train:0.8462; eval:0.5555; lr:0.000125
epoch:62; metric:emoval; train:0.8525; eval:0.5768; lr:0.000125
epoch:63; metric:emoval; train:0.8567; eval:0.5495; lr:0.000125
epoch:64; metric:emoval; train:0.8549; eval:0.5388; lr:0.000063
epoch:65; metric:emoval; train:0.8592; eval:0.5495; lr:0.000063
epoch:66; metric:emoval; train:0.8712; eval:0.5609; lr:0.000063
epoch:67; metric:emoval; train:0.8705; eval:0.6015; lr:0.000063
epoch:68; metric:emoval; train:0.8672; eval:0.5383; lr:0.000063
epoch:69; metric:emoval; train:0.8702; eval:0.5669; lr:0.000063
epoch:70; metric:emoval; train:0.8528; eval:0.5422; lr:0.000063
epoch:71; metric:emoval; train:0.8682; eval:0.5600; lr:0.000063
epoch:72; metric:emoval; train:0.8701; eval:0.5696; lr:0.000063
epoch:73; metric:emoval; train:0.8644; eval:0.5792; lr:0.000063
epoch:74; metric:emoval; train:0.8791; eval:0.5711; lr:0.000063
epoch:75; metric:emoval; train:0.8669; eval:0.5762; lr:0.000063
epoch:76; metric:emoval; train:0.8767; eval:0.5574; lr:0.000063
epoch:77; metric:emoval; train:0.8853; eval:0.5706; lr:0.000063
epoch:78; metric:emoval; train:0.8808; eval:0.5406; lr:0.000031
epoch:79; metric:emoval; train:0.8793; eval:0.5705; lr:0.000031
epoch:80; metric:emoval; train:0.8755; eval:0.5697; lr:0.000031
epoch:81; metric:emoval; train:0.8755; eval:0.5711; lr:0.000031
epoch:82; metric:emoval; train:0.8879; eval:0.5726; lr:0.000031
epoch:83; metric:emoval; train:0.9013; eval:0.5720; lr:0.000031
epoch:84; metric:emoval; train:0.8741; eval:0.5552; lr:0.000031
epoch:85; metric:emoval; train:0.8822; eval:0.5686; lr:0.000031
epoch:86; metric:emoval; train:0.8860; eval:0.5553; lr:0.000031
epoch:87; metric:emoval; train:0.8853; eval:0.5721; lr:0.000031
epoch:88; metric:emoval; train:0.8910; eval:0.5629; lr:0.000031
epoch:89; metric:emoval; train:0.8952; eval:0.5611; lr:0.000016
epoch:90; metric:emoval; train:0.8932; eval:0.5824; lr:0.000016
epoch:91; metric:emoval; train:0.8942; eval:0.5786; lr:0.000016
epoch:92; metric:emoval; train:0.8851; eval:0.5665; lr:0.000016
epoch:93; metric:emoval; train:0.8902; eval:0.5752; lr:0.000016
epoch:94; metric:emoval; train:0.8911; eval:0.5681; lr:0.000016
epoch:95; metric:emoval; train:0.9034; eval:0.5713; lr:0.000016
epoch:96; metric:emoval; train:0.8782; eval:0.5691; lr:0.000016
epoch:97; metric:emoval; train:0.8904; eval:0.5762; lr:0.000016
Early stopping at epoch 97, best epoch: 67
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 66, duration: 401.67883229255676 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.7647_acc:0.7655_val:0.5940_1770137908.5230365.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.8259_acc:0.8248_val:0.6328_1770137908.5230365.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.7660_acc:0.7694_val:0.5999_1770137908.5230365.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v4+utt+None_f1:0.8805_acc:0.8825_val:79.5502_1770137908.5230365.npz
