====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=0.5, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v5', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s]  2%|▏         | 81/3373 [00:00<00:11, 280.67it/s]  4%|▍         | 149/3373 [00:00<00:07, 410.25it/s]  6%|▌         | 200/3373 [00:00<00:07, 429.66it/s]  9%|▉         | 300/3373 [00:00<00:05, 604.20it/s] 11%|█         | 369/3373 [00:00<00:06, 487.85it/s] 14%|█▍        | 470/3373 [00:00<00:04, 619.37it/s] 17%|█▋        | 582/3373 [00:00<00:03, 751.81it/s] 20%|█▉        | 667/3373 [00:01<00:03, 776.70it/s] 23%|██▎       | 766/3373 [00:01<00:03, 834.28it/s] 25%|██▌       | 855/3373 [00:01<00:02, 849.64it/s] 28%|██▊       | 944/3373 [00:01<00:02, 856.68it/s] 31%|███       | 1033/3373 [00:01<00:05, 404.36it/s] 33%|███▎      | 1101/3373 [00:02<00:06, 340.23it/s] 34%|███▍      | 1155/3373 [00:02<00:06, 366.80it/s] 36%|███▌      | 1208/3373 [00:02<00:07, 295.32it/s] 40%|████      | 1362/3373 [00:02<00:04, 488.81it/s] 44%|████▎     | 1472/3373 [00:02<00:03, 599.48it/s] 46%|████▌     | 1557/3373 [00:02<00:02, 648.84it/s] 49%|████▉     | 1660/3373 [00:02<00:02, 734.20it/s] 52%|█████▏    | 1750/3373 [00:03<00:02, 622.55it/s] 54%|█████▍    | 1833/3373 [00:03<00:02, 667.51it/s] 57%|█████▋    | 1912/3373 [00:03<00:02, 567.17it/s] 59%|█████▊    | 1979/3373 [00:03<00:02, 586.10it/s] 62%|██████▏   | 2081/3373 [00:03<00:01, 678.86it/s] 64%|██████▍   | 2157/3373 [00:03<00:02, 474.38it/s] 66%|██████▌   | 2218/3373 [00:04<00:02, 500.18it/s] 69%|██████▉   | 2330/3373 [00:04<00:01, 632.98it/s] 71%|███████▏  | 2406/3373 [00:04<00:01, 656.40it/s] 74%|███████▎  | 2481/3373 [00:04<00:01, 672.68it/s] 76%|███████▌  | 2555/3373 [00:04<00:01, 556.79it/s] 78%|███████▊  | 2619/3373 [00:04<00:01, 575.49it/s] 80%|███████▉  | 2694/3373 [00:04<00:01, 616.87it/s] 82%|████████▏ | 2761/3373 [00:04<00:00, 627.41it/s] 84%|████████▍ | 2833/3373 [00:04<00:00, 650.47it/s] 87%|████████▋ | 2928/3373 [00:05<00:00, 727.30it/s] 90%|████████▉ | 3024/3373 [00:05<00:00, 790.13it/s] 93%|█████████▎| 3129/3373 [00:05<00:00, 691.93it/s] 97%|█████████▋| 3272/3373 [00:05<00:00, 874.18it/s]100%|█████████▉| 3367/3373 [00:05<00:00, 881.26it/s]100%|██████████| 3373/3373 [00:05<00:00, 602.92it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s]  2%|▏         | 51/3373 [00:00<00:06, 509.28it/s]  3%|▎         | 118/3373 [00:00<00:08, 391.98it/s]  5%|▍         | 167/3373 [00:00<00:07, 425.67it/s]  6%|▋         | 216/3373 [00:00<00:07, 442.88it/s]  9%|▉         | 298/3373 [00:00<00:05, 563.25it/s] 11%|█         | 357/3373 [00:00<00:05, 570.13it/s] 12%|█▏        | 416/3373 [00:00<00:06, 445.10it/s] 14%|█▍        | 473/3373 [00:00<00:06, 476.21it/s] 16%|█▌        | 542/3373 [00:01<00:05, 532.75it/s] 18%|█▊        | 600/3373 [00:01<00:06, 422.32it/s] 21%|██        | 693/3373 [00:01<00:06, 443.63it/s] 22%|██▏       | 742/3373 [00:01<00:05, 452.16it/s] 23%|██▎       | 791/3373 [00:01<00:05, 459.74it/s] 25%|██▍       | 840/3373 [00:01<00:05, 461.70it/s] 26%|██▋       | 888/3373 [00:02<00:13, 183.25it/s] 27%|██▋       | 924/3373 [00:02<00:11, 204.27it/s] 28%|██▊       | 960/3373 [00:02<00:10, 226.89it/s] 30%|███       | 1018/3373 [00:02<00:09, 249.57it/s] 31%|███       | 1052/3373 [00:02<00:08, 263.32it/s] 32%|███▏      | 1086/3373 [00:03<00:09, 232.85it/s] 33%|███▎      | 1115/3373 [00:03<00:09, 242.78it/s] 35%|███▍      | 1166/3373 [00:03<00:08, 248.42it/s] 36%|███▋      | 1223/3373 [00:03<00:08, 261.43it/s] 39%|███▉      | 1329/3373 [00:04<00:07, 262.84it/s] 42%|████▏     | 1401/3373 [00:04<00:06, 288.90it/s] 43%|████▎     | 1443/3373 [00:04<00:07, 266.88it/s] 44%|████▍     | 1477/3373 [00:04<00:06, 278.51it/s] 46%|████▌     | 1544/3373 [00:04<00:05, 350.82it/s] 47%|████▋     | 1585/3373 [00:04<00:04, 359.76it/s] 48%|████▊     | 1625/3373 [00:05<00:06, 254.62it/s] 49%|████▉     | 1658/3373 [00:05<00:07, 226.56it/s] 50%|████▉     | 1686/3373 [00:05<00:07, 235.19it/s] 52%|█████▏    | 1769/3373 [00:05<00:04, 355.93it/s] 54%|█████▍    | 1813/3373 [00:05<00:05, 260.35it/s] 55%|█████▍    | 1848/3373 [00:06<00:07, 203.71it/s] 56%|█████▌    | 1877/3373 [00:06<00:06, 214.81it/s] 56%|█████▋    | 1905/3373 [00:06<00:07, 192.91it/s] 58%|█████▊    | 1945/3373 [00:06<00:06, 229.00it/s] 59%|█████▊    | 1974/3373 [00:06<00:05, 241.19it/s] 61%|██████    | 2049/3373 [00:06<00:04, 294.70it/s] 62%|██████▏   | 2081/3373 [00:06<00:04, 296.75it/s] 63%|██████▎   | 2119/3373 [00:07<00:04, 260.09it/s] 64%|██████▍   | 2169/3373 [00:07<00:04, 255.54it/s] 65%|██████▌   | 2208/3373 [00:07<00:04, 282.06it/s] 66%|██████▋   | 2239/3373 [00:07<00:03, 286.37it/s] 68%|██████▊   | 2282/3373 [00:07<00:05, 217.55it/s] 69%|██████▉   | 2334/3373 [00:07<00:03, 274.50it/s] 70%|███████   | 2368/3373 [00:07<00:03, 287.95it/s] 71%|███████   | 2402/3373 [00:08<00:03, 299.48it/s] 72%|███████▏  | 2436/3373 [00:08<00:03, 249.59it/s] 73%|███████▎  | 2465/3373 [00:08<00:03, 255.83it/s] 74%|███████▍  | 2494/3373 [00:08<00:05, 153.93it/s] 76%|███████▌  | 2552/3373 [00:09<00:05, 150.77it/s] 77%|███████▋  | 2592/3373 [00:09<00:04, 184.86it/s] 78%|███████▊  | 2618/3373 [00:09<00:03, 194.74it/s] 79%|███████▉  | 2674/3373 [00:09<00:03, 193.52it/s] 81%|████████▏ | 2745/3373 [00:09<00:02, 278.49it/s] 84%|████████▎ | 2822/3373 [00:09<00:01, 372.09it/s] 85%|████████▌ | 2872/3373 [00:10<00:01, 329.46it/s] 87%|████████▋ | 2918/3373 [00:10<00:01, 296.36it/s] 89%|████████▉ | 3008/3373 [00:10<00:00, 407.55it/s] 91%|█████████ | 3059/3373 [00:10<00:01, 303.88it/s] 94%|█████████▎| 3160/3373 [00:10<00:00, 423.03it/s] 95%|█████████▌| 3217/3373 [00:10<00:00, 380.63it/s] 97%|█████████▋| 3266/3373 [00:11<00:00, 401.64it/s]100%|█████████▉| 3362/3373 [00:11<00:00, 522.50it/s]100%|██████████| 3373/3373 [00:11<00:00, 301.18it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s]  0%|          | 3/3373 [00:00<03:35, 15.63it/s]  5%|▌         | 169/3373 [00:00<00:04, 716.52it/s]  8%|▊         | 268/3373 [00:00<00:08, 383.79it/s] 10%|▉         | 333/3373 [00:01<00:11, 273.21it/s] 16%|█▌        | 528/3373 [00:01<00:05, 524.85it/s] 18%|█▊        | 624/3373 [00:01<00:05, 516.18it/s] 21%|██        | 705/3373 [00:01<00:04, 556.22it/s] 24%|██▍       | 802/3373 [00:01<00:04, 544.04it/s] 27%|██▋       | 894/3373 [00:01<00:04, 516.18it/s] 29%|██▉       | 994/3373 [00:02<00:04, 510.79it/s] 31%|███       | 1053/3373 [00:02<00:04, 525.07it/s] 35%|███▌      | 1190/3373 [00:02<00:03, 694.10it/s] 38%|███▊      | 1271/3373 [00:02<00:02, 714.46it/s] 40%|████      | 1351/3373 [00:02<00:03, 596.48it/s] 42%|████▏     | 1419/3373 [00:02<00:03, 612.16it/s] 44%|████▍     | 1492/3373 [00:02<00:03, 523.55it/s] 47%|████▋     | 1571/3373 [00:02<00:03, 581.33it/s] 49%|████▊     | 1636/3373 [00:03<00:02, 589.30it/s] 51%|█████▏    | 1734/3373 [00:03<00:02, 686.74it/s] 54%|█████▎    | 1808/3373 [00:03<00:02, 558.56it/s] 57%|█████▋    | 1917/3373 [00:03<00:02, 678.07it/s] 61%|██████    | 2059/3373 [00:03<00:01, 697.67it/s] 66%|██████▌   | 2217/3373 [00:03<00:01, 893.29it/s] 69%|██████▊   | 2317/3373 [00:03<00:01, 914.19it/s] 72%|███████▏  | 2417/3373 [00:03<00:01, 927.25it/s] 76%|███████▌  | 2558/3373 [00:04<00:00, 1047.10it/s] 79%|███████▉  | 2668/3373 [00:04<00:00, 851.08it/s]  84%|████████▍ | 2835/3373 [00:04<00:00, 1036.80it/s] 87%|████████▋ | 2950/3373 [00:04<00:00, 854.59it/s]  91%|█████████ | 3071/3373 [00:04<00:00, 931.77it/s] 94%|█████████▍| 3176/3373 [00:04<00:00, 777.83it/s] 97%|█████████▋| 3265/3373 [00:05<00:00, 645.37it/s]100%|██████████| 3373/3373 [00:05<00:00, 649.12it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]  1%|▏         | 6/411 [00:00<00:06, 59.83it/s] 17%|█▋        | 69/411 [00:00<00:00, 392.67it/s] 27%|██▋       | 109/411 [00:00<00:00, 391.75it/s] 73%|███████▎  | 299/411 [00:00<00:00, 981.84it/s]100%|██████████| 411/411 [00:00<00:00, 998.59it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]  8%|▊         | 34/411 [00:00<00:01, 339.29it/s] 24%|██▎       | 97/411 [00:00<00:00, 510.01it/s] 53%|█████▎    | 216/411 [00:00<00:00, 582.43it/s] 77%|███████▋  | 317/411 [00:00<00:00, 714.68it/s]100%|██████████| 411/411 [00:00<00:00, 822.19it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]  1%|          | 4/411 [00:00<00:10, 39.96it/s] 24%|██▍       | 100/411 [00:00<00:00, 577.32it/s] 46%|████▌     | 188/411 [00:00<00:00, 696.65it/s] 73%|███████▎  | 301/411 [00:00<00:00, 864.65it/s]100%|██████████| 411/411 [00:00<00:00, 814.19it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]  0%|          | 1/412 [00:00<01:18,  5.22it/s] 15%|█▌        | 63/412 [00:00<00:01, 267.06it/s] 54%|█████▎    | 221/412 [00:00<00:00, 763.41it/s] 87%|████████▋ | 360/412 [00:00<00:00, 978.14it/s]100%|██████████| 412/412 [00:00<00:00, 823.86it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]  7%|▋         | 28/412 [00:00<00:01, 278.77it/s] 29%|██▉       | 121/412 [00:00<00:00, 660.48it/s] 46%|████▌     | 188/412 [00:00<00:00, 467.73it/s] 62%|██████▏   | 257/412 [00:00<00:00, 537.14it/s] 83%|████████▎ | 342/412 [00:00<00:00, 620.87it/s]100%|██████████| 412/412 [00:00<00:00, 597.70it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s] 13%|█▎        | 53/412 [00:00<00:00, 529.90it/s] 26%|██▌       | 106/412 [00:00<00:00, 508.90it/s] 78%|███████▊  | 322/412 [00:00<00:00, 879.57it/s]100%|██████████| 412/412 [00:00<00:00, 1018.25it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s] 16%|█▌        | 135/834 [00:00<00:00, 1332.67it/s] 39%|███▊      | 323/834 [00:00<00:00, 1651.58it/s] 59%|█████▊    | 489/834 [00:00<00:00, 1627.29it/s] 78%|███████▊  | 652/834 [00:00<00:00, 1192.13it/s] 94%|█████████▍| 784/834 [00:00<00:00, 1222.85it/s]100%|██████████| 834/834 [00:00<00:00, 1374.96it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]  2%|▏         | 16/834 [00:00<00:05, 159.83it/s]  9%|▉         | 73/834 [00:00<00:01, 393.11it/s] 15%|█▍        | 123/834 [00:00<00:02, 309.19it/s] 20%|█▉        | 163/834 [00:00<00:02, 329.34it/s] 30%|██▉       | 249/834 [00:00<00:01, 488.28it/s] 36%|███▌      | 302/834 [00:00<00:01, 389.16it/s] 42%|████▏     | 347/834 [00:01<00:01, 269.28it/s] 46%|████▋     | 386/834 [00:01<00:01, 292.18it/s] 54%|█████▍    | 453/834 [00:01<00:01, 373.17it/s] 60%|█████▉    | 499/834 [00:01<00:00, 393.02it/s] 65%|██████▌   | 545/834 [00:01<00:00, 327.53it/s] 70%|███████   | 584/834 [00:01<00:00, 339.16it/s] 75%|███████▍  | 623/834 [00:01<00:00, 240.88it/s] 86%|████████▌ | 716/834 [00:02<00:00, 372.14it/s] 92%|█████████▏| 766/834 [00:02<00:00, 396.13it/s]100%|██████████| 834/834 [00:02<00:00, 362.39it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]  2%|▏         | 14/834 [00:00<00:05, 139.43it/s]  6%|▌         | 49/834 [00:00<00:02, 262.51it/s] 14%|█▎        | 113/834 [00:00<00:01, 429.46it/s] 19%|█▊        | 156/834 [00:00<00:01, 419.68it/s] 24%|██▍       | 199/834 [00:00<00:01, 319.44it/s] 37%|███▋      | 305/834 [00:00<00:01, 522.37it/s] 44%|████▎     | 364/834 [00:00<00:01, 421.66it/s] 50%|█████     | 419/834 [00:01<00:00, 451.83it/s] 63%|██████▎   | 526/834 [00:01<00:00, 608.97it/s] 74%|███████▍  | 619/834 [00:01<00:00, 692.56it/s] 83%|████████▎ | 695/834 [00:01<00:00, 563.80it/s] 91%|█████████▏| 762/834 [00:01<00:00, 475.27it/s]100%|██████████| 834/834 [00:01<00:00, 520.79it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3338; eval:-0.1422; lr:0.000500
epoch:2; metric:emoval; train:0.1380; eval:0.2131; lr:0.000500
epoch:3; metric:emoval; train:0.3781; eval:0.4399; lr:0.000500
epoch:4; metric:emoval; train:0.5279; eval:0.4355; lr:0.000500
epoch:5; metric:emoval; train:0.6038; eval:0.5047; lr:0.000500
epoch:6; metric:emoval; train:0.6584; eval:0.5091; lr:0.000500
epoch:7; metric:emoval; train:0.7177; eval:0.5278; lr:0.000500
epoch:8; metric:emoval; train:0.7345; eval:0.5354; lr:0.000500
epoch:9; metric:emoval; train:0.7639; eval:0.4881; lr:0.000500
epoch:10; metric:emoval; train:0.7447; eval:0.4830; lr:0.000500
epoch:11; metric:emoval; train:0.7961; eval:0.5056; lr:0.000500
epoch:12; metric:emoval; train:0.7991; eval:0.4646; lr:0.000500
epoch:13; metric:emoval; train:0.8077; eval:0.5189; lr:0.000500
epoch:14; metric:emoval; train:0.8361; eval:0.4709; lr:0.000500
epoch:15; metric:emoval; train:0.8543; eval:0.4988; lr:0.000500
epoch:16; metric:emoval; train:0.8849; eval:0.4991; lr:0.000500
epoch:17; metric:emoval; train:0.8493; eval:0.5467; lr:0.000500
epoch:18; metric:emoval; train:0.8672; eval:0.5121; lr:0.000500
epoch:19; metric:emoval; train:0.8830; eval:0.5393; lr:0.000500
epoch:20; metric:emoval; train:0.8899; eval:0.5208; lr:0.000500
epoch:21; metric:emoval; train:0.8852; eval:0.5264; lr:0.000500
epoch:22; metric:emoval; train:0.8860; eval:0.5207; lr:0.000500
epoch:23; metric:emoval; train:0.8953; eval:0.4820; lr:0.000500
epoch:24; metric:emoval; train:0.8430; eval:0.5051; lr:0.000500
epoch:25; metric:emoval; train:0.8637; eval:0.4912; lr:0.000500
epoch:26; metric:emoval; train:0.8788; eval:0.5025; lr:0.000500
epoch:27; metric:emoval; train:0.8830; eval:0.4926; lr:0.000500
epoch:28; metric:emoval; train:0.8727; eval:0.5345; lr:0.000250
epoch:29; metric:emoval; train:0.9194; eval:0.5040; lr:0.000250
epoch:30; metric:emoval; train:0.9141; eval:0.4851; lr:0.000250
epoch:31; metric:emoval; train:0.9225; eval:0.5313; lr:0.000250
epoch:32; metric:emoval; train:0.9099; eval:0.4703; lr:0.000250
epoch:33; metric:emoval; train:0.9146; eval:0.5182; lr:0.000250
epoch:34; metric:emoval; train:0.9177; eval:0.5226; lr:0.000250
epoch:35; metric:emoval; train:0.9226; eval:0.4907; lr:0.000250
epoch:36; metric:emoval; train:0.9012; eval:0.4913; lr:0.000250
epoch:37; metric:emoval; train:0.9115; eval:0.4893; lr:0.000250
epoch:38; metric:emoval; train:0.8946; eval:0.5328; lr:0.000250
epoch:39; metric:emoval; train:0.9078; eval:0.5100; lr:0.000125
epoch:40; metric:emoval; train:0.9200; eval:0.5409; lr:0.000125
epoch:41; metric:emoval; train:0.9201; eval:0.5435; lr:0.000125
epoch:42; metric:emoval; train:0.9107; eval:0.5230; lr:0.000125
epoch:43; metric:emoval; train:0.9149; eval:0.5494; lr:0.000125
epoch:44; metric:emoval; train:0.9103; eval:0.5444; lr:0.000125
epoch:45; metric:emoval; train:0.9121; eval:0.5273; lr:0.000125
epoch:46; metric:emoval; train:0.9248; eval:0.5205; lr:0.000125
epoch:47; metric:emoval; train:0.9243; eval:0.5455; lr:0.000125
epoch:48; metric:emoval; train:0.9145; eval:0.5237; lr:0.000125
epoch:49; metric:emoval; train:0.9160; eval:0.5325; lr:0.000125
epoch:50; metric:emoval; train:0.9121; eval:0.5179; lr:0.000125
epoch:51; metric:emoval; train:0.9274; eval:0.5253; lr:0.000125
epoch:52; metric:emoval; train:0.9282; eval:0.5244; lr:0.000125
epoch:53; metric:emoval; train:0.9176; eval:0.5351; lr:0.000125
epoch:54; metric:emoval; train:0.9254; eval:0.5199; lr:0.000063
epoch:55; metric:emoval; train:0.9357; eval:0.5297; lr:0.000063
epoch:56; metric:emoval; train:0.9307; eval:0.5384; lr:0.000063
epoch:57; metric:emoval; train:0.9286; eval:0.5338; lr:0.000063
epoch:58; metric:emoval; train:0.9310; eval:0.5135; lr:0.000063
epoch:59; metric:emoval; train:0.9428; eval:0.5197; lr:0.000063
epoch:60; metric:emoval; train:0.9411; eval:0.5156; lr:0.000063
epoch:61; metric:emoval; train:0.9455; eval:0.5366; lr:0.000063
epoch:62; metric:emoval; train:0.9335; eval:0.5248; lr:0.000063
epoch:63; metric:emoval; train:0.9286; eval:0.5324; lr:0.000063
epoch:64; metric:emoval; train:0.9389; eval:0.5120; lr:0.000063
epoch:65; metric:emoval; train:0.9451; eval:0.5229; lr:0.000031
epoch:66; metric:emoval; train:0.9438; eval:0.5176; lr:0.000031
epoch:67; metric:emoval; train:0.9421; eval:0.5272; lr:0.000031
epoch:68; metric:emoval; train:0.9404; eval:0.5112; lr:0.000031
epoch:69; metric:emoval; train:0.9439; eval:0.5259; lr:0.000031
epoch:70; metric:emoval; train:0.9403; eval:0.5016; lr:0.000031
epoch:71; metric:emoval; train:0.9386; eval:0.5043; lr:0.000031
epoch:72; metric:emoval; train:0.9476; eval:0.5268; lr:0.000031
epoch:73; metric:emoval; train:0.9441; eval:0.5123; lr:0.000031
Early stopping at epoch 73, best epoch: 43
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 42, duration: 4615.906002283096 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3190; eval:0.0912; lr:0.000500
epoch:2; metric:emoval; train:0.2208; eval:0.1551; lr:0.000500
epoch:3; metric:emoval; train:0.4282; eval:0.5087; lr:0.000500
epoch:4; metric:emoval; train:0.5647; eval:0.4616; lr:0.000500
epoch:5; metric:emoval; train:0.6314; eval:0.5713; lr:0.000500
epoch:6; metric:emoval; train:0.6755; eval:0.4816; lr:0.000500
epoch:7; metric:emoval; train:0.7292; eval:0.5349; lr:0.000500
epoch:8; metric:emoval; train:0.7362; eval:0.5416; lr:0.000500
epoch:9; metric:emoval; train:0.7883; eval:0.4446; lr:0.000500
epoch:10; metric:emoval; train:0.7735; eval:0.5576; lr:0.000500
epoch:11; metric:emoval; train:0.8048; eval:0.4577; lr:0.000500
epoch:12; metric:emoval; train:0.8399; eval:0.5832; lr:0.000500
epoch:13; metric:emoval; train:0.8543; eval:0.5477; lr:0.000500
epoch:14; metric:emoval; train:0.8532; eval:0.5793; lr:0.000500
epoch:15; metric:emoval; train:0.8748; eval:0.5160; lr:0.000500
epoch:16; metric:emoval; train:0.8878; eval:0.5364; lr:0.000500
epoch:17; metric:emoval; train:0.8870; eval:0.4843; lr:0.000500
epoch:18; metric:emoval; train:0.8993; eval:0.5259; lr:0.000500
epoch:19; metric:emoval; train:0.8881; eval:0.5227; lr:0.000500
epoch:20; metric:emoval; train:0.8998; eval:0.4948; lr:0.000500
epoch:21; metric:emoval; train:0.8763; eval:0.5113; lr:0.000500
epoch:22; metric:emoval; train:0.8704; eval:0.4667; lr:0.000500
epoch:23; metric:emoval; train:0.8938; eval:0.5254; lr:0.000250
epoch:24; metric:emoval; train:0.9147; eval:0.5441; lr:0.000250
epoch:25; metric:emoval; train:0.9156; eval:0.5155; lr:0.000250
epoch:26; metric:emoval; train:0.9358; eval:0.5345; lr:0.000250
epoch:27; metric:emoval; train:0.9324; eval:0.4987; lr:0.000250
epoch:28; metric:emoval; train:0.9247; eval:0.5156; lr:0.000250
epoch:29; metric:emoval; train:0.9164; eval:0.5249; lr:0.000250
epoch:30; metric:emoval; train:0.9125; eval:0.5028; lr:0.000250
epoch:31; metric:emoval; train:0.9171; eval:0.5324; lr:0.000250
epoch:32; metric:emoval; train:0.9122; eval:0.5587; lr:0.000250
epoch:33; metric:emoval; train:0.8964; eval:0.5386; lr:0.000250
epoch:34; metric:emoval; train:0.9135; eval:0.5476; lr:0.000125
epoch:35; metric:emoval; train:0.9182; eval:0.5686; lr:0.000125
epoch:36; metric:emoval; train:0.9294; eval:0.5319; lr:0.000125
epoch:37; metric:emoval; train:0.9275; eval:0.5066; lr:0.000125
epoch:38; metric:emoval; train:0.9191; eval:0.5276; lr:0.000125
epoch:39; metric:emoval; train:0.9179; eval:0.5372; lr:0.000125
epoch:40; metric:emoval; train:0.9184; eval:0.5473; lr:0.000125
epoch:41; metric:emoval; train:0.9182; eval:0.5413; lr:0.000125
epoch:42; metric:emoval; train:0.9185; eval:0.5531; lr:0.000125
Early stopping at epoch 42, best epoch: 12
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 11, duration: 2474.1954946517944 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3926; eval:0.0148; lr:0.000500
epoch:2; metric:emoval; train:0.1138; eval:0.3707; lr:0.000500
epoch:3; metric:emoval; train:0.3904; eval:0.5085; lr:0.000500
epoch:4; metric:emoval; train:0.5363; eval:0.4893; lr:0.000500
epoch:5; metric:emoval; train:0.6315; eval:0.5489; lr:0.000500
epoch:6; metric:emoval; train:0.6753; eval:0.5115; lr:0.000500
epoch:7; metric:emoval; train:0.7089; eval:0.5198; lr:0.000500
epoch:8; metric:emoval; train:0.7471; eval:0.5499; lr:0.000500
epoch:9; metric:emoval; train:0.7782; eval:0.5633; lr:0.000500
epoch:10; metric:emoval; train:0.7838; eval:0.5634; lr:0.000500
epoch:11; metric:emoval; train:0.8078; eval:0.5428; lr:0.000500
epoch:12; metric:emoval; train:0.8406; eval:0.5037; lr:0.000500
epoch:13; metric:emoval; train:0.8394; eval:0.5553; lr:0.000500
epoch:14; metric:emoval; train:0.8461; eval:0.5529; lr:0.000500
epoch:15; metric:emoval; train:0.8562; eval:0.5226; lr:0.000500
epoch:16; metric:emoval; train:0.8692; eval:0.5565; lr:0.000500
epoch:17; metric:emoval; train:0.8369; eval:0.5516; lr:0.000500
epoch:18; metric:emoval; train:0.8581; eval:0.5652; lr:0.000500
epoch:19; metric:emoval; train:0.8804; eval:0.5292; lr:0.000500
epoch:20; metric:emoval; train:0.8992; eval:0.5626; lr:0.000500
epoch:21; metric:emoval; train:0.8956; eval:0.5563; lr:0.000500
epoch:22; metric:emoval; train:0.8859; eval:0.5627; lr:0.000500
epoch:23; metric:emoval; train:0.8714; eval:0.5310; lr:0.000500
epoch:24; metric:emoval; train:0.8874; eval:0.5554; lr:0.000500
epoch:25; metric:emoval; train:0.8834; eval:0.5321; lr:0.000500
epoch:26; metric:emoval; train:0.8794; eval:0.5357; lr:0.000500
epoch:27; metric:emoval; train:0.9009; eval:0.5161; lr:0.000500
epoch:28; metric:emoval; train:0.8832; eval:0.5302; lr:0.000500
epoch:29; metric:emoval; train:0.8273; eval:0.5500; lr:0.000250
epoch:30; metric:emoval; train:0.9016; eval:0.5960; lr:0.000250
epoch:31; metric:emoval; train:0.9167; eval:0.5901; lr:0.000250
epoch:32; metric:emoval; train:0.9200; eval:0.5377; lr:0.000250
epoch:33; metric:emoval; train:0.9118; eval:0.5479; lr:0.000250
epoch:34; metric:emoval; train:0.9098; eval:0.5953; lr:0.000250
epoch:35; metric:emoval; train:0.9091; eval:0.5319; lr:0.000250
epoch:36; metric:emoval; train:0.9033; eval:0.5703; lr:0.000250
epoch:37; metric:emoval; train:0.8998; eval:0.5567; lr:0.000250
epoch:38; metric:emoval; train:0.8959; eval:0.5440; lr:0.000250
epoch:39; metric:emoval; train:0.8886; eval:0.5413; lr:0.000250
epoch:40; metric:emoval; train:0.9080; eval:0.5748; lr:0.000250
epoch:41; metric:emoval; train:0.9021; eval:0.5399; lr:0.000125
epoch:42; metric:emoval; train:0.9149; eval:0.5609; lr:0.000125
epoch:43; metric:emoval; train:0.9243; eval:0.5607; lr:0.000125
epoch:44; metric:emoval; train:0.9227; eval:0.5742; lr:0.000125
epoch:45; metric:emoval; train:0.9161; eval:0.5588; lr:0.000125
epoch:46; metric:emoval; train:0.9215; eval:0.5765; lr:0.000125
epoch:47; metric:emoval; train:0.9059; eval:0.5715; lr:0.000125
epoch:48; metric:emoval; train:0.9213; eval:0.5571; lr:0.000125
epoch:49; metric:emoval; train:0.9043; eval:0.5145; lr:0.000125
epoch:50; metric:emoval; train:0.9276; eval:0.5623; lr:0.000125
epoch:51; metric:emoval; train:0.9247; eval:0.5655; lr:0.000125
epoch:52; metric:emoval; train:0.9229; eval:0.5617; lr:0.000063
epoch:53; metric:emoval; train:0.9314; eval:0.5736; lr:0.000063
epoch:54; metric:emoval; train:0.9377; eval:0.5703; lr:0.000063
epoch:55; metric:emoval; train:0.9241; eval:0.5701; lr:0.000063
epoch:56; metric:emoval; train:0.9423; eval:0.5763; lr:0.000063
epoch:57; metric:emoval; train:0.9368; eval:0.5541; lr:0.000063
epoch:58; metric:emoval; train:0.9314; eval:0.5769; lr:0.000063
epoch:59; metric:emoval; train:0.9377; eval:0.5765; lr:0.000063
epoch:60; metric:emoval; train:0.9390; eval:0.5633; lr:0.000063
Early stopping at epoch 60, best epoch: 30
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 29, duration: 3580.2123811244965 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3676; eval:0.0539; lr:0.000500
epoch:2; metric:emoval; train:0.1340; eval:0.4215; lr:0.000500
epoch:3; metric:emoval; train:0.4225; eval:0.4975; lr:0.000500
epoch:4; metric:emoval; train:0.5329; eval:0.5366; lr:0.000500
epoch:5; metric:emoval; train:0.6333; eval:0.4943; lr:0.000500
epoch:6; metric:emoval; train:0.6672; eval:0.5196; lr:0.000500
epoch:7; metric:emoval; train:0.7163; eval:0.5087; lr:0.000500
epoch:8; metric:emoval; train:0.7246; eval:0.5264; lr:0.000500
epoch:9; metric:emoval; train:0.8018; eval:0.5618; lr:0.000500
epoch:10; metric:emoval; train:0.7503; eval:0.5366; lr:0.000500
epoch:11; metric:emoval; train:0.7152; eval:0.5833; lr:0.000500
epoch:12; metric:emoval; train:0.8056; eval:0.5724; lr:0.000500
epoch:13; metric:emoval; train:0.8507; eval:0.4898; lr:0.000500
epoch:14; metric:emoval; train:0.8536; eval:0.5049; lr:0.000500
epoch:15; metric:emoval; train:0.8713; eval:0.5341; lr:0.000500
epoch:16; metric:emoval; train:0.8920; eval:0.5751; lr:0.000500
epoch:17; metric:emoval; train:0.8838; eval:0.5330; lr:0.000500
epoch:18; metric:emoval; train:0.8654; eval:0.4877; lr:0.000500
epoch:19; metric:emoval; train:0.8889; eval:0.5342; lr:0.000500
epoch:20; metric:emoval; train:0.8846; eval:0.5457; lr:0.000500
epoch:21; metric:emoval; train:0.8998; eval:0.4537; lr:0.000500
epoch:22; metric:emoval; train:0.8867; eval:0.5497; lr:0.000250
epoch:23; metric:emoval; train:0.9270; eval:0.5624; lr:0.000250
epoch:24; metric:emoval; train:0.9344; eval:0.5431; lr:0.000250
epoch:25; metric:emoval; train:0.9308; eval:0.5265; lr:0.000250
epoch:26; metric:emoval; train:0.9282; eval:0.5355; lr:0.000250
epoch:27; metric:emoval; train:0.9326; eval:0.5520; lr:0.000250
epoch:28; metric:emoval; train:0.9266; eval:0.5541; lr:0.000250
epoch:29; metric:emoval; train:0.9123; eval:0.5720; lr:0.000250
epoch:30; metric:emoval; train:0.9204; eval:0.5419; lr:0.000250
epoch:31; metric:emoval; train:0.9236; eval:0.5383; lr:0.000250
epoch:32; metric:emoval; train:0.9227; eval:0.5260; lr:0.000250
epoch:33; metric:emoval; train:0.9059; eval:0.5224; lr:0.000125
epoch:34; metric:emoval; train:0.9086; eval:0.5465; lr:0.000125
epoch:35; metric:emoval; train:0.9228; eval:0.5369; lr:0.000125
epoch:36; metric:emoval; train:0.9231; eval:0.5485; lr:0.000125
epoch:37; metric:emoval; train:0.9156; eval:0.5006; lr:0.000125
epoch:38; metric:emoval; train:0.9200; eval:0.5027; lr:0.000125
epoch:39; metric:emoval; train:0.9272; eval:0.5290; lr:0.000125
epoch:40; metric:emoval; train:0.9096; eval:0.5519; lr:0.000125
epoch:41; metric:emoval; train:0.8993; eval:0.5399; lr:0.000125
Early stopping at epoch 41, best epoch: 11
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 10, duration: 1491.71577501297 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2578; eval:0.1057; lr:0.000500
epoch:2; metric:emoval; train:0.2150; eval:0.3907; lr:0.000500
epoch:3; metric:emoval; train:0.4422; eval:0.4466; lr:0.000500
epoch:4; metric:emoval; train:0.5713; eval:0.4838; lr:0.000500
epoch:5; metric:emoval; train:0.6442; eval:0.4611; lr:0.000500
epoch:6; metric:emoval; train:0.6992; eval:0.5390; lr:0.000500
epoch:7; metric:emoval; train:0.7013; eval:0.4927; lr:0.000500
epoch:8; metric:emoval; train:0.7275; eval:0.5042; lr:0.000500
epoch:9; metric:emoval; train:0.7844; eval:0.4347; lr:0.000500
epoch:10; metric:emoval; train:0.8111; eval:0.4970; lr:0.000500
epoch:11; metric:emoval; train:0.8230; eval:0.4924; lr:0.000500
epoch:12; metric:emoval; train:0.8569; eval:0.4449; lr:0.000500
epoch:13; metric:emoval; train:0.8349; eval:0.4814; lr:0.000500
epoch:14; metric:emoval; train:0.8733; eval:0.4883; lr:0.000500
epoch:15; metric:emoval; train:0.8703; eval:0.5197; lr:0.000500
epoch:16; metric:emoval; train:0.8773; eval:0.4910; lr:0.000500
epoch:17; metric:emoval; train:0.8816; eval:0.4972; lr:0.000250
epoch:18; metric:emoval; train:0.9246; eval:0.5242; lr:0.000250
epoch:19; metric:emoval; train:0.9413; eval:0.5132; lr:0.000250
epoch:20; metric:emoval; train:0.9505; eval:0.4972; lr:0.000250
epoch:21; metric:emoval; train:0.9486; eval:0.4865; lr:0.000250
epoch:22; metric:emoval; train:0.9447; eval:0.4697; lr:0.000250
epoch:23; metric:emoval; train:0.9451; eval:0.4786; lr:0.000250
epoch:24; metric:emoval; train:0.9287; eval:0.4914; lr:0.000250
epoch:25; metric:emoval; train:0.9203; eval:0.4362; lr:0.000250
epoch:26; metric:emoval; train:0.9259; eval:0.4258; lr:0.000250
epoch:27; metric:emoval; train:0.9207; eval:0.4910; lr:0.000250
epoch:28; metric:emoval; train:0.9104; eval:0.5164; lr:0.000125
epoch:29; metric:emoval; train:0.9268; eval:0.4768; lr:0.000125
epoch:30; metric:emoval; train:0.9369; eval:0.5214; lr:0.000125
epoch:31; metric:emoval; train:0.9348; eval:0.5103; lr:0.000125
epoch:32; metric:emoval; train:0.9326; eval:0.5167; lr:0.000125
epoch:33; metric:emoval; train:0.9286; eval:0.5203; lr:0.000125
epoch:34; metric:emoval; train:0.9236; eval:0.4718; lr:0.000125
epoch:35; metric:emoval; train:0.9310; eval:0.4915; lr:0.000125
epoch:36; metric:emoval; train:0.9207; eval:0.4801; lr:0.000125
Early stopping at epoch 36, best epoch: 6
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 5, duration: 762.7079744338989 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7393_acc:0.7406_val:0.6766_1770136801.2949858.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.8113_acc:0.8127_val:0.6933_1770136801.2949858.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7492_acc:0.7500_val:0.6508_1770136801.2949858.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.8818_acc:0.8837_val:78.8558_1770136801.2949858.npz
