====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.3, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=64, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v5', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s]  1%|▏         | 44/3373 [00:00<00:14, 236.82it/s]  4%|▍         | 127/3373 [00:00<00:06, 476.64it/s] 10%|█         | 341/3373 [00:00<00:02, 1097.61it/s] 14%|█▍        | 469/3373 [00:00<00:02, 1158.04it/s] 18%|█▊        | 597/3373 [00:00<00:03, 915.60it/s]  21%|██        | 703/3373 [00:00<00:04, 619.62it/s] 24%|██▍       | 802/3373 [00:01<00:03, 692.68it/s] 26%|██▋       | 890/3373 [00:01<00:03, 729.67it/s] 29%|██▉       | 977/3373 [00:01<00:03, 616.47it/s] 31%|███       | 1054/3373 [00:01<00:03, 649.11it/s] 33%|███▎      | 1129/3373 [00:01<00:03, 670.81it/s] 36%|███▋      | 1224/3373 [00:01<00:02, 740.70it/s] 39%|███▊      | 1305/3373 [00:01<00:03, 614.64it/s] 41%|████      | 1375/3373 [00:01<00:03, 629.61it/s] 43%|████▎     | 1457/3373 [00:02<00:02, 670.15it/s] 46%|████▌     | 1546/3373 [00:02<00:02, 721.97it/s] 50%|████▉     | 1670/3373 [00:02<00:02, 694.09it/s] 52%|█████▏    | 1751/3373 [00:02<00:02, 718.97it/s] 55%|█████▍    | 1853/3373 [00:02<00:01, 794.54it/s] 57%|█████▋    | 1937/3373 [00:02<00:02, 630.92it/s] 60%|█████▉    | 2023/3373 [00:02<00:01, 682.82it/s] 62%|██████▏   | 2099/3373 [00:03<00:02, 562.56it/s] 64%|██████▍   | 2163/3373 [00:03<00:02, 480.58it/s] 67%|██████▋   | 2265/3373 [00:03<00:01, 591.94it/s] 69%|██████▉   | 2334/3373 [00:03<00:01, 606.64it/s] 71%|███████   | 2403/3373 [00:03<00:01, 623.08it/s] 73%|███████▎  | 2471/3373 [00:03<00:01, 509.65it/s] 75%|███████▍  | 2529/3373 [00:03<00:01, 522.59it/s] 77%|███████▋  | 2606/3373 [00:04<00:01, 398.18it/s] 80%|███████▉  | 2685/3373 [00:04<00:01, 473.16it/s] 82%|████████▏ | 2757/3373 [00:04<00:01, 524.97it/s] 84%|████████▎ | 2820/3373 [00:04<00:01, 548.25it/s] 85%|████████▌ | 2882/3373 [00:04<00:01, 446.29it/s] 88%|████████▊ | 2955/3373 [00:04<00:00, 424.84it/s] 90%|█████████ | 3040/3373 [00:04<00:00, 512.78it/s] 93%|█████████▎| 3128/3373 [00:05<00:00, 596.05it/s] 95%|█████████▍| 3197/3373 [00:05<00:00, 617.56it/s] 99%|█████████▊| 3327/3373 [00:05<00:00, 794.55it/s]100%|██████████| 3373/3373 [00:05<00:00, 636.93it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s]  1%|          | 18/3373 [00:00<00:19, 174.95it/s]  2%|▏         | 62/3373 [00:00<00:10, 327.23it/s]  5%|▍         | 152/3373 [00:00<00:05, 580.63it/s]  6%|▋         | 219/3373 [00:00<00:05, 612.44it/s]  8%|▊         | 282/3373 [00:00<00:05, 615.59it/s] 11%|█         | 360/3373 [00:00<00:04, 670.76it/s] 13%|█▎        | 428/3373 [00:00<00:05, 519.35it/s] 15%|█▍        | 505/3373 [00:00<00:04, 583.28it/s] 17%|█▋        | 580/3373 [00:01<00:04, 627.87it/s] 19%|█▉        | 647/3373 [00:01<00:04, 629.38it/s] 21%|██        | 713/3373 [00:01<00:04, 636.45it/s] 23%|██▎       | 779/3373 [00:01<00:05, 501.68it/s] 25%|██▍       | 835/3373 [00:01<00:04, 510.16it/s] 26%|██▋       | 891/3373 [00:01<00:05, 426.33it/s] 28%|██▊       | 939/3373 [00:01<00:05, 436.93it/s] 29%|██▉       | 987/3373 [00:02<00:06, 360.68it/s] 31%|███       | 1029/3373 [00:02<00:06, 371.36it/s] 33%|███▎      | 1102/3373 [00:02<00:07, 311.06it/s] 34%|███▎      | 1138/3373 [00:02<00:08, 268.94it/s] 35%|███▍      | 1169/3373 [00:02<00:08, 275.17it/s] 36%|███▌      | 1200/3373 [00:02<00:07, 277.85it/s] 37%|███▋      | 1239/3373 [00:02<00:08, 252.83it/s] 38%|███▊      | 1267/3373 [00:03<00:10, 209.73it/s] 39%|███▉      | 1324/3373 [00:03<00:07, 279.96it/s] 40%|████      | 1357/3373 [00:03<00:08, 242.44it/s] 42%|████▏     | 1408/3373 [00:03<00:06, 295.80it/s] 43%|████▎     | 1443/3373 [00:03<00:07, 250.75it/s] 44%|████▍     | 1491/3373 [00:03<00:06, 298.62it/s] 45%|████▌     | 1526/3373 [00:04<00:07, 252.51it/s] 48%|████▊     | 1623/3373 [00:04<00:04, 402.50it/s] 50%|████▉     | 1673/3373 [00:04<00:06, 253.70it/s] 51%|█████     | 1712/3373 [00:04<00:06, 275.95it/s] 52%|█████▏    | 1751/3373 [00:04<00:05, 295.69it/s] 53%|█████▎    | 1790/3373 [00:05<00:07, 221.01it/s] 54%|█████▍    | 1827/3373 [00:05<00:07, 211.92it/s] 56%|█████▌    | 1884/3373 [00:05<00:05, 274.52it/s] 57%|█████▋    | 1920/3373 [00:05<00:06, 207.71it/s] 58%|█████▊    | 1949/3373 [00:05<00:06, 221.59it/s] 59%|█████▊    | 1979/3373 [00:05<00:05, 236.69it/s] 60%|█████▉    | 2009/3373 [00:06<00:05, 250.12it/s] 60%|██████    | 2039/3373 [00:06<00:05, 258.87it/s] 61%|██████▏   | 2074/3373 [00:06<00:05, 226.40it/s] 62%|██████▏   | 2100/3373 [00:06<00:06, 193.50it/s] 64%|██████▍   | 2156/3373 [00:06<00:04, 268.21it/s] 65%|██████▍   | 2188/3373 [00:06<00:04, 279.64it/s] 66%|██████▌   | 2220/3373 [00:06<00:03, 288.61it/s] 67%|██████▋   | 2259/3373 [00:07<00:05, 208.46it/s] 69%|██████▉   | 2326/3373 [00:07<00:04, 251.08it/s] 70%|███████   | 2376/3373 [00:07<00:03, 297.59it/s] 71%|███████▏  | 2411/3373 [00:07<00:04, 216.66it/s] 73%|███████▎  | 2474/3373 [00:07<00:03, 287.12it/s] 74%|███████▍  | 2512/3373 [00:07<00:02, 303.80it/s] 77%|███████▋  | 2581/3373 [00:08<00:02, 387.61it/s] 78%|███████▊  | 2629/3373 [00:08<00:02, 337.77it/s] 79%|███████▉  | 2670/3373 [00:08<00:02, 351.02it/s] 80%|████████  | 2713/3373 [00:08<00:01, 369.76it/s] 82%|████████▏ | 2754/3373 [00:08<00:02, 300.79it/s] 83%|████████▎ | 2798/3373 [00:08<00:02, 231.35it/s] 85%|████████▍ | 2851/3373 [00:09<00:01, 285.46it/s] 86%|████████▌ | 2887/3373 [00:09<00:01, 249.70it/s] 88%|████████▊ | 2956/3373 [00:09<00:01, 336.10it/s] 89%|████████▉ | 3007/3373 [00:09<00:00, 373.69it/s] 90%|█████████ | 3052/3373 [00:09<00:01, 315.30it/s] 92%|█████████▏| 3094/3373 [00:09<00:01, 278.03it/s] 94%|█████████▎| 3161/3373 [00:09<00:00, 353.70it/s] 95%|█████████▍| 3204/3373 [00:10<00:00, 260.26it/s] 96%|█████████▌| 3238/3373 [00:10<00:00, 232.48it/s] 97%|█████████▋| 3278/3373 [00:10<00:00, 262.24it/s]100%|██████████| 3373/3373 [00:10<00:00, 321.02it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s]  0%|          | 7/3373 [00:00<01:33, 36.18it/s]  2%|▏         | 67/3373 [00:00<00:12, 267.66it/s]  3%|▎         | 102/3373 [00:00<00:14, 222.54it/s]  5%|▌         | 180/3373 [00:00<00:08, 372.36it/s]  9%|▊         | 292/3373 [00:00<00:05, 582.28it/s] 11%|█         | 361/3373 [00:00<00:07, 392.65it/s] 12%|█▏        | 415/3373 [00:01<00:08, 350.08it/s] 14%|█▎        | 460/3373 [00:01<00:10, 266.85it/s] 17%|█▋        | 590/3373 [00:01<00:06, 441.20it/s] 21%|██        | 702/3373 [00:01<00:04, 572.99it/s] 23%|██▎       | 782/3373 [00:01<00:05, 511.12it/s] 27%|██▋       | 896/3373 [00:01<00:03, 631.62it/s] 29%|██▉       | 976/3373 [00:02<00:04, 552.23it/s] 32%|███▏      | 1075/3373 [00:02<00:03, 636.83it/s] 34%|███▍      | 1151/3373 [00:02<00:05, 399.53it/s] 36%|███▋      | 1228/3373 [00:02<00:04, 456.50it/s] 38%|███▊      | 1292/3373 [00:03<00:05, 357.69it/s] 44%|████▎     | 1475/3373 [00:03<00:03, 595.11it/s] 46%|████▋     | 1564/3373 [00:03<00:03, 552.24it/s] 49%|████▉     | 1648/3373 [00:03<00:02, 602.24it/s] 51%|█████     | 1726/3373 [00:03<00:02, 638.16it/s] 55%|█████▌    | 1862/3373 [00:03<00:01, 802.04it/s] 58%|█████▊    | 1957/3373 [00:03<00:02, 673.34it/s] 60%|██████    | 2038/3373 [00:04<00:03, 426.69it/s] 64%|██████▍   | 2151/3373 [00:04<00:02, 531.56it/s] 66%|██████▌   | 2226/3373 [00:04<00:02, 570.65it/s] 69%|██████▉   | 2328/3373 [00:04<00:01, 664.01it/s] 71%|███████▏  | 2411/3373 [00:04<00:01, 582.59it/s] 74%|███████▍  | 2491/3373 [00:04<00:01, 628.55it/s] 76%|███████▌  | 2565/3373 [00:05<00:01, 527.36it/s] 79%|███████▊  | 2653/3373 [00:05<00:01, 601.99it/s] 82%|████████▏ | 2752/3373 [00:05<00:00, 691.26it/s] 85%|████████▌ | 2881/3373 [00:05<00:00, 839.77it/s] 89%|████████▉ | 3016/3373 [00:05<00:00, 960.47it/s] 93%|█████████▎| 3121/3373 [00:05<00:00, 780.31it/s] 98%|█████████▊| 3313/3373 [00:05<00:00, 858.43it/s]100%|██████████| 3373/3373 [00:05<00:00, 562.89it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]  0%|          | 2/411 [00:00<00:20, 19.81it/s] 13%|█▎        | 52/411 [00:00<00:01, 301.04it/s] 29%|██▉       | 120/411 [00:00<00:00, 336.57it/s] 63%|██████▎   | 257/411 [00:00<00:00, 641.64it/s] 79%|███████▉  | 326/411 [00:00<00:00, 655.36it/s] 96%|█████████▌| 395/411 [00:00<00:00, 518.65it/s]100%|██████████| 411/411 [00:00<00:00, 515.34it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]  1%|▏         | 6/411 [00:00<00:07, 57.84it/s]  7%|▋         | 27/411 [00:00<00:04, 91.37it/s] 41%|████      | 168/411 [00:00<00:00, 541.00it/s] 81%|████████  | 332/411 [00:00<00:00, 897.14it/s]100%|██████████| 411/411 [00:00<00:00, 674.16it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s] 10%|▉         | 40/411 [00:00<00:00, 393.82it/s] 25%|██▍       | 101/411 [00:00<00:00, 501.07it/s] 37%|███▋      | 151/411 [00:00<00:00, 359.46it/s] 46%|████▋     | 191/411 [00:00<00:00, 228.64it/s] 92%|█████████▏| 379/411 [00:00<00:00, 584.82it/s]100%|██████████| 411/411 [00:00<00:00, 514.47it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]  6%|▌         | 23/412 [00:00<00:03, 121.13it/s]  9%|▉         | 37/412 [00:00<00:02, 128.90it/s] 24%|██▍       | 99/412 [00:00<00:00, 313.74it/s] 50%|████▉     | 205/412 [00:00<00:00, 424.23it/s] 73%|███████▎  | 299/412 [00:00<00:00, 562.89it/s]100%|██████████| 412/412 [00:00<00:00, 719.72it/s]100%|██████████| 412/412 [00:00<00:00, 518.63it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]  1%|▏         | 6/412 [00:00<00:06, 59.44it/s] 17%|█▋        | 70/412 [00:00<00:00, 397.34it/s] 43%|████▎     | 177/412 [00:00<00:00, 702.34it/s] 66%|██████▌   | 270/412 [00:00<00:00, 791.85it/s] 85%|████████▍ | 350/412 [00:00<00:00, 585.13it/s]100%|██████████| 412/412 [00:00<00:00, 582.22it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]  5%|▌         | 21/412 [00:00<00:05, 74.68it/s] 46%|████▌     | 188/412 [00:00<00:00, 611.42it/s] 77%|███████▋  | 317/412 [00:00<00:00, 829.71it/s]100%|██████████| 412/412 [00:00<00:00, 843.61it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]  0%|          | 1/834 [00:00<02:35,  5.37it/s] 16%|█▌        | 134/834 [00:00<00:01, 579.97it/s] 31%|███       | 259/834 [00:00<00:00, 835.94it/s] 43%|████▎     | 359/834 [00:00<00:00, 883.29it/s] 56%|█████▌    | 467/834 [00:00<00:00, 948.12it/s] 68%|██████▊   | 569/834 [00:00<00:00, 965.84it/s] 80%|████████  | 671/834 [00:00<00:00, 747.45it/s] 94%|█████████▍| 787/834 [00:00<00:00, 852.22it/s]100%|██████████| 834/834 [00:01<00:00, 766.89it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]  0%|          | 4/834 [00:00<00:41, 19.95it/s]  3%|▎         | 27/834 [00:00<00:07, 107.10it/s] 13%|█▎        | 110/834 [00:00<00:01, 375.54it/s] 19%|█▊        | 156/834 [00:00<00:02, 242.17it/s] 27%|██▋       | 227/834 [00:00<00:01, 344.27it/s] 33%|███▎      | 273/834 [00:01<00:02, 250.01it/s] 37%|███▋      | 309/834 [00:01<00:02, 230.53it/s] 44%|████▎     | 363/834 [00:01<00:01, 286.18it/s] 48%|████▊     | 400/834 [00:01<00:01, 252.96it/s] 52%|█████▏    | 437/834 [00:01<00:01, 273.53it/s] 63%|██████▎   | 526/834 [00:01<00:00, 339.24it/s] 68%|██████▊   | 563/834 [00:01<00:00, 341.98it/s] 76%|███████▌  | 635/834 [00:02<00:00, 427.09it/s] 82%|████████▏ | 682/834 [00:02<00:00, 253.55it/s] 87%|████████▋ | 726/834 [00:02<00:00, 284.76it/s] 93%|█████████▎| 778/834 [00:02<00:00, 329.50it/s] 98%|█████████▊| 821/834 [00:02<00:00, 287.99it/s]100%|██████████| 834/834 [00:02<00:00, 287.56it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]  0%|          | 1/834 [00:00<02:37,  5.29it/s]  5%|▍         | 41/834 [00:00<00:04, 168.93it/s] 13%|█▎        | 107/834 [00:00<00:02, 262.61it/s] 16%|█▌        | 135/834 [00:00<00:02, 266.96it/s] 21%|██        | 177/834 [00:00<00:02, 305.75it/s] 31%|███       | 259/834 [00:00<00:01, 357.97it/s] 38%|███▊      | 319/834 [00:00<00:01, 417.14it/s] 46%|████▋     | 387/834 [00:01<00:00, 480.73it/s] 61%|██████    | 510/834 [00:01<00:00, 542.09it/s] 86%|████████▋ | 721/834 [00:01<00:00, 907.99it/s]100%|██████████| 834/834 [00:01<00:00, 596.47it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2774; eval:-0.1045; lr:0.000500
epoch:2; metric:emoval; train:0.1334; eval:0.2610; lr:0.000500
epoch:3; metric:emoval; train:0.3746; eval:0.3742; lr:0.000500
epoch:4; metric:emoval; train:0.5004; eval:0.4891; lr:0.000500
epoch:5; metric:emoval; train:0.5861; eval:0.4788; lr:0.000500
epoch:6; metric:emoval; train:0.6576; eval:0.3752; lr:0.000500
epoch:7; metric:emoval; train:0.6845; eval:0.5366; lr:0.000500
epoch:8; metric:emoval; train:0.7264; eval:0.5379; lr:0.000500
epoch:9; metric:emoval; train:0.7686; eval:0.4904; lr:0.000500
epoch:10; metric:emoval; train:0.7822; eval:0.3737; lr:0.000500
epoch:11; metric:emoval; train:0.7790; eval:0.4358; lr:0.000500
epoch:12; metric:emoval; train:0.7947; eval:0.4705; lr:0.000500
epoch:13; metric:emoval; train:0.8302; eval:0.4666; lr:0.000500
epoch:14; metric:emoval; train:0.8522; eval:0.4828; lr:0.000500
epoch:15; metric:emoval; train:0.8449; eval:0.4804; lr:0.000500
epoch:16; metric:emoval; train:0.8579; eval:0.4598; lr:0.000500
epoch:17; metric:emoval; train:0.8691; eval:0.4863; lr:0.000500
epoch:18; metric:emoval; train:0.8737; eval:0.4688; lr:0.000500
epoch:19; metric:emoval; train:0.8727; eval:0.4617; lr:0.000250
epoch:20; metric:emoval; train:0.9113; eval:0.4874; lr:0.000250
epoch:21; metric:emoval; train:0.9290; eval:0.4895; lr:0.000250
epoch:22; metric:emoval; train:0.9325; eval:0.4741; lr:0.000250
epoch:23; metric:emoval; train:0.9168; eval:0.4789; lr:0.000250
epoch:24; metric:emoval; train:0.9262; eval:0.4539; lr:0.000250
epoch:25; metric:emoval; train:0.9216; eval:0.4717; lr:0.000250
epoch:26; metric:emoval; train:0.9187; eval:0.4189; lr:0.000250
epoch:27; metric:emoval; train:0.9061; eval:0.4493; lr:0.000250
epoch:28; metric:emoval; train:0.9113; eval:0.4837; lr:0.000250
epoch:29; metric:emoval; train:0.9125; eval:0.4667; lr:0.000250
epoch:30; metric:emoval; train:0.9124; eval:0.4595; lr:0.000125
epoch:31; metric:emoval; train:0.9193; eval:0.4751; lr:0.000125
epoch:32; metric:emoval; train:0.9171; eval:0.4749; lr:0.000125
epoch:33; metric:emoval; train:0.9157; eval:0.4744; lr:0.000125
epoch:34; metric:emoval; train:0.9169; eval:0.4549; lr:0.000125
epoch:35; metric:emoval; train:0.9187; eval:0.4579; lr:0.000125
epoch:36; metric:emoval; train:0.9131; eval:0.4591; lr:0.000125
epoch:37; metric:emoval; train:0.9221; eval:0.4575; lr:0.000125
epoch:38; metric:emoval; train:0.9144; eval:0.4569; lr:0.000125
Early stopping at epoch 38, best epoch: 8
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 7, duration: 2325.704969882965 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1759; eval:0.2052; lr:0.000500
epoch:2; metric:emoval; train:0.1849; eval:0.3132; lr:0.000500
epoch:3; metric:emoval; train:0.3721; eval:0.3713; lr:0.000500
epoch:4; metric:emoval; train:0.4702; eval:0.4793; lr:0.000500
epoch:5; metric:emoval; train:0.5642; eval:0.5294; lr:0.000500
epoch:6; metric:emoval; train:0.6256; eval:0.5329; lr:0.000500
epoch:7; metric:emoval; train:0.6968; eval:0.4543; lr:0.000500
epoch:8; metric:emoval; train:0.7335; eval:0.4700; lr:0.000500
epoch:9; metric:emoval; train:0.7572; eval:0.5320; lr:0.000500
epoch:10; metric:emoval; train:0.7889; eval:0.5197; lr:0.000500
epoch:11; metric:emoval; train:0.8014; eval:0.5389; lr:0.000500
epoch:12; metric:emoval; train:0.8362; eval:0.5511; lr:0.000500
epoch:13; metric:emoval; train:0.8449; eval:0.5227; lr:0.000500
epoch:14; metric:emoval; train:0.8173; eval:0.5394; lr:0.000500
epoch:15; metric:emoval; train:0.8574; eval:0.5507; lr:0.000500
epoch:16; metric:emoval; train:0.8567; eval:0.5495; lr:0.000500
epoch:17; metric:emoval; train:0.8763; eval:0.4956; lr:0.000500
epoch:18; metric:emoval; train:0.8736; eval:0.5236; lr:0.000500
epoch:19; metric:emoval; train:0.8838; eval:0.4767; lr:0.000500
epoch:20; metric:emoval; train:0.8667; eval:0.5359; lr:0.000500
epoch:21; metric:emoval; train:0.8984; eval:0.4824; lr:0.000500
epoch:22; metric:emoval; train:0.8904; eval:0.4801; lr:0.000500
epoch:23; metric:emoval; train:0.8844; eval:0.5330; lr:0.000250
epoch:24; metric:emoval; train:0.9114; eval:0.5203; lr:0.000250
epoch:25; metric:emoval; train:0.9234; eval:0.5447; lr:0.000250
epoch:26; metric:emoval; train:0.9317; eval:0.5291; lr:0.000250
epoch:27; metric:emoval; train:0.9216; eval:0.4882; lr:0.000250
epoch:28; metric:emoval; train:0.9203; eval:0.5307; lr:0.000250
epoch:29; metric:emoval; train:0.9221; eval:0.5189; lr:0.000250
epoch:30; metric:emoval; train:0.9071; eval:0.4892; lr:0.000250
epoch:31; metric:emoval; train:0.9019; eval:0.4461; lr:0.000250
epoch:32; metric:emoval; train:0.9052; eval:0.5024; lr:0.000250
epoch:33; metric:emoval; train:0.9074; eval:0.5405; lr:0.000250
epoch:34; metric:emoval; train:0.8905; eval:0.4813; lr:0.000125
epoch:35; metric:emoval; train:0.9042; eval:0.5403; lr:0.000125
epoch:36; metric:emoval; train:0.9052; eval:0.5270; lr:0.000125
epoch:37; metric:emoval; train:0.9179; eval:0.5188; lr:0.000125
epoch:38; metric:emoval; train:0.9101; eval:0.5601; lr:0.000125
epoch:39; metric:emoval; train:0.9099; eval:0.5164; lr:0.000125
epoch:40; metric:emoval; train:0.9018; eval:0.5290; lr:0.000125
epoch:41; metric:emoval; train:0.9005; eval:0.5243; lr:0.000125
epoch:42; metric:emoval; train:0.9060; eval:0.5303; lr:0.000125
epoch:43; metric:emoval; train:0.9011; eval:0.4831; lr:0.000125
epoch:44; metric:emoval; train:0.9033; eval:0.5415; lr:0.000125
epoch:45; metric:emoval; train:0.9129; eval:0.5054; lr:0.000125
epoch:46; metric:emoval; train:0.9107; eval:0.5400; lr:0.000125
epoch:47; metric:emoval; train:0.9088; eval:0.5007; lr:0.000125
epoch:48; metric:emoval; train:0.9100; eval:0.5247; lr:0.000125
epoch:49; metric:emoval; train:0.9137; eval:0.5092; lr:0.000063
epoch:50; metric:emoval; train:0.9183; eval:0.5301; lr:0.000063
epoch:51; metric:emoval; train:0.9220; eval:0.5254; lr:0.000063
epoch:52; metric:emoval; train:0.9271; eval:0.5277; lr:0.000063
epoch:53; metric:emoval; train:0.9213; eval:0.4944; lr:0.000063
epoch:54; metric:emoval; train:0.9254; eval:0.5307; lr:0.000063
epoch:55; metric:emoval; train:0.9144; eval:0.5335; lr:0.000063
epoch:56; metric:emoval; train:0.9309; eval:0.5245; lr:0.000063
epoch:57; metric:emoval; train:0.9315; eval:0.5201; lr:0.000063
epoch:58; metric:emoval; train:0.9365; eval:0.5244; lr:0.000063
epoch:59; metric:emoval; train:0.9341; eval:0.4933; lr:0.000063
epoch:60; metric:emoval; train:0.9337; eval:0.5400; lr:0.000031
epoch:61; metric:emoval; train:0.9314; eval:0.5248; lr:0.000031
epoch:62; metric:emoval; train:0.9121; eval:0.5192; lr:0.000031
epoch:63; metric:emoval; train:0.9369; eval:0.5235; lr:0.000031
epoch:64; metric:emoval; train:0.9322; eval:0.5217; lr:0.000031
epoch:65; metric:emoval; train:0.9297; eval:0.5178; lr:0.000031
epoch:66; metric:emoval; train:0.9338; eval:0.5108; lr:0.000031
epoch:67; metric:emoval; train:0.9304; eval:0.5140; lr:0.000031
epoch:68; metric:emoval; train:0.9349; eval:0.5079; lr:0.000031
Early stopping at epoch 68, best epoch: 38
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 37, duration: 4198.50013923645 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3183; eval:-0.0687; lr:0.000500
epoch:2; metric:emoval; train:0.0873; eval:0.2456; lr:0.000500
epoch:3; metric:emoval; train:0.3376; eval:0.4572; lr:0.000500
epoch:4; metric:emoval; train:0.5134; eval:0.5408; lr:0.000500
epoch:5; metric:emoval; train:0.5911; eval:0.4984; lr:0.000500
epoch:6; metric:emoval; train:0.6269; eval:0.5132; lr:0.000500
epoch:7; metric:emoval; train:0.6824; eval:0.4597; lr:0.000500
epoch:8; metric:emoval; train:0.7301; eval:0.4623; lr:0.000500
epoch:9; metric:emoval; train:0.7820; eval:0.4622; lr:0.000500
epoch:10; metric:emoval; train:0.7813; eval:0.5109; lr:0.000500
epoch:11; metric:emoval; train:0.7862; eval:0.5107; lr:0.000500
epoch:12; metric:emoval; train:0.8046; eval:0.4989; lr:0.000500
epoch:13; metric:emoval; train:0.8257; eval:0.4601; lr:0.000500
epoch:14; metric:emoval; train:0.8491; eval:0.4932; lr:0.000500
epoch:15; metric:emoval; train:0.8510; eval:0.5344; lr:0.000250
epoch:16; metric:emoval; train:0.8970; eval:0.5147; lr:0.000250
epoch:17; metric:emoval; train:0.9187; eval:0.5120; lr:0.000250
epoch:18; metric:emoval; train:0.9286; eval:0.4806; lr:0.000250
epoch:19; metric:emoval; train:0.9215; eval:0.5177; lr:0.000250
epoch:20; metric:emoval; train:0.9362; eval:0.5147; lr:0.000250
epoch:21; metric:emoval; train:0.9331; eval:0.5003; lr:0.000250
epoch:22; metric:emoval; train:0.9315; eval:0.5363; lr:0.000250
epoch:23; metric:emoval; train:0.9106; eval:0.4870; lr:0.000250
epoch:24; metric:emoval; train:0.9097; eval:0.5253; lr:0.000250
epoch:25; metric:emoval; train:0.8992; eval:0.5153; lr:0.000250
epoch:26; metric:emoval; train:0.9118; eval:0.5046; lr:0.000125
epoch:27; metric:emoval; train:0.9250; eval:0.5233; lr:0.000125
epoch:28; metric:emoval; train:0.9337; eval:0.4828; lr:0.000125
epoch:29; metric:emoval; train:0.9248; eval:0.5056; lr:0.000125
epoch:30; metric:emoval; train:0.9265; eval:0.5184; lr:0.000125
epoch:31; metric:emoval; train:0.9242; eval:0.5196; lr:0.000125
epoch:32; metric:emoval; train:0.9343; eval:0.5344; lr:0.000125
epoch:33; metric:emoval; train:0.9002; eval:0.4729; lr:0.000125
epoch:34; metric:emoval; train:0.9096; eval:0.5001; lr:0.000125
Early stopping at epoch 34, best epoch: 4
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 3, duration: 1983.1040062904358 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2670; eval:-0.0907; lr:0.000500
epoch:2; metric:emoval; train:0.0962; eval:0.3686; lr:0.000500
epoch:3; metric:emoval; train:0.3501; eval:0.2932; lr:0.000500
epoch:4; metric:emoval; train:0.4663; eval:0.5222; lr:0.000500
epoch:5; metric:emoval; train:0.5866; eval:0.5438; lr:0.000500
epoch:6; metric:emoval; train:0.6684; eval:0.4632; lr:0.000500
epoch:7; metric:emoval; train:0.7109; eval:0.5122; lr:0.000500
epoch:8; metric:emoval; train:0.7341; eval:0.4095; lr:0.000500
epoch:9; metric:emoval; train:0.7705; eval:0.5001; lr:0.000500
epoch:10; metric:emoval; train:0.8107; eval:0.5102; lr:0.000500
epoch:11; metric:emoval; train:0.8059; eval:0.4886; lr:0.000500
epoch:12; metric:emoval; train:0.8123; eval:0.4963; lr:0.000500
epoch:13; metric:emoval; train:0.8194; eval:0.4965; lr:0.000500
epoch:14; metric:emoval; train:0.8668; eval:0.4540; lr:0.000500
epoch:15; metric:emoval; train:0.8589; eval:0.4952; lr:0.000500
epoch:16; metric:emoval; train:0.8729; eval:0.5210; lr:0.000250
epoch:17; metric:emoval; train:0.9093; eval:0.5261; lr:0.000250
epoch:18; metric:emoval; train:0.9274; eval:0.4950; lr:0.000250
epoch:19; metric:emoval; train:0.9302; eval:0.5191; lr:0.000250
epoch:20; metric:emoval; train:0.9386; eval:0.4896; lr:0.000250
epoch:21; metric:emoval; train:0.9331; eval:0.5065; lr:0.000250
epoch:22; metric:emoval; train:0.9300; eval:0.5361; lr:0.000250
epoch:23; metric:emoval; train:0.9258; eval:0.5050; lr:0.000250
epoch:24; metric:emoval; train:0.9312; eval:0.5129; lr:0.000250
epoch:25; metric:emoval; train:0.9123; eval:0.5224; lr:0.000250
epoch:26; metric:emoval; train:0.9130; eval:0.4882; lr:0.000250
epoch:27; metric:emoval; train:0.9094; eval:0.5415; lr:0.000125
epoch:28; metric:emoval; train:0.9161; eval:0.5395; lr:0.000125
epoch:29; metric:emoval; train:0.9235; eval:0.5303; lr:0.000125
epoch:30; metric:emoval; train:0.9234; eval:0.4884; lr:0.000125
epoch:31; metric:emoval; train:0.9276; eval:0.5207; lr:0.000125
epoch:32; metric:emoval; train:0.9205; eval:0.5008; lr:0.000125
epoch:33; metric:emoval; train:0.9324; eval:0.5123; lr:0.000125
epoch:34; metric:emoval; train:0.9187; eval:0.4993; lr:0.000125
epoch:35; metric:emoval; train:0.9129; eval:0.5215; lr:0.000125
Early stopping at epoch 35, best epoch: 5
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 4, duration: 1884.8891940116882 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2333; eval:-0.0140; lr:0.000500
epoch:2; metric:emoval; train:0.1374; eval:0.2894; lr:0.000500
epoch:3; metric:emoval; train:0.3149; eval:0.1776; lr:0.000500
epoch:4; metric:emoval; train:0.4537; eval:0.5139; lr:0.000500
epoch:5; metric:emoval; train:0.5549; eval:0.4909; lr:0.000500
epoch:6; metric:emoval; train:0.6166; eval:0.2538; lr:0.000500
epoch:7; metric:emoval; train:0.6783; eval:0.5442; lr:0.000500
epoch:8; metric:emoval; train:0.6979; eval:0.3817; lr:0.000500
epoch:9; metric:emoval; train:0.7169; eval:0.5336; lr:0.000500
epoch:10; metric:emoval; train:0.7616; eval:0.5227; lr:0.000500
epoch:11; metric:emoval; train:0.7936; eval:0.5181; lr:0.000500
epoch:12; metric:emoval; train:0.8047; eval:0.5321; lr:0.000500
epoch:13; metric:emoval; train:0.8172; eval:0.5002; lr:0.000500
epoch:14; metric:emoval; train:0.8475; eval:0.5085; lr:0.000500
epoch:15; metric:emoval; train:0.8508; eval:0.5314; lr:0.000500
epoch:16; metric:emoval; train:0.8589; eval:0.5394; lr:0.000500
epoch:17; metric:emoval; train:0.8688; eval:0.5339; lr:0.000500
epoch:18; metric:emoval; train:0.8614; eval:0.5144; lr:0.000250
epoch:19; metric:emoval; train:0.9114; eval:0.5641; lr:0.000250
epoch:20; metric:emoval; train:0.9295; eval:0.5359; lr:0.000250
epoch:21; metric:emoval; train:0.9278; eval:0.5330; lr:0.000250
epoch:22; metric:emoval; train:0.9295; eval:0.5243; lr:0.000250
epoch:23; metric:emoval; train:0.9220; eval:0.5347; lr:0.000250
epoch:24; metric:emoval; train:0.9129; eval:0.5275; lr:0.000250
epoch:25; metric:emoval; train:0.9168; eval:0.5240; lr:0.000250
epoch:26; metric:emoval; train:0.9093; eval:0.4743; lr:0.000250
epoch:27; metric:emoval; train:0.9010; eval:0.5389; lr:0.000250
epoch:28; metric:emoval; train:0.8854; eval:0.5271; lr:0.000250
epoch:29; metric:emoval; train:0.8958; eval:0.5254; lr:0.000250
epoch:30; metric:emoval; train:0.8983; eval:0.5171; lr:0.000125
epoch:31; metric:emoval; train:0.9151; eval:0.5186; lr:0.000125
epoch:32; metric:emoval; train:0.9283; eval:0.4942; lr:0.000125
epoch:33; metric:emoval; train:0.9066; eval:0.5144; lr:0.000125
epoch:34; metric:emoval; train:0.9165; eval:0.5230; lr:0.000125
epoch:35; metric:emoval; train:0.9118; eval:0.5288; lr:0.000125
epoch:36; metric:emoval; train:0.8962; eval:0.5238; lr:0.000125
epoch:37; metric:emoval; train:0.8955; eval:0.5287; lr:0.000125
epoch:38; metric:emoval; train:0.9176; eval:0.4971; lr:0.000125
epoch:39; metric:emoval; train:0.8969; eval:0.5214; lr:0.000125
epoch:40; metric:emoval; train:0.8959; eval:0.4790; lr:0.000125
epoch:41; metric:emoval; train:0.9051; eval:0.5140; lr:0.000063
epoch:42; metric:emoval; train:0.9080; eval:0.5145; lr:0.000063
epoch:43; metric:emoval; train:0.9186; eval:0.5246; lr:0.000063
epoch:44; metric:emoval; train:0.9091; eval:0.5161; lr:0.000063
epoch:45; metric:emoval; train:0.9122; eval:0.5256; lr:0.000063
epoch:46; metric:emoval; train:0.9104; eval:0.5168; lr:0.000063
epoch:47; metric:emoval; train:0.9083; eval:0.5158; lr:0.000063
epoch:48; metric:emoval; train:0.9292; eval:0.5129; lr:0.000063
epoch:49; metric:emoval; train:0.9175; eval:0.5220; lr:0.000063
Early stopping at epoch 49, best epoch: 19
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 18, duration: 1751.4063792228699 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7303_acc:0.7338_val:0.7237_1770135032.1994815.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7878_acc:0.7859_val:0.6805_1770135032.1994815.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7655_acc:0.7670_val:0.6422_1770135032.1994815.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.8717_acc:0.8741_val:80.0128_1770135032.1994815.npz
