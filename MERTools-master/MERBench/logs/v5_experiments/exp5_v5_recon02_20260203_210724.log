====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', focal_gamma=2.0, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_dropout=0.15, modality_dropout_warmup=20, model='attention_robust_v5', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.2, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 11%|█         | 367/3373 [00:00<00:00, 3655.93it/s] 22%|██▏       | 733/3373 [00:00<00:00, 3639.94it/s] 33%|███▎      | 1097/3373 [00:00<00:00, 2512.09it/s] 41%|████▏     | 1397/3373 [00:00<00:00, 2662.96it/s] 50%|████▉     | 1686/3373 [00:00<00:00, 2723.39it/s] 59%|█████▊    | 1974/3373 [00:00<00:00, 2688.78it/s] 67%|██████▋   | 2253/3373 [00:00<00:00, 2213.91it/s] 74%|███████▍  | 2508/3373 [00:00<00:00, 2298.43it/s] 82%|████████▏ | 2753/3373 [00:01<00:00, 2243.58it/s] 89%|████████▊ | 2987/3373 [00:01<00:00, 2267.36it/s] 95%|█████████▌| 3221/3373 [00:01<00:00, 1824.95it/s]100%|██████████| 3373/3373 [00:01<00:00, 2399.36it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s]  6%|▋         | 217/3373 [00:00<00:01, 2165.59it/s] 13%|█▎        | 434/3373 [00:00<00:01, 2119.36it/s] 19%|█▉        | 647/3373 [00:00<00:01, 2053.23it/s] 25%|██▌       | 853/3373 [00:00<00:01, 2007.64it/s] 31%|███       | 1054/3373 [00:00<00:01, 1568.66it/s] 39%|███▉      | 1317/3373 [00:00<00:01, 1859.57it/s] 45%|████▍     | 1517/3373 [00:00<00:01, 1850.65it/s] 51%|█████     | 1712/3373 [00:00<00:00, 1878.65it/s] 57%|█████▋    | 1907/3373 [00:01<00:00, 1511.89it/s] 61%|██████▏   | 2073/3373 [00:01<00:00, 1497.53it/s] 66%|██████▌   | 2233/3373 [00:01<00:00, 1499.31it/s] 71%|███████   | 2390/3373 [00:01<00:00, 1482.70it/s] 75%|███████▌  | 2543/3373 [00:01<00:00, 1238.94it/s] 80%|███████▉  | 2688/3373 [00:01<00:00, 1286.95it/s] 84%|████████▍ | 2825/3373 [00:01<00:00, 1306.26it/s] 88%|████████▊ | 2969/3373 [00:01<00:00, 1341.29it/s] 93%|█████████▎| 3128/3373 [00:02<00:00, 1409.73it/s] 99%|█████████▊| 3328/3373 [00:02<00:00, 1575.65it/s]100%|██████████| 3373/3373 [00:02<00:00, 1598.29it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s]  9%|▉         | 298/3373 [00:00<00:01, 2955.82it/s] 18%|█▊        | 594/3373 [00:00<00:00, 2907.80it/s] 26%|██▌       | 885/3373 [00:00<00:00, 2875.79it/s] 35%|███▌      | 1195/3373 [00:00<00:00, 2961.88it/s] 44%|████▍     | 1492/3373 [00:00<00:00, 2945.35it/s] 53%|█████▎    | 1787/3373 [00:00<00:00, 2274.69it/s] 60%|██████    | 2035/3373 [00:00<00:00, 2322.96it/s] 69%|██████▊   | 2313/3373 [00:00<00:00, 2449.30it/s] 76%|███████▋  | 2579/3373 [00:01<00:00, 2488.82it/s] 85%|████████▌ | 2880/3373 [00:01<00:00, 2638.10it/s] 95%|█████████▍| 3199/3373 [00:01<00:00, 2792.22it/s]100%|██████████| 3373/3373 [00:01<00:00, 2774.38it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s] 71%|███████   | 290/411 [00:00<00:00, 2782.19it/s]100%|██████████| 411/411 [00:00<00:00, 2256.49it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s] 62%|██████▏   | 253/411 [00:00<00:00, 2514.45it/s]100%|██████████| 411/411 [00:00<00:00, 2160.13it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s] 57%|█████▋    | 235/411 [00:00<00:00, 2348.37it/s]100%|██████████| 411/411 [00:00<00:00, 2183.08it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s] 43%|████▎     | 176/412 [00:00<00:00, 1753.60it/s]100%|██████████| 412/412 [00:00<00:00, 2073.42it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s] 64%|██████▎   | 262/412 [00:00<00:00, 1557.81it/s]100%|██████████| 412/412 [00:00<00:00, 2238.91it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 4270.17it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s] 34%|███▍      | 287/834 [00:00<00:00, 2869.72it/s] 73%|███████▎  | 605/834 [00:00<00:00, 3049.86it/s]100%|██████████| 834/834 [00:00<00:00, 3857.81it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s] 24%|██▍       | 200/834 [00:00<00:00, 1995.98it/s] 49%|████▉     | 412/834 [00:00<00:00, 2068.70it/s] 87%|████████▋ | 728/834 [00:00<00:00, 2562.43it/s]100%|██████████| 834/834 [00:00<00:00, 2217.35it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s] 57%|█████▋    | 479/834 [00:00<00:00, 4787.94it/s]100%|██████████| 834/834 [00:00<00:00, 6419.33it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.6083; eval:-0.1868; lr:0.000500
epoch:2; metric:emoval; train:-0.0813; eval:0.1656; lr:0.000500
epoch:3; metric:emoval; train:0.3034; eval:0.2878; lr:0.000500
epoch:4; metric:emoval; train:0.4690; eval:0.4882; lr:0.000500
epoch:5; metric:emoval; train:0.5612; eval:0.4977; lr:0.000500
epoch:6; metric:emoval; train:0.6343; eval:0.4992; lr:0.000500
epoch:7; metric:emoval; train:0.6734; eval:0.5421; lr:0.000500
epoch:8; metric:emoval; train:0.7372; eval:0.5459; lr:0.000500
epoch:9; metric:emoval; train:0.7564; eval:0.5634; lr:0.000500
epoch:10; metric:emoval; train:0.8011; eval:0.5419; lr:0.000500
epoch:11; metric:emoval; train:0.8036; eval:0.5545; lr:0.000500
epoch:12; metric:emoval; train:0.8018; eval:0.5399; lr:0.000500
epoch:13; metric:emoval; train:0.8338; eval:0.5070; lr:0.000500
epoch:14; metric:emoval; train:0.8357; eval:0.5199; lr:0.000500
epoch:15; metric:emoval; train:0.8505; eval:0.4934; lr:0.000500
epoch:16; metric:emoval; train:0.8554; eval:0.5104; lr:0.000500
epoch:17; metric:emoval; train:0.8776; eval:0.4949; lr:0.000500
epoch:18; metric:emoval; train:0.8478; eval:0.4836; lr:0.000500
epoch:19; metric:emoval; train:0.8700; eval:0.5297; lr:0.000500
epoch:20; metric:emoval; train:0.8726; eval:0.4509; lr:0.000250
epoch:21; metric:emoval; train:0.9207; eval:0.5186; lr:0.000250
epoch:22; metric:emoval; train:0.9378; eval:0.5288; lr:0.000250
epoch:23; metric:emoval; train:0.9339; eval:0.5086; lr:0.000250
epoch:24; metric:emoval; train:0.9324; eval:0.5191; lr:0.000250
epoch:25; metric:emoval; train:0.9361; eval:0.5047; lr:0.000250
epoch:26; metric:emoval; train:0.9131; eval:0.5386; lr:0.000250
epoch:27; metric:emoval; train:0.9079; eval:0.5239; lr:0.000250
epoch:28; metric:emoval; train:0.9085; eval:0.4954; lr:0.000250
epoch:29; metric:emoval; train:0.9024; eval:0.5131; lr:0.000250
epoch:30; metric:emoval; train:0.9179; eval:0.5090; lr:0.000250
epoch:31; metric:emoval; train:0.9096; eval:0.4526; lr:0.000125
epoch:32; metric:emoval; train:0.9253; eval:0.5316; lr:0.000125
epoch:33; metric:emoval; train:0.9264; eval:0.5443; lr:0.000125
epoch:34; metric:emoval; train:0.9284; eval:0.5148; lr:0.000125
epoch:35; metric:emoval; train:0.9273; eval:0.5142; lr:0.000125
epoch:36; metric:emoval; train:0.9123; eval:0.5218; lr:0.000125
epoch:37; metric:emoval; train:0.9141; eval:0.5216; lr:0.000125
epoch:38; metric:emoval; train:0.9162; eval:0.5087; lr:0.000125
epoch:39; metric:emoval; train:0.9145; eval:0.5351; lr:0.000125
Early stopping at epoch 39, best epoch: 9
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 8, duration: 829.6962695121765 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.4068; eval:-0.3862; lr:0.000500
epoch:2; metric:emoval; train:0.0068; eval:0.2324; lr:0.000500
epoch:3; metric:emoval; train:0.3423; eval:0.4313; lr:0.000500
epoch:4; metric:emoval; train:0.5020; eval:0.4817; lr:0.000500
epoch:5; metric:emoval; train:0.5756; eval:0.4946; lr:0.000500
epoch:6; metric:emoval; train:0.6402; eval:0.4887; lr:0.000500
epoch:7; metric:emoval; train:0.7070; eval:0.5463; lr:0.000500
epoch:8; metric:emoval; train:0.7393; eval:0.5233; lr:0.000500
epoch:9; metric:emoval; train:0.7623; eval:0.5373; lr:0.000500
epoch:10; metric:emoval; train:0.7776; eval:0.4787; lr:0.000500
epoch:11; metric:emoval; train:0.7862; eval:0.5412; lr:0.000500
epoch:12; metric:emoval; train:0.8299; eval:0.4869; lr:0.000500
epoch:13; metric:emoval; train:0.8425; eval:0.4919; lr:0.000500
epoch:14; metric:emoval; train:0.8316; eval:0.5100; lr:0.000500
epoch:15; metric:emoval; train:0.8528; eval:0.5094; lr:0.000500
epoch:16; metric:emoval; train:0.8796; eval:0.5183; lr:0.000500
epoch:17; metric:emoval; train:0.8780; eval:0.5252; lr:0.000500
epoch:18; metric:emoval; train:0.8846; eval:0.5386; lr:0.000250
epoch:19; metric:emoval; train:0.9248; eval:0.5457; lr:0.000250
epoch:20; metric:emoval; train:0.9448; eval:0.5272; lr:0.000250
epoch:21; metric:emoval; train:0.9435; eval:0.5449; lr:0.000250
epoch:22; metric:emoval; train:0.9380; eval:0.5522; lr:0.000250
epoch:23; metric:emoval; train:0.9423; eval:0.5403; lr:0.000250
epoch:24; metric:emoval; train:0.9306; eval:0.5124; lr:0.000250
epoch:25; metric:emoval; train:0.8994; eval:0.5212; lr:0.000250
epoch:26; metric:emoval; train:0.8883; eval:0.5132; lr:0.000250
epoch:27; metric:emoval; train:0.8922; eval:0.5424; lr:0.000250
epoch:28; metric:emoval; train:0.9065; eval:0.5026; lr:0.000250
epoch:29; metric:emoval; train:0.9024; eval:0.5477; lr:0.000250
epoch:30; metric:emoval; train:0.8929; eval:0.5378; lr:0.000250
epoch:31; metric:emoval; train:0.8812; eval:0.5187; lr:0.000250
epoch:32; metric:emoval; train:0.8875; eval:0.5119; lr:0.000250
epoch:33; metric:emoval; train:0.8982; eval:0.5670; lr:0.000250
epoch:34; metric:emoval; train:0.8947; eval:0.5132; lr:0.000250
epoch:35; metric:emoval; train:0.8780; eval:0.5304; lr:0.000250
epoch:36; metric:emoval; train:0.8920; eval:0.5720; lr:0.000250
epoch:37; metric:emoval; train:0.8870; eval:0.5000; lr:0.000250
epoch:38; metric:emoval; train:0.8855; eval:0.4959; lr:0.000250
epoch:39; metric:emoval; train:0.8717; eval:0.4603; lr:0.000250
epoch:40; metric:emoval; train:0.8793; eval:0.4344; lr:0.000250
epoch:41; metric:emoval; train:0.8792; eval:0.5545; lr:0.000250
epoch:42; metric:emoval; train:0.8822; eval:0.5602; lr:0.000250
epoch:43; metric:emoval; train:0.8977; eval:0.5534; lr:0.000250
epoch:44; metric:emoval; train:0.8860; eval:0.5388; lr:0.000250
epoch:45; metric:emoval; train:0.8796; eval:0.5290; lr:0.000250
epoch:46; metric:emoval; train:0.8870; eval:0.5683; lr:0.000250
epoch:47; metric:emoval; train:0.8953; eval:0.5533; lr:0.000125
epoch:48; metric:emoval; train:0.9123; eval:0.5706; lr:0.000125
epoch:49; metric:emoval; train:0.9040; eval:0.5680; lr:0.000125
epoch:50; metric:emoval; train:0.9209; eval:0.5424; lr:0.000125
epoch:51; metric:emoval; train:0.9206; eval:0.5408; lr:0.000125
epoch:52; metric:emoval; train:0.9261; eval:0.5809; lr:0.000125
epoch:53; metric:emoval; train:0.9285; eval:0.5435; lr:0.000125
epoch:54; metric:emoval; train:0.9195; eval:0.5549; lr:0.000125
epoch:55; metric:emoval; train:0.9285; eval:0.5602; lr:0.000125
epoch:56; metric:emoval; train:0.9191; eval:0.5525; lr:0.000125
epoch:57; metric:emoval; train:0.9159; eval:0.5769; lr:0.000125
epoch:58; metric:emoval; train:0.9205; eval:0.5199; lr:0.000125
epoch:59; metric:emoval; train:0.9269; eval:0.5331; lr:0.000125
epoch:60; metric:emoval; train:0.9271; eval:0.5657; lr:0.000125
epoch:61; metric:emoval; train:0.9213; eval:0.5368; lr:0.000125
epoch:62; metric:emoval; train:0.9162; eval:0.5746; lr:0.000125
epoch:63; metric:emoval; train:0.9277; eval:0.5692; lr:0.000063
epoch:64; metric:emoval; train:0.9315; eval:0.5590; lr:0.000063
epoch:65; metric:emoval; train:0.9300; eval:0.5704; lr:0.000063
epoch:66; metric:emoval; train:0.9405; eval:0.5336; lr:0.000063
epoch:67; metric:emoval; train:0.9405; eval:0.5376; lr:0.000063
epoch:68; metric:emoval; train:0.9334; eval:0.5438; lr:0.000063
epoch:69; metric:emoval; train:0.9237; eval:0.5323; lr:0.000063
epoch:70; metric:emoval; train:0.9377; eval:0.5661; lr:0.000063
epoch:71; metric:emoval; train:0.9311; eval:0.5137; lr:0.000063
epoch:72; metric:emoval; train:0.9365; eval:0.5444; lr:0.000063
epoch:73; metric:emoval; train:0.9392; eval:0.5166; lr:0.000063
epoch:74; metric:emoval; train:0.9457; eval:0.5435; lr:0.000031
epoch:75; metric:emoval; train:0.9434; eval:0.5277; lr:0.000031
epoch:76; metric:emoval; train:0.9412; eval:0.5381; lr:0.000031
epoch:77; metric:emoval; train:0.9417; eval:0.5139; lr:0.000031
epoch:78; metric:emoval; train:0.9463; eval:0.5387; lr:0.000031
epoch:79; metric:emoval; train:0.9402; eval:0.5460; lr:0.000031
epoch:80; metric:emoval; train:0.9467; eval:0.5465; lr:0.000031
epoch:81; metric:emoval; train:0.9452; eval:0.5288; lr:0.000031
epoch:82; metric:emoval; train:0.9453; eval:0.5303; lr:0.000031
Early stopping at epoch 82, best epoch: 52
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 51, duration: 5132.00172495842 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3877; eval:0.0228; lr:0.000500
epoch:2; metric:emoval; train:0.1549; eval:0.3553; lr:0.000500
epoch:3; metric:emoval; train:0.3900; eval:0.4956; lr:0.000500
epoch:4; metric:emoval; train:0.5192; eval:0.5265; lr:0.000500
epoch:5; metric:emoval; train:0.6145; eval:0.5035; lr:0.000500
epoch:6; metric:emoval; train:0.6633; eval:0.5486; lr:0.000500
epoch:7; metric:emoval; train:0.7102; eval:0.5700; lr:0.000500
epoch:8; metric:emoval; train:0.7505; eval:0.4791; lr:0.000500
epoch:9; metric:emoval; train:0.7863; eval:0.4618; lr:0.000500
epoch:10; metric:emoval; train:0.8137; eval:0.5706; lr:0.000500
epoch:11; metric:emoval; train:0.8235; eval:0.5691; lr:0.000500
epoch:12; metric:emoval; train:0.8257; eval:0.5667; lr:0.000500
epoch:13; metric:emoval; train:0.8572; eval:0.5558; lr:0.000500
epoch:14; metric:emoval; train:0.8703; eval:0.5261; lr:0.000500
epoch:15; metric:emoval; train:0.8418; eval:0.5523; lr:0.000500
epoch:16; metric:emoval; train:0.8443; eval:0.5285; lr:0.000500
epoch:17; metric:emoval; train:0.8461; eval:0.5541; lr:0.000500
epoch:18; metric:emoval; train:0.8930; eval:0.5273; lr:0.000500
epoch:19; metric:emoval; train:0.8942; eval:0.5683; lr:0.000500
epoch:20; metric:emoval; train:0.8913; eval:0.5263; lr:0.000500
epoch:21; metric:emoval; train:0.9019; eval:0.4895; lr:0.000250
epoch:22; metric:emoval; train:0.9212; eval:0.5319; lr:0.000250
epoch:23; metric:emoval; train:0.9405; eval:0.5352; lr:0.000250
epoch:24; metric:emoval; train:0.9403; eval:0.5600; lr:0.000250
epoch:25; metric:emoval; train:0.9291; eval:0.5813; lr:0.000250
epoch:26; metric:emoval; train:0.9343; eval:0.5599; lr:0.000250
epoch:27; metric:emoval; train:0.9232; eval:0.5441; lr:0.000250
epoch:28; metric:emoval; train:0.9370; eval:0.4740; lr:0.000250
epoch:29; metric:emoval; train:0.9191; eval:0.4969; lr:0.000250
epoch:30; metric:emoval; train:0.9193; eval:0.5551; lr:0.000250
epoch:31; metric:emoval; train:0.9155; eval:0.5161; lr:0.000250
epoch:32; metric:emoval; train:0.8995; eval:0.5505; lr:0.000250
epoch:33; metric:emoval; train:0.8919; eval:0.5559; lr:0.000250
epoch:34; metric:emoval; train:0.8986; eval:0.5626; lr:0.000250
epoch:35; metric:emoval; train:0.8801; eval:0.5348; lr:0.000250
epoch:36; metric:emoval; train:0.8829; eval:0.5244; lr:0.000125
epoch:37; metric:emoval; train:0.8937; eval:0.5614; lr:0.000125
epoch:38; metric:emoval; train:0.9274; eval:0.5813; lr:0.000125
epoch:39; metric:emoval; train:0.9124; eval:0.5546; lr:0.000125
epoch:40; metric:emoval; train:0.9014; eval:0.5663; lr:0.000125
epoch:41; metric:emoval; train:0.9158; eval:0.5459; lr:0.000125
epoch:42; metric:emoval; train:0.9132; eval:0.5419; lr:0.000125
epoch:43; metric:emoval; train:0.9119; eval:0.5341; lr:0.000125
epoch:44; metric:emoval; train:0.9192; eval:0.5436; lr:0.000125
epoch:45; metric:emoval; train:0.9194; eval:0.5624; lr:0.000125
epoch:46; metric:emoval; train:0.9171; eval:0.5487; lr:0.000125
epoch:47; metric:emoval; train:0.9170; eval:0.5617; lr:0.000063
epoch:48; metric:emoval; train:0.9219; eval:0.5390; lr:0.000063
epoch:49; metric:emoval; train:0.9237; eval:0.5528; lr:0.000063
epoch:50; metric:emoval; train:0.9341; eval:0.5431; lr:0.000063
epoch:51; metric:emoval; train:0.9300; eval:0.5663; lr:0.000063
epoch:52; metric:emoval; train:0.9378; eval:0.5473; lr:0.000063
epoch:53; metric:emoval; train:0.9259; eval:0.5608; lr:0.000063
epoch:54; metric:emoval; train:0.9261; eval:0.5358; lr:0.000063
epoch:55; metric:emoval; train:0.9356; eval:0.5565; lr:0.000063
Early stopping at epoch 55, best epoch: 25
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 24, duration: 3341.2963976860046 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.3866; eval:-0.0782; lr:0.000500
epoch:2; metric:emoval; train:0.1789; eval:0.3289; lr:0.000500
epoch:3; metric:emoval; train:0.4517; eval:0.4574; lr:0.000500
epoch:4; metric:emoval; train:0.5570; eval:0.4447; lr:0.000500
epoch:5; metric:emoval; train:0.6169; eval:0.4601; lr:0.000500
epoch:6; metric:emoval; train:0.6781; eval:0.4805; lr:0.000500
epoch:7; metric:emoval; train:0.7119; eval:0.5286; lr:0.000500
epoch:8; metric:emoval; train:0.7600; eval:0.5062; lr:0.000500
epoch:9; metric:emoval; train:0.7523; eval:0.4711; lr:0.000500
epoch:10; metric:emoval; train:0.7916; eval:0.4857; lr:0.000500
epoch:11; metric:emoval; train:0.8024; eval:0.5262; lr:0.000500
epoch:12; metric:emoval; train:0.8212; eval:0.5313; lr:0.000500
epoch:13; metric:emoval; train:0.8469; eval:0.4815; lr:0.000500
epoch:14; metric:emoval; train:0.8360; eval:0.4970; lr:0.000500
epoch:15; metric:emoval; train:0.8506; eval:0.4464; lr:0.000500
epoch:16; metric:emoval; train:0.8965; eval:0.5191; lr:0.000500
epoch:17; metric:emoval; train:0.8444; eval:0.4618; lr:0.000500
epoch:18; metric:emoval; train:0.8804; eval:0.5284; lr:0.000500
epoch:19; metric:emoval; train:0.8656; eval:0.4801; lr:0.000500
epoch:20; metric:emoval; train:0.9080; eval:0.4479; lr:0.000500
epoch:21; metric:emoval; train:0.8344; eval:0.4965; lr:0.000500
epoch:22; metric:emoval; train:0.8846; eval:0.5213; lr:0.000500
epoch:23; metric:emoval; train:0.8792; eval:0.5365; lr:0.000500
epoch:24; metric:emoval; train:0.8827; eval:0.4661; lr:0.000500
epoch:25; metric:emoval; train:0.8879; eval:0.4873; lr:0.000500
epoch:26; metric:emoval; train:0.8940; eval:0.5196; lr:0.000500
epoch:27; metric:emoval; train:0.8400; eval:0.4696; lr:0.000500
epoch:28; metric:emoval; train:0.8704; eval:0.3953; lr:0.000500
epoch:29; metric:emoval; train:0.8694; eval:0.4725; lr:0.000500
epoch:30; metric:emoval; train:0.8617; eval:0.5340; lr:0.000500
epoch:31; metric:emoval; train:0.8567; eval:0.5098; lr:0.000500
epoch:32; metric:emoval; train:0.8278; eval:0.5352; lr:0.000500
epoch:33; metric:emoval; train:0.8240; eval:0.5174; lr:0.000500
epoch:34; metric:emoval; train:0.8207; eval:0.5041; lr:0.000250
epoch:35; metric:emoval; train:0.8637; eval:0.5662; lr:0.000250
epoch:36; metric:emoval; train:0.8926; eval:0.4821; lr:0.000250
epoch:37; metric:emoval; train:0.9073; eval:0.5445; lr:0.000250
epoch:38; metric:emoval; train:0.8774; eval:0.5477; lr:0.000250
epoch:39; metric:emoval; train:0.9041; eval:0.5157; lr:0.000250
epoch:40; metric:emoval; train:0.9060; eval:0.5258; lr:0.000250
epoch:41; metric:emoval; train:0.8877; eval:0.5177; lr:0.000250
epoch:42; metric:emoval; train:0.8904; eval:0.4902; lr:0.000250
epoch:43; metric:emoval; train:0.8836; eval:0.5249; lr:0.000250
epoch:44; metric:emoval; train:0.8821; eval:0.5434; lr:0.000250
epoch:45; metric:emoval; train:0.8890; eval:0.4946; lr:0.000250
epoch:46; metric:emoval; train:0.8849; eval:0.5363; lr:0.000125
epoch:47; metric:emoval; train:0.9051; eval:0.5204; lr:0.000125
epoch:48; metric:emoval; train:0.9126; eval:0.5492; lr:0.000125
epoch:49; metric:emoval; train:0.9148; eval:0.5084; lr:0.000125
epoch:50; metric:emoval; train:0.9172; eval:0.5337; lr:0.000125
epoch:51; metric:emoval; train:0.9287; eval:0.5456; lr:0.000125
epoch:52; metric:emoval; train:0.9134; eval:0.4841; lr:0.000125
epoch:53; metric:emoval; train:0.9219; eval:0.5251; lr:0.000125
epoch:54; metric:emoval; train:0.9321; eval:0.5142; lr:0.000125
epoch:55; metric:emoval; train:0.9349; eval:0.5045; lr:0.000125
epoch:56; metric:emoval; train:0.9175; eval:0.4899; lr:0.000125
epoch:57; metric:emoval; train:0.9300; eval:0.4994; lr:0.000063
epoch:58; metric:emoval; train:0.9229; eval:0.5250; lr:0.000063
epoch:59; metric:emoval; train:0.9338; eval:0.5250; lr:0.000063
epoch:60; metric:emoval; train:0.9222; eval:0.5083; lr:0.000063
epoch:61; metric:emoval; train:0.9318; eval:0.5177; lr:0.000063
epoch:62; metric:emoval; train:0.9234; eval:0.5042; lr:0.000063
epoch:63; metric:emoval; train:0.9412; eval:0.5223; lr:0.000063
epoch:64; metric:emoval; train:0.9312; eval:0.4962; lr:0.000063
epoch:65; metric:emoval; train:0.9313; eval:0.5194; lr:0.000063
Early stopping at epoch 65, best epoch: 35
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 34, duration: 3020.8133039474487 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.4068; eval:-0.1662; lr:0.000500
epoch:2; metric:emoval; train:0.1047; eval:0.3521; lr:0.000500
epoch:3; metric:emoval; train:0.3780; eval:0.5122; lr:0.000500
epoch:4; metric:emoval; train:0.5346; eval:0.4224; lr:0.000500
epoch:5; metric:emoval; train:0.5984; eval:0.5347; lr:0.000500
epoch:6; metric:emoval; train:0.6696; eval:0.4963; lr:0.000500
epoch:7; metric:emoval; train:0.6980; eval:0.4571; lr:0.000500
epoch:8; metric:emoval; train:0.7096; eval:0.5405; lr:0.000500
epoch:9; metric:emoval; train:0.7574; eval:0.4724; lr:0.000500
epoch:10; metric:emoval; train:0.7922; eval:0.4604; lr:0.000500
epoch:11; metric:emoval; train:0.7880; eval:0.4243; lr:0.000500
epoch:12; metric:emoval; train:0.8090; eval:0.4770; lr:0.000500
epoch:13; metric:emoval; train:0.8140; eval:0.5090; lr:0.000500
epoch:14; metric:emoval; train:0.8431; eval:0.5119; lr:0.000500
epoch:15; metric:emoval; train:0.8458; eval:0.4151; lr:0.000500
epoch:16; metric:emoval; train:0.8074; eval:0.4662; lr:0.000500
epoch:17; metric:emoval; train:0.7261; eval:0.4886; lr:0.000500
epoch:18; metric:emoval; train:0.7050; eval:0.4550; lr:0.000500
epoch:19; metric:emoval; train:0.8025; eval:0.5187; lr:0.000250
epoch:20; metric:emoval; train:0.8795; eval:0.5407; lr:0.000250
epoch:21; metric:emoval; train:0.9092; eval:0.5576; lr:0.000250
epoch:22; metric:emoval; train:0.9156; eval:0.5112; lr:0.000250
epoch:23; metric:emoval; train:0.9006; eval:0.5255; lr:0.000250
epoch:24; metric:emoval; train:0.9162; eval:0.5118; lr:0.000250
epoch:25; metric:emoval; train:0.9095; eval:0.4047; lr:0.000250
epoch:26; metric:emoval; train:0.8982; eval:0.5211; lr:0.000250
epoch:27; metric:emoval; train:0.9043; eval:0.5251; lr:0.000250
epoch:28; metric:emoval; train:0.8907; eval:0.5013; lr:0.000250
epoch:29; metric:emoval; train:0.8993; eval:0.5466; lr:0.000250
epoch:30; metric:emoval; train:0.9050; eval:0.4666; lr:0.000250
epoch:31; metric:emoval; train:0.8933; eval:0.5384; lr:0.000250
epoch:32; metric:emoval; train:0.8937; eval:0.5181; lr:0.000125
epoch:33; metric:emoval; train:0.9171; eval:0.5120; lr:0.000125
epoch:34; metric:emoval; train:0.9090; eval:0.5094; lr:0.000125
epoch:35; metric:emoval; train:0.9203; eval:0.5438; lr:0.000125
epoch:36; metric:emoval; train:0.8947; eval:0.5290; lr:0.000125
epoch:37; metric:emoval; train:0.9154; eval:0.5244; lr:0.000125
epoch:38; metric:emoval; train:0.8988; eval:0.5156; lr:0.000125
epoch:39; metric:emoval; train:0.9041; eval:0.5510; lr:0.000125
epoch:40; metric:emoval; train:0.8909; eval:0.5391; lr:0.000125
epoch:41; metric:emoval; train:0.8919; eval:0.4965; lr:0.000125
epoch:42; metric:emoval; train:0.8998; eval:0.5172; lr:0.000125
epoch:43; metric:emoval; train:0.9093; eval:0.5059; lr:0.000063
epoch:44; metric:emoval; train:0.9053; eval:0.4849; lr:0.000063
epoch:45; metric:emoval; train:0.8990; eval:0.4579; lr:0.000063
epoch:46; metric:emoval; train:0.9212; eval:0.5157; lr:0.000063
epoch:47; metric:emoval; train:0.9082; eval:0.4942; lr:0.000063
epoch:48; metric:emoval; train:0.9062; eval:0.5154; lr:0.000063
epoch:49; metric:emoval; train:0.9062; eval:0.5454; lr:0.000063
epoch:50; metric:emoval; train:0.9224; eval:0.5206; lr:0.000063
epoch:51; metric:emoval; train:0.9214; eval:0.5020; lr:0.000063
Early stopping at epoch 51, best epoch: 21
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 20, duration: 1187.595984697342 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7383_acc:0.7382_val:0.6739_1770136395.311066.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7912_acc:0.7908_val:0.6622_1770136395.311066.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.7723_acc:0.7767_val:0.6377_1770136395.311066.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v5+utt+None_f1:0.8751_acc:0.8753_val:80.3440_1770136395.311066.npz
