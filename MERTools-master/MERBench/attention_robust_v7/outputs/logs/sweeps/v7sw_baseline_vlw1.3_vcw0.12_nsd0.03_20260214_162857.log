====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=32, contrastive_temperature=0.07, contrastive_weight=0.1, cross_kl_weight=0.01, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, emo_loss_weight=1.0, epochs=100, feat_scale=1, feat_type='utt', feature_noise_prob=0.35, feature_noise_std=0.03, feature_noise_warmup=5, focal_gamma=2.0, fusion_residual_scale=0.4, fusion_temperature=1.0, gate_alpha=0.5, gpu=0, grad_clip=1.0, hidden_dim=128, huber_beta=0.8, hyper_path=None, kl_warmup_epochs=20, kl_weight=0.01, l2=5e-05, label_smoothing=0.1, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=10, mixup_alpha=0.4, modality_agreement_weight=0.01, modality_dropout=0.18, modality_dropout_warmup=15, model='attention_robust_v7', n_classes=None, num_attention_heads=4, num_workers=0, print_iters=100000000.0, recon_weight=0.1, reg_loss_type='smoothl1', reliability_temperature=1.0, save_iters=100000000.0, save_root='/root/autodl-tmp/MERTools-master/MERBench/attention_robust_v7/outputs/sweep_results/v7sw_baseline_vlw1.3_vcw0.12_nsd0.03_20260214_162857-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_contrastive=True, use_dynamic_kl=True, use_gated_fusion=True, use_gated_uncertainty=True, use_mixup=False, use_modality_dropout=True, use_proxy_attention=True, use_vae=True, use_valence_prior=True, val_loss_weight=1.3, valence_center_reg_weight=0.005, valence_consistency_weight=0.12, video_feature='clip-vit-large-patch14-UTT', weight_consistency_weight=0.02)
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/3373 [00:00<?, ?it/s] 46%|████▋     | 1562/3373 [00:00<00:00, 15582.01it/s] 93%|█████████▎| 3121/3373 [00:00<00:00, 12791.22it/s]100%|██████████| 3373/3373 [00:00<00:00, 13449.05it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/3373 [00:00<?, ?it/s] 30%|██▉       | 997/3373 [00:00<00:00, 9964.97it/s] 59%|█████▉    | 1994/3373 [00:00<00:00, 8614.71it/s] 85%|████████▌ | 2868/3373 [00:00<00:00, 8045.48it/s]100%|██████████| 3373/3373 [00:00<00:00, 8332.14it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/3373 [00:00<?, ?it/s] 48%|████▊     | 1613/3373 [00:00<00:00, 16031.90it/s] 95%|█████████▌| 3217/3373 [00:00<00:00, 14192.91it/s]100%|██████████| 3373/3373 [00:00<00:00, 14572.38it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 11793.12it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 14279.34it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/411 [00:00<?, ?it/s]100%|██████████| 411/411 [00:00<00:00, 15444.27it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 17743.82it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 13663.20it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/412 [00:00<?, ?it/s]100%|██████████| 412/412 [00:00<00:00, 14655.32it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/MERTools-master/MERBench/dataset/mer2023-dataset-process/features/chinese-hubert-large-UTT
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 14796.48it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 10283.27it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|          | 0/834 [00:00<?, ?it/s]100%|██████████| 834/834 [00:00<00:00, 14986.07it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.2374; eval:0.2489; lr:0.000500
epoch:2; metric:emoval; train:0.2916; eval:0.4457; lr:0.000500
epoch:3; metric:emoval; train:0.5032; eval:0.5407; lr:0.000500
epoch:4; metric:emoval; train:0.5624; eval:0.5722; lr:0.000500
epoch:5; metric:emoval; train:0.5907; eval:0.5520; lr:0.000500
epoch:6; metric:emoval; train:0.6434; eval:0.5711; lr:0.000500
epoch:7; metric:emoval; train:0.6607; eval:0.5830; lr:0.000500
epoch:8; metric:emoval; train:0.6740; eval:0.5610; lr:0.000500
epoch:9; metric:emoval; train:0.6930; eval:0.4586; lr:0.000500
epoch:10; metric:emoval; train:0.6901; eval:0.4595; lr:0.000500
epoch:11; metric:emoval; train:0.7053; eval:0.5622; lr:0.000500
epoch:12; metric:emoval; train:0.7270; eval:0.5357; lr:0.000500
epoch:13; metric:emoval; train:0.7313; eval:0.5708; lr:0.000500
epoch:14; metric:emoval; train:0.7346; eval:0.5348; lr:0.000500
epoch:15; metric:emoval; train:0.7186; eval:0.5660; lr:0.000500
epoch:16; metric:emoval; train:0.7617; eval:0.5646; lr:0.000500
epoch:17; metric:emoval; train:0.7386; eval:0.5660; lr:0.000500
epoch:18; metric:emoval; train:0.7392; eval:0.5615; lr:0.000250
epoch:19; metric:emoval; train:0.7855; eval:0.5851; lr:0.000250
epoch:20; metric:emoval; train:0.7996; eval:0.5575; lr:0.000250
epoch:21; metric:emoval; train:0.7960; eval:0.5813; lr:0.000250
epoch:22; metric:emoval; train:0.7810; eval:0.5857; lr:0.000250
epoch:23; metric:emoval; train:0.7888; eval:0.5928; lr:0.000250
epoch:24; metric:emoval; train:0.7760; eval:0.5877; lr:0.000250
epoch:25; metric:emoval; train:0.7631; eval:0.5773; lr:0.000250
epoch:26; metric:emoval; train:0.7866; eval:0.5725; lr:0.000250
epoch:27; metric:emoval; train:0.7816; eval:0.5665; lr:0.000250
epoch:28; metric:emoval; train:0.7776; eval:0.5484; lr:0.000250
epoch:29; metric:emoval; train:0.7529; eval:0.5838; lr:0.000250
epoch:30; metric:emoval; train:0.7683; eval:0.5895; lr:0.000250
epoch:31; metric:emoval; train:0.7642; eval:0.5633; lr:0.000250
epoch:32; metric:emoval; train:0.7691; eval:0.5671; lr:0.000250
epoch:33; metric:emoval; train:0.7647; eval:0.5707; lr:0.000250
epoch:34; metric:emoval; train:0.7625; eval:0.5680; lr:0.000125
epoch:35; metric:emoval; train:0.7856; eval:0.5827; lr:0.000125
epoch:36; metric:emoval; train:0.7970; eval:0.5701; lr:0.000125
epoch:37; metric:emoval; train:0.8070; eval:0.5430; lr:0.000125
epoch:38; metric:emoval; train:0.8163; eval:0.5722; lr:0.000125
epoch:39; metric:emoval; train:0.7953; eval:0.5514; lr:0.000125
epoch:40; metric:emoval; train:0.8105; eval:0.5781; lr:0.000125
epoch:41; metric:emoval; train:0.8134; eval:0.5896; lr:0.000125
epoch:42; metric:emoval; train:0.8141; eval:0.5737; lr:0.000125
epoch:43; metric:emoval; train:0.8233; eval:0.5722; lr:0.000125
epoch:44; metric:emoval; train:0.8116; eval:0.5807; lr:0.000125
epoch:45; metric:emoval; train:0.8159; eval:0.5665; lr:0.000063
epoch:46; metric:emoval; train:0.8225; eval:0.5803; lr:0.000063
epoch:47; metric:emoval; train:0.8292; eval:0.5779; lr:0.000063
epoch:48; metric:emoval; train:0.8384; eval:0.5955; lr:0.000063
epoch:49; metric:emoval; train:0.8259; eval:0.5743; lr:0.000063
epoch:50; metric:emoval; train:0.8353; eval:0.5871; lr:0.000063
epoch:51; metric:emoval; train:0.8404; eval:0.5765; lr:0.000063
epoch:52; metric:emoval; train:0.8266; eval:0.5890; lr:0.000063
epoch:53; metric:emoval; train:0.8418; eval:0.6046; lr:0.000063
epoch:54; metric:emoval; train:0.8270; eval:0.5869; lr:0.000063
epoch:55; metric:emoval; train:0.8443; eval:0.5905; lr:0.000063
epoch:56; metric:emoval; train:0.8439; eval:0.5760; lr:0.000063
epoch:57; metric:emoval; train:0.8336; eval:0.5951; lr:0.000063
epoch:58; metric:emoval; train:0.8491; eval:0.5999; lr:0.000063
epoch:59; metric:emoval; train:0.8528; eval:0.5738; lr:0.000063
epoch:60; metric:emoval; train:0.8223; eval:0.5890; lr:0.000063
epoch:61; metric:emoval; train:0.8377; eval:0.5801; lr:0.000063
epoch:62; metric:emoval; train:0.8367; eval:0.5675; lr:0.000063
epoch:63; metric:emoval; train:0.8375; eval:0.5963; lr:0.000063
epoch:64; metric:emoval; train:0.8391; eval:0.5980; lr:0.000031
epoch:65; metric:emoval; train:0.8509; eval:0.5775; lr:0.000031
epoch:66; metric:emoval; train:0.8562; eval:0.5908; lr:0.000031
epoch:67; metric:emoval; train:0.8514; eval:0.5754; lr:0.000031
epoch:68; metric:emoval; train:0.8492; eval:0.5903; lr:0.000031
epoch:69; metric:emoval; train:0.8519; eval:0.5812; lr:0.000031
epoch:70; metric:emoval; train:0.8455; eval:0.5869; lr:0.000031
epoch:71; metric:emoval; train:0.8631; eval:0.5862; lr:0.000031
epoch:72; metric:emoval; train:0.8611; eval:0.5928; lr:0.000031
epoch:73; metric:emoval; train:0.8560; eval:0.5809; lr:0.000031
epoch:74; metric:emoval; train:0.8510; eval:0.5960; lr:0.000031
epoch:75; metric:emoval; train:0.8448; eval:0.5914; lr:0.000016
epoch:76; metric:emoval; train:0.8683; eval:0.5941; lr:0.000016
epoch:77; metric:emoval; train:0.8573; eval:0.5962; lr:0.000016
epoch:78; metric:emoval; train:0.8552; eval:0.5965; lr:0.000016
epoch:79; metric:emoval; train:0.8626; eval:0.5904; lr:0.000016
epoch:80; metric:emoval; train:0.8654; eval:0.5834; lr:0.000016
epoch:81; metric:emoval; train:0.8577; eval:0.5743; lr:0.000016
epoch:82; metric:emoval; train:0.8757; eval:0.6009; lr:0.000016
epoch:83; metric:emoval; train:0.8640; eval:0.6011; lr:0.000016
Early stopping at epoch 83, best epoch: 53
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 52, duration: 239.23147416114807 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1794; eval:0.2325; lr:0.000500
epoch:2; metric:emoval; train:0.3380; eval:0.4038; lr:0.000500
epoch:3; metric:emoval; train:0.5166; eval:0.4882; lr:0.000500
epoch:4; metric:emoval; train:0.6004; eval:0.5182; lr:0.000500
epoch:5; metric:emoval; train:0.6438; eval:0.4965; lr:0.000500
epoch:6; metric:emoval; train:0.6236; eval:0.5224; lr:0.000500
epoch:7; metric:emoval; train:0.6683; eval:0.5527; lr:0.000500
epoch:8; metric:emoval; train:0.6906; eval:0.4821; lr:0.000500
epoch:9; metric:emoval; train:0.7073; eval:0.5473; lr:0.000500
epoch:10; metric:emoval; train:0.7100; eval:0.5397; lr:0.000500
epoch:11; metric:emoval; train:0.7247; eval:0.5459; lr:0.000500
epoch:12; metric:emoval; train:0.7298; eval:0.4116; lr:0.000500
epoch:13; metric:emoval; train:0.7193; eval:0.5409; lr:0.000500
epoch:14; metric:emoval; train:0.7350; eval:0.5020; lr:0.000500
epoch:15; metric:emoval; train:0.7272; eval:0.5148; lr:0.000500
epoch:16; metric:emoval; train:0.7344; eval:0.5362; lr:0.000500
epoch:17; metric:emoval; train:0.7448; eval:0.5303; lr:0.000500
epoch:18; metric:emoval; train:0.7546; eval:0.5502; lr:0.000250
epoch:19; metric:emoval; train:0.7870; eval:0.5458; lr:0.000250
epoch:20; metric:emoval; train:0.7875; eval:0.5710; lr:0.000250
epoch:21; metric:emoval; train:0.7891; eval:0.5285; lr:0.000250
epoch:22; metric:emoval; train:0.7975; eval:0.5082; lr:0.000250
epoch:23; metric:emoval; train:0.7908; eval:0.5220; lr:0.000250
epoch:24; metric:emoval; train:0.7901; eval:0.5591; lr:0.000250
epoch:25; metric:emoval; train:0.7925; eval:0.5212; lr:0.000250
epoch:26; metric:emoval; train:0.7762; eval:0.5799; lr:0.000250
epoch:27; metric:emoval; train:0.7785; eval:0.5491; lr:0.000250
epoch:28; metric:emoval; train:0.7583; eval:0.5376; lr:0.000250
epoch:29; metric:emoval; train:0.7848; eval:0.5118; lr:0.000250
epoch:30; metric:emoval; train:0.7695; eval:0.5283; lr:0.000250
epoch:31; metric:emoval; train:0.7788; eval:0.5338; lr:0.000250
epoch:32; metric:emoval; train:0.7776; eval:0.5380; lr:0.000250
epoch:33; metric:emoval; train:0.7563; eval:0.5297; lr:0.000250
epoch:34; metric:emoval; train:0.7697; eval:0.5515; lr:0.000250
epoch:35; metric:emoval; train:0.7588; eval:0.5375; lr:0.000250
epoch:36; metric:emoval; train:0.7891; eval:0.5029; lr:0.000250
epoch:37; metric:emoval; train:0.7843; eval:0.5237; lr:0.000125
epoch:38; metric:emoval; train:0.7990; eval:0.5516; lr:0.000125
epoch:39; metric:emoval; train:0.8149; eval:0.5280; lr:0.000125
epoch:40; metric:emoval; train:0.7930; eval:0.5377; lr:0.000125
epoch:41; metric:emoval; train:0.8040; eval:0.5478; lr:0.000125
epoch:42; metric:emoval; train:0.8221; eval:0.5377; lr:0.000125
epoch:43; metric:emoval; train:0.8079; eval:0.5734; lr:0.000125
epoch:44; metric:emoval; train:0.8255; eval:0.5351; lr:0.000125
epoch:45; metric:emoval; train:0.8111; eval:0.5605; lr:0.000125
epoch:46; metric:emoval; train:0.8192; eval:0.5682; lr:0.000125
epoch:47; metric:emoval; train:0.8332; eval:0.5316; lr:0.000125
epoch:48; metric:emoval; train:0.8174; eval:0.5672; lr:0.000063
epoch:49; metric:emoval; train:0.8301; eval:0.5496; lr:0.000063
epoch:50; metric:emoval; train:0.8476; eval:0.5511; lr:0.000063
epoch:51; metric:emoval; train:0.8468; eval:0.5510; lr:0.000063
epoch:52; metric:emoval; train:0.8253; eval:0.5554; lr:0.000063
epoch:53; metric:emoval; train:0.8327; eval:0.5536; lr:0.000063
epoch:54; metric:emoval; train:0.8340; eval:0.5563; lr:0.000063
epoch:55; metric:emoval; train:0.8453; eval:0.5443; lr:0.000063
epoch:56; metric:emoval; train:0.8433; eval:0.5446; lr:0.000063
Early stopping at epoch 56, best epoch: 26
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 25, duration: 154.95435237884521 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.0944; eval:0.3263; lr:0.000500
epoch:2; metric:emoval; train:0.3703; eval:0.5245; lr:0.000500
epoch:3; metric:emoval; train:0.5341; eval:0.5077; lr:0.000500
epoch:4; metric:emoval; train:0.5861; eval:0.5089; lr:0.000500
epoch:5; metric:emoval; train:0.6234; eval:0.5554; lr:0.000500
epoch:6; metric:emoval; train:0.6379; eval:0.5675; lr:0.000500
epoch:7; metric:emoval; train:0.6597; eval:0.3723; lr:0.000500
epoch:8; metric:emoval; train:0.6824; eval:0.5051; lr:0.000500
epoch:9; metric:emoval; train:0.6902; eval:0.5377; lr:0.000500
epoch:10; metric:emoval; train:0.7147; eval:0.5600; lr:0.000500
epoch:11; metric:emoval; train:0.7120; eval:0.5409; lr:0.000500
epoch:12; metric:emoval; train:0.7209; eval:0.5418; lr:0.000500
epoch:13; metric:emoval; train:0.7200; eval:0.5120; lr:0.000500
epoch:14; metric:emoval; train:0.7374; eval:0.5289; lr:0.000500
epoch:15; metric:emoval; train:0.7478; eval:0.5246; lr:0.000500
epoch:16; metric:emoval; train:0.7455; eval:0.5349; lr:0.000500
epoch:17; metric:emoval; train:0.7494; eval:0.4667; lr:0.000250
epoch:18; metric:emoval; train:0.7850; eval:0.5527; lr:0.000250
epoch:19; metric:emoval; train:0.7879; eval:0.5274; lr:0.000250
epoch:20; metric:emoval; train:0.8065; eval:0.5397; lr:0.000250
epoch:21; metric:emoval; train:0.7944; eval:0.5260; lr:0.000250
epoch:22; metric:emoval; train:0.7808; eval:0.5237; lr:0.000250
epoch:23; metric:emoval; train:0.7960; eval:0.5571; lr:0.000250
epoch:24; metric:emoval; train:0.7805; eval:0.5715; lr:0.000250
epoch:25; metric:emoval; train:0.7816; eval:0.5550; lr:0.000250
epoch:26; metric:emoval; train:0.7812; eval:0.5174; lr:0.000250
epoch:27; metric:emoval; train:0.7856; eval:0.5147; lr:0.000250
epoch:28; metric:emoval; train:0.7913; eval:0.4859; lr:0.000250
epoch:29; metric:emoval; train:0.7640; eval:0.5288; lr:0.000250
epoch:30; metric:emoval; train:0.7746; eval:0.5525; lr:0.000250
epoch:31; metric:emoval; train:0.7791; eval:0.5358; lr:0.000250
epoch:32; metric:emoval; train:0.7649; eval:0.5622; lr:0.000250
epoch:33; metric:emoval; train:0.7664; eval:0.5523; lr:0.000250
epoch:34; metric:emoval; train:0.7664; eval:0.5564; lr:0.000250
epoch:35; metric:emoval; train:0.7800; eval:0.5692; lr:0.000125
epoch:36; metric:emoval; train:0.7954; eval:0.5427; lr:0.000125
epoch:37; metric:emoval; train:0.8082; eval:0.5592; lr:0.000125
epoch:38; metric:emoval; train:0.7948; eval:0.5746; lr:0.000125
epoch:39; metric:emoval; train:0.8022; eval:0.5657; lr:0.000125
epoch:40; metric:emoval; train:0.8304; eval:0.5364; lr:0.000125
epoch:41; metric:emoval; train:0.7951; eval:0.5648; lr:0.000125
epoch:42; metric:emoval; train:0.8164; eval:0.5403; lr:0.000125
epoch:43; metric:emoval; train:0.8104; eval:0.5690; lr:0.000125
epoch:44; metric:emoval; train:0.8060; eval:0.5602; lr:0.000125
epoch:45; metric:emoval; train:0.8140; eval:0.5503; lr:0.000125
epoch:46; metric:emoval; train:0.8240; eval:0.5581; lr:0.000125
epoch:47; metric:emoval; train:0.8242; eval:0.5592; lr:0.000125
epoch:48; metric:emoval; train:0.8146; eval:0.5550; lr:0.000125
epoch:49; metric:emoval; train:0.8125; eval:0.5447; lr:0.000063
epoch:50; metric:emoval; train:0.8266; eval:0.5648; lr:0.000063
epoch:51; metric:emoval; train:0.8429; eval:0.5701; lr:0.000063
epoch:52; metric:emoval; train:0.8397; eval:0.5525; lr:0.000063
epoch:53; metric:emoval; train:0.8405; eval:0.5511; lr:0.000063
epoch:54; metric:emoval; train:0.8200; eval:0.5490; lr:0.000063
epoch:55; metric:emoval; train:0.8483; eval:0.5567; lr:0.000063
epoch:56; metric:emoval; train:0.8360; eval:0.5465; lr:0.000063
epoch:57; metric:emoval; train:0.8394; eval:0.5693; lr:0.000063
epoch:58; metric:emoval; train:0.8371; eval:0.5708; lr:0.000063
epoch:59; metric:emoval; train:0.8500; eval:0.5524; lr:0.000063
epoch:60; metric:emoval; train:0.8389; eval:0.5604; lr:0.000031
epoch:61; metric:emoval; train:0.8644; eval:0.5738; lr:0.000031
epoch:62; metric:emoval; train:0.8404; eval:0.5739; lr:0.000031
epoch:63; metric:emoval; train:0.8598; eval:0.5688; lr:0.000031
epoch:64; metric:emoval; train:0.8525; eval:0.5521; lr:0.000031
epoch:65; metric:emoval; train:0.8544; eval:0.5579; lr:0.000031
epoch:66; metric:emoval; train:0.8438; eval:0.5557; lr:0.000031
epoch:67; metric:emoval; train:0.8561; eval:0.5506; lr:0.000031
epoch:68; metric:emoval; train:0.8454; eval:0.5781; lr:0.000031
epoch:69; metric:emoval; train:0.8546; eval:0.5789; lr:0.000031
epoch:70; metric:emoval; train:0.8537; eval:0.5750; lr:0.000031
epoch:71; metric:emoval; train:0.8535; eval:0.5622; lr:0.000031
epoch:72; metric:emoval; train:0.8514; eval:0.5751; lr:0.000031
epoch:73; metric:emoval; train:0.8584; eval:0.5708; lr:0.000031
epoch:74; metric:emoval; train:0.8573; eval:0.5580; lr:0.000031
epoch:75; metric:emoval; train:0.8486; eval:0.5561; lr:0.000031
epoch:76; metric:emoval; train:0.8514; eval:0.5518; lr:0.000031
epoch:77; metric:emoval; train:0.8569; eval:0.5555; lr:0.000031
epoch:78; metric:emoval; train:0.8477; eval:0.5617; lr:0.000031
epoch:79; metric:emoval; train:0.8460; eval:0.5630; lr:0.000031
epoch:80; metric:emoval; train:0.8530; eval:0.5543; lr:0.000016
epoch:81; metric:emoval; train:0.8498; eval:0.5604; lr:0.000016
epoch:82; metric:emoval; train:0.8661; eval:0.5552; lr:0.000016
epoch:83; metric:emoval; train:0.8655; eval:0.5505; lr:0.000016
epoch:84; metric:emoval; train:0.8683; eval:0.5618; lr:0.000016
epoch:85; metric:emoval; train:0.8662; eval:0.5532; lr:0.000016
epoch:86; metric:emoval; train:0.8694; eval:0.5612; lr:0.000016
epoch:87; metric:emoval; train:0.8623; eval:0.5608; lr:0.000016
epoch:88; metric:emoval; train:0.8665; eval:0.5503; lr:0.000016
epoch:89; metric:emoval; train:0.8599; eval:0.5580; lr:0.000016
epoch:90; metric:emoval; train:0.8700; eval:0.5598; lr:0.000016
epoch:91; metric:emoval; train:0.8604; eval:0.5570; lr:0.000008
epoch:92; metric:emoval; train:0.8626; eval:0.5622; lr:0.000008
epoch:93; metric:emoval; train:0.8604; eval:0.5532; lr:0.000008
epoch:94; metric:emoval; train:0.8730; eval:0.5536; lr:0.000008
epoch:95; metric:emoval; train:0.8714; eval:0.5516; lr:0.000008
epoch:96; metric:emoval; train:0.8685; eval:0.5548; lr:0.000008
epoch:97; metric:emoval; train:0.8598; eval:0.5565; lr:0.000008
epoch:98; metric:emoval; train:0.8662; eval:0.5532; lr:0.000008
Early stopping at epoch 98, best epoch: 68
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 68, duration: 279.6107909679413 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.0714; eval:0.3190; lr:0.000500
epoch:2; metric:emoval; train:0.3962; eval:0.4221; lr:0.000500
epoch:3; metric:emoval; train:0.5308; eval:0.5061; lr:0.000500
epoch:4; metric:emoval; train:0.5672; eval:0.5307; lr:0.000500
epoch:5; metric:emoval; train:0.6299; eval:0.5322; lr:0.000500
epoch:6; metric:emoval; train:0.6239; eval:0.5402; lr:0.000500
epoch:7; metric:emoval; train:0.6627; eval:0.5419; lr:0.000500
epoch:8; metric:emoval; train:0.6880; eval:0.6019; lr:0.000500
epoch:9; metric:emoval; train:0.6872; eval:0.5436; lr:0.000500
epoch:10; metric:emoval; train:0.7022; eval:0.5843; lr:0.000500
epoch:11; metric:emoval; train:0.7118; eval:0.5458; lr:0.000500
epoch:12; metric:emoval; train:0.7232; eval:0.5397; lr:0.000500
epoch:13; metric:emoval; train:0.7254; eval:0.5357; lr:0.000500
epoch:14; metric:emoval; train:0.7148; eval:0.5485; lr:0.000500
epoch:15; metric:emoval; train:0.7395; eval:0.5490; lr:0.000500
epoch:16; metric:emoval; train:0.7535; eval:0.5364; lr:0.000500
epoch:17; metric:emoval; train:0.7440; eval:0.5730; lr:0.000500
epoch:18; metric:emoval; train:0.7542; eval:0.5304; lr:0.000500
epoch:19; metric:emoval; train:0.7393; eval:0.5051; lr:0.000250
epoch:20; metric:emoval; train:0.7850; eval:0.5831; lr:0.000250
epoch:21; metric:emoval; train:0.7940; eval:0.5265; lr:0.000250
epoch:22; metric:emoval; train:0.7731; eval:0.5830; lr:0.000250
epoch:23; metric:emoval; train:0.7743; eval:0.5742; lr:0.000250
epoch:24; metric:emoval; train:0.7814; eval:0.5874; lr:0.000250
epoch:25; metric:emoval; train:0.7799; eval:0.5640; lr:0.000250
epoch:26; metric:emoval; train:0.7731; eval:0.6120; lr:0.000250
epoch:27; metric:emoval; train:0.7708; eval:0.5778; lr:0.000250
epoch:28; metric:emoval; train:0.7727; eval:0.5804; lr:0.000250
epoch:29; metric:emoval; train:0.7717; eval:0.5538; lr:0.000250
epoch:30; metric:emoval; train:0.7758; eval:0.5969; lr:0.000250
epoch:31; metric:emoval; train:0.7667; eval:0.5447; lr:0.000250
epoch:32; metric:emoval; train:0.7690; eval:0.5381; lr:0.000250
epoch:33; metric:emoval; train:0.7600; eval:0.5563; lr:0.000250
epoch:34; metric:emoval; train:0.7645; eval:0.5919; lr:0.000250
epoch:35; metric:emoval; train:0.7501; eval:0.4984; lr:0.000250
epoch:36; metric:emoval; train:0.7846; eval:0.5524; lr:0.000250
epoch:37; metric:emoval; train:0.7619; eval:0.5589; lr:0.000125
epoch:38; metric:emoval; train:0.7931; eval:0.5898; lr:0.000125
epoch:39; metric:emoval; train:0.8072; eval:0.5862; lr:0.000125
epoch:40; metric:emoval; train:0.8044; eval:0.5741; lr:0.000125
epoch:41; metric:emoval; train:0.7920; eval:0.5862; lr:0.000125
epoch:42; metric:emoval; train:0.8049; eval:0.5397; lr:0.000125
epoch:43; metric:emoval; train:0.8109; eval:0.5652; lr:0.000125
epoch:44; metric:emoval; train:0.8242; eval:0.6086; lr:0.000125
epoch:45; metric:emoval; train:0.8281; eval:0.5751; lr:0.000125
epoch:46; metric:emoval; train:0.8182; eval:0.5460; lr:0.000125
epoch:47; metric:emoval; train:0.8245; eval:0.5674; lr:0.000125
epoch:48; metric:emoval; train:0.8136; eval:0.5542; lr:0.000063
epoch:49; metric:emoval; train:0.8320; eval:0.5637; lr:0.000063
epoch:50; metric:emoval; train:0.8416; eval:0.5670; lr:0.000063
epoch:51; metric:emoval; train:0.8170; eval:0.5713; lr:0.000063
epoch:52; metric:emoval; train:0.8329; eval:0.5714; lr:0.000063
epoch:53; metric:emoval; train:0.8349; eval:0.5449; lr:0.000063
epoch:54; metric:emoval; train:0.8482; eval:0.5610; lr:0.000063
epoch:55; metric:emoval; train:0.8397; eval:0.5717; lr:0.000063
epoch:56; metric:emoval; train:0.8276; eval:0.5591; lr:0.000063
Early stopping at epoch 56, best epoch: 26
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 25, duration: 171.9126160144806 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.1066; eval:0.2549; lr:0.000500
epoch:2; metric:emoval; train:0.3869; eval:0.4919; lr:0.000500
epoch:3; metric:emoval; train:0.5129; eval:0.4991; lr:0.000500
epoch:4; metric:emoval; train:0.5820; eval:0.5194; lr:0.000500
epoch:5; metric:emoval; train:0.6001; eval:0.5235; lr:0.000500
epoch:6; metric:emoval; train:0.6375; eval:0.5043; lr:0.000500
epoch:7; metric:emoval; train:0.6411; eval:0.3609; lr:0.000500
epoch:8; metric:emoval; train:0.6887; eval:0.5659; lr:0.000500
epoch:9; metric:emoval; train:0.6884; eval:0.4090; lr:0.000500
epoch:10; metric:emoval; train:0.6758; eval:0.5424; lr:0.000500
epoch:11; metric:emoval; train:0.6931; eval:0.5321; lr:0.000500
epoch:12; metric:emoval; train:0.7194; eval:0.5850; lr:0.000500
epoch:13; metric:emoval; train:0.7202; eval:0.5584; lr:0.000500
epoch:14; metric:emoval; train:0.7237; eval:0.5502; lr:0.000500
epoch:15; metric:emoval; train:0.7314; eval:0.5275; lr:0.000500
epoch:16; metric:emoval; train:0.7252; eval:0.5392; lr:0.000500
epoch:17; metric:emoval; train:0.7231; eval:0.5154; lr:0.000500
epoch:18; metric:emoval; train:0.7204; eval:0.5067; lr:0.000500
epoch:19; metric:emoval; train:0.7310; eval:0.5477; lr:0.000500
epoch:20; metric:emoval; train:0.7247; eval:0.5427; lr:0.000500
epoch:21; metric:emoval; train:0.7380; eval:0.5182; lr:0.000500
epoch:22; metric:emoval; train:0.7259; eval:0.5660; lr:0.000500
epoch:23; metric:emoval; train:0.7418; eval:0.5806; lr:0.000250
epoch:24; metric:emoval; train:0.7698; eval:0.5677; lr:0.000250
epoch:25; metric:emoval; train:0.7691; eval:0.5966; lr:0.000250
epoch:26; metric:emoval; train:0.7744; eval:0.5959; lr:0.000250
epoch:27; metric:emoval; train:0.7737; eval:0.5972; lr:0.000250
epoch:28; metric:emoval; train:0.7661; eval:0.6082; lr:0.000250
epoch:29; metric:emoval; train:0.7436; eval:0.5267; lr:0.000250
epoch:30; metric:emoval; train:0.7420; eval:0.5621; lr:0.000250
epoch:31; metric:emoval; train:0.7722; eval:0.5642; lr:0.000250
epoch:32; metric:emoval; train:0.7611; eval:0.5904; lr:0.000250
epoch:33; metric:emoval; train:0.7653; eval:0.5575; lr:0.000250
epoch:34; metric:emoval; train:0.7794; eval:0.5705; lr:0.000250
epoch:35; metric:emoval; train:0.7674; eval:0.5655; lr:0.000250
epoch:36; metric:emoval; train:0.7569; eval:0.5861; lr:0.000250
epoch:37; metric:emoval; train:0.7853; eval:0.5864; lr:0.000250
epoch:38; metric:emoval; train:0.7506; eval:0.5853; lr:0.000250
epoch:39; metric:emoval; train:0.7724; eval:0.6104; lr:0.000250
epoch:40; metric:emoval; train:0.7920; eval:0.5609; lr:0.000250
epoch:41; metric:emoval; train:0.7933; eval:0.5701; lr:0.000250
epoch:42; metric:emoval; train:0.7844; eval:0.5878; lr:0.000250
epoch:43; metric:emoval; train:0.7865; eval:0.5774; lr:0.000250
epoch:44; metric:emoval; train:0.8030; eval:0.5618; lr:0.000250
epoch:45; metric:emoval; train:0.7672; eval:0.5494; lr:0.000250
epoch:46; metric:emoval; train:0.7784; eval:0.5686; lr:0.000250
epoch:47; metric:emoval; train:0.7885; eval:0.5982; lr:0.000250
epoch:48; metric:emoval; train:0.7904; eval:0.5933; lr:0.000250
epoch:49; metric:emoval; train:0.7841; eval:0.5333; lr:0.000250
epoch:50; metric:emoval; train:0.7971; eval:0.5901; lr:0.000125
epoch:51; metric:emoval; train:0.8020; eval:0.5955; lr:0.000125
epoch:52; metric:emoval; train:0.8364; eval:0.5779; lr:0.000125
epoch:53; metric:emoval; train:0.8078; eval:0.5958; lr:0.000125
epoch:54; metric:emoval; train:0.8194; eval:0.5869; lr:0.000125
epoch:55; metric:emoval; train:0.8189; eval:0.5814; lr:0.000125
epoch:56; metric:emoval; train:0.8078; eval:0.6043; lr:0.000125
epoch:57; metric:emoval; train:0.8130; eval:0.5933; lr:0.000125
epoch:58; metric:emoval; train:0.8244; eval:0.5935; lr:0.000125
epoch:59; metric:emoval; train:0.8208; eval:0.5975; lr:0.000125
epoch:60; metric:emoval; train:0.8246; eval:0.5880; lr:0.000125
epoch:61; metric:emoval; train:0.8331; eval:0.5784; lr:0.000063
epoch:62; metric:emoval; train:0.8284; eval:0.5961; lr:0.000063
epoch:63; metric:emoval; train:0.8423; eval:0.5981; lr:0.000063
epoch:64; metric:emoval; train:0.8516; eval:0.6002; lr:0.000063
epoch:65; metric:emoval; train:0.8448; eval:0.5959; lr:0.000063
epoch:66; metric:emoval; train:0.8344; eval:0.6107; lr:0.000063
epoch:67; metric:emoval; train:0.8361; eval:0.6063; lr:0.000063
epoch:68; metric:emoval; train:0.8249; eval:0.5931; lr:0.000063
epoch:69; metric:emoval; train:0.8387; eval:0.5973; lr:0.000063
Early stopping at epoch 69, best epoch: 39
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 65, duration: 202.26726055145264 >>>>>
====== Prediction and Saving =======
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v7/outputs/sweep_results/v7sw_baseline_vlw1.3_vcw0.12_nsd0.03_20260214_162857-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v7+utt+None_f1:0.7547_acc:0.7572_val:0.6298_1771058590.1657307.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v7/outputs/sweep_results/v7sw_baseline_vlw1.3_vcw0.12_nsd0.03_20260214_162857-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v7+utt+None_f1:0.8231_acc:0.8224_val:0.6553_1771058590.1657307.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v7/outputs/sweep_results/v7sw_baseline_vlw1.3_vcw0.12_nsd0.03_20260214_162857-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v7+utt+None_f1:0.7646_acc:0.7694_val:0.6123_1771058590.1657307.npz
save results in /root/autodl-tmp/MERTools-master/MERBench/attention_robust_v7/outputs/sweep_results/v7sw_baseline_vlw1.3_vcw0.12_nsd0.03_20260214_162857-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust_v7+utt+None_f1:0.8853_acc:0.8873_val:80.1998_1771058590.1657307.npz
