====== Params Pre-analysis =======
args:  Namespace(audio_feature='chinese-hubert-large-UTT', batch_size=512, dataset='MER2023', debug=False, dropout=0.35, e2e_dim=None, e2e_name=None, early_stopping_patience=30, epochs=100, feat_scale=1, feat_type='utt', gpu=0, grad_clip=1.0, hidden_dim=128, hyper_path=None, l2=5e-05, lr=0.0005, lr_adjust='case1', lr_factor=0.5, lr_patience=15, modality_dropout=0.2, modality_dropout_warmup=30, model='attention_robust', n_classes=None, num_workers=0, print_iters=100000000.0, save_iters=100000000.0, save_root='./saved-trimodal', savemodel=False, test_dataset=None, text_feature='Baichuan-13B-Base-UTT', train_dataset=None, use_modality_dropout=True, video_feature='clip-vit-large-patch14-UTT')
====== Reading Data =======
train: sample number 3373
audio feature root: /root/autodl-tmp/features/chinese-hubert-large-UTT
  0%|                                                                                                     | 0/3373 [00:00<?, ?it/s] 37%|████████████████████████████████▏                                                      | 1248/3373 [00:00<00:00, 12472.13it/s] 74%|████████████████████████████████████████████████████████████████▍                      | 2496/3373 [00:00<00:00, 11079.18it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 3373/3373 [00:00<00:00, 10341.65it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|                                                                                                     | 0/3373 [00:00<?, ?it/s] 31%|██████████████████████████▉                                                            | 1043/3373 [00:00<00:00, 10218.30it/s] 61%|█████████████████████████████████████████████████████▊                                  | 2065/3373 [00:00<00:00, 7855.87it/s] 85%|███████████████████████████████████████████████████████████████████████████▏            | 2883/3373 [00:00<00:00, 7324.31it/s]100%|████████████████████████████████████████████████████████████████████████████████████████| 3373/3373 [00:00<00:00, 7753.36it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|                                                                                                     | 0/3373 [00:00<?, ?it/s] 40%|██████████████████████████████████▍                                                    | 1335/3373 [00:00<00:00, 13091.08it/s] 78%|████████████████████████████████████████████████████████████████████▏                  | 2645/3373 [00:00<00:00, 12249.38it/s]100%|███████████████████████████████████████████████████████████████████████████████████████| 3373/3373 [00:00<00:00, 13553.85it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test1: sample number 411
audio feature root: /root/autodl-tmp/features/chinese-hubert-large-UTT
  0%|                                                                                                      | 0/411 [00:00<?, ?it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 9452.69it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|                                                                                                      | 0/411 [00:00<?, ?it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 11280.10it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|                                                                                                      | 0/411 [00:00<?, ?it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████| 411/411 [00:00<00:00, 13925.00it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test2: sample number 412
audio feature root: /root/autodl-tmp/features/chinese-hubert-large-UTT
  0%|                                                                                                      | 0/412 [00:00<?, ?it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████| 412/412 [00:00<00:00, 12172.48it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|                                                                                                      | 0/412 [00:00<?, ?it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████| 412/412 [00:00<00:00, 10391.06it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|                                                                                                      | 0/412 [00:00<?, ?it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████| 412/412 [00:00<00:00, 13882.84it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
test3: sample number 834
audio feature root: /root/autodl-tmp/features/chinese-hubert-large-UTT
  0%|                                                                                                      | 0/834 [00:00<?, ?it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████| 834/834 [00:00<00:00, 11576.58it/s]
Input feature chinese-hubert-large-UTT ===> dim is (1, 1024)
  0%|                                                                                                      | 0/834 [00:00<?, ?it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████| 834/834 [00:00<00:00, 8420.86it/s]
Input feature Baichuan-13B-Base-UTT ===> dim is (1, 5120)
  0%|                                                                                                      | 0/834 [00:00<?, ?it/s]100%|█████████████████████████████████████████████████████████████████████████████████████████| 834/834 [00:00<00:00, 11997.37it/s]
Input feature clip-vit-large-patch14-UTT ===> dim is (1, 768)
train&val folder:5; test sets:3
audio dimension: 1024; text dimension: 5120; video dimension: 768
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.8945; eval:-0.3923; lr:0.000500
epoch:2; metric:emoval; train:-0.4785; eval:-0.2916; lr:0.000500
epoch:3; metric:emoval; train:-0.2696; eval:-0.0837; lr:0.000500
epoch:4; metric:emoval; train:-0.0890; eval:-0.0228; lr:0.000500
epoch:5; metric:emoval; train:0.0091; eval:0.1138; lr:0.000500
epoch:6; metric:emoval; train:0.1024; eval:0.1833; lr:0.000500
epoch:7; metric:emoval; train:0.1783; eval:0.2728; lr:0.000500
epoch:8; metric:emoval; train:0.2367; eval:0.3058; lr:0.000500
epoch:9; metric:emoval; train:0.2947; eval:0.3701; lr:0.000500
epoch:10; metric:emoval; train:0.3650; eval:0.4164; lr:0.000500
epoch:11; metric:emoval; train:0.3895; eval:0.4065; lr:0.000500
epoch:12; metric:emoval; train:0.4334; eval:0.4713; lr:0.000500
epoch:13; metric:emoval; train:0.4836; eval:0.4581; lr:0.000500
epoch:14; metric:emoval; train:0.4888; eval:0.5031; lr:0.000500
epoch:15; metric:emoval; train:0.5317; eval:0.4779; lr:0.000500
epoch:16; metric:emoval; train:0.5275; eval:0.5347; lr:0.000500
epoch:17; metric:emoval; train:0.5766; eval:0.5324; lr:0.000500
epoch:18; metric:emoval; train:0.5686; eval:0.5478; lr:0.000500
epoch:19; metric:emoval; train:0.6012; eval:0.5413; lr:0.000500
epoch:20; metric:emoval; train:0.6211; eval:0.5530; lr:0.000500
epoch:21; metric:emoval; train:0.6401; eval:0.5646; lr:0.000500
epoch:22; metric:emoval; train:0.6517; eval:0.5571; lr:0.000500
epoch:23; metric:emoval; train:0.6524; eval:0.5676; lr:0.000500
epoch:24; metric:emoval; train:0.6708; eval:0.5817; lr:0.000500
epoch:25; metric:emoval; train:0.6927; eval:0.5925; lr:0.000500
epoch:26; metric:emoval; train:0.7025; eval:0.5989; lr:0.000500
epoch:27; metric:emoval; train:0.7109; eval:0.5739; lr:0.000500
epoch:28; metric:emoval; train:0.7095; eval:0.5889; lr:0.000500
epoch:29; metric:emoval; train:0.7424; eval:0.6007; lr:0.000500
epoch:30; metric:emoval; train:0.7473; eval:0.5889; lr:0.000500
epoch:31; metric:emoval; train:0.7448; eval:0.5834; lr:0.000500
epoch:32; metric:emoval; train:0.7692; eval:0.5998; lr:0.000500
epoch:33; metric:emoval; train:0.7630; eval:0.5845; lr:0.000500
epoch:34; metric:emoval; train:0.7698; eval:0.5756; lr:0.000500
epoch:35; metric:emoval; train:0.7587; eval:0.6115; lr:0.000500
epoch:36; metric:emoval; train:0.7646; eval:0.5961; lr:0.000500
epoch:37; metric:emoval; train:0.7716; eval:0.5760; lr:0.000500
epoch:38; metric:emoval; train:0.7781; eval:0.5987; lr:0.000500
epoch:39; metric:emoval; train:0.7908; eval:0.6015; lr:0.000500
epoch:40; metric:emoval; train:0.8000; eval:0.5834; lr:0.000500
epoch:41; metric:emoval; train:0.8018; eval:0.5795; lr:0.000500
epoch:42; metric:emoval; train:0.7979; eval:0.5799; lr:0.000500
epoch:43; metric:emoval; train:0.7986; eval:0.5625; lr:0.000500
epoch:44; metric:emoval; train:0.7781; eval:0.5820; lr:0.000500
epoch:45; metric:emoval; train:0.7822; eval:0.5937; lr:0.000500
epoch:46; metric:emoval; train:0.7870; eval:0.5799; lr:0.000500
epoch:47; metric:emoval; train:0.7959; eval:0.5888; lr:0.000500
epoch:48; metric:emoval; train:0.8000; eval:0.5700; lr:0.000500
epoch:49; metric:emoval; train:0.7987; eval:0.5791; lr:0.000500
epoch:50; metric:emoval; train:0.7927; eval:0.5864; lr:0.000500
epoch:51; metric:emoval; train:0.7949; eval:0.5645; lr:0.000250
epoch:52; metric:emoval; train:0.8063; eval:0.5855; lr:0.000250
epoch:53; metric:emoval; train:0.7903; eval:0.5808; lr:0.000250
epoch:54; metric:emoval; train:0.7824; eval:0.5765; lr:0.000250
epoch:55; metric:emoval; train:0.7992; eval:0.5810; lr:0.000250
epoch:56; metric:emoval; train:0.7981; eval:0.5776; lr:0.000250
epoch:57; metric:emoval; train:0.7907; eval:0.5754; lr:0.000250
epoch:58; metric:emoval; train:0.7911; eval:0.5764; lr:0.000250
epoch:59; metric:emoval; train:0.7917; eval:0.5727; lr:0.000250
epoch:60; metric:emoval; train:0.8046; eval:0.5795; lr:0.000250
epoch:61; metric:emoval; train:0.7960; eval:0.5746; lr:0.000250
epoch:62; metric:emoval; train:0.7857; eval:0.5795; lr:0.000250
epoch:63; metric:emoval; train:0.7966; eval:0.5751; lr:0.000250
epoch:64; metric:emoval; train:0.7953; eval:0.5835; lr:0.000250
epoch:65; metric:emoval; train:0.7973; eval:0.5728; lr:0.000250
Early stopping at epoch 65, best epoch: 35
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1-th folder, best_index: 34, duration: 17.25121784210205 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.8565; eval:-0.6341; lr:0.000500
epoch:2; metric:emoval; train:-0.5451; eval:-0.3607; lr:0.000500
epoch:3; metric:emoval; train:-0.3322; eval:-0.1914; lr:0.000500
epoch:4; metric:emoval; train:-0.1321; eval:0.0125; lr:0.000500
epoch:5; metric:emoval; train:0.0058; eval:0.1184; lr:0.000500
epoch:6; metric:emoval; train:0.1177; eval:0.1621; lr:0.000500
epoch:7; metric:emoval; train:0.1698; eval:0.2401; lr:0.000500
epoch:8; metric:emoval; train:0.2495; eval:0.3452; lr:0.000500
epoch:9; metric:emoval; train:0.3329; eval:0.3203; lr:0.000500
epoch:10; metric:emoval; train:0.4002; eval:0.3777; lr:0.000500
epoch:11; metric:emoval; train:0.4328; eval:0.4079; lr:0.000500
epoch:12; metric:emoval; train:0.4731; eval:0.4343; lr:0.000500
epoch:13; metric:emoval; train:0.4972; eval:0.4360; lr:0.000500
epoch:14; metric:emoval; train:0.5364; eval:0.4652; lr:0.000500
epoch:15; metric:emoval; train:0.5536; eval:0.4710; lr:0.000500
epoch:16; metric:emoval; train:0.5751; eval:0.4721; lr:0.000500
epoch:17; metric:emoval; train:0.6026; eval:0.4908; lr:0.000500
epoch:18; metric:emoval; train:0.6159; eval:0.4761; lr:0.000500
epoch:19; metric:emoval; train:0.6349; eval:0.4927; lr:0.000500
epoch:20; metric:emoval; train:0.6405; eval:0.4736; lr:0.000500
epoch:21; metric:emoval; train:0.6648; eval:0.5030; lr:0.000500
epoch:22; metric:emoval; train:0.6910; eval:0.5009; lr:0.000500
epoch:23; metric:emoval; train:0.7039; eval:0.4993; lr:0.000500
epoch:24; metric:emoval; train:0.7070; eval:0.4916; lr:0.000500
epoch:25; metric:emoval; train:0.7174; eval:0.4936; lr:0.000500
epoch:26; metric:emoval; train:0.7208; eval:0.5063; lr:0.000500
epoch:27; metric:emoval; train:0.7378; eval:0.5029; lr:0.000500
epoch:28; metric:emoval; train:0.7416; eval:0.5336; lr:0.000500
epoch:29; metric:emoval; train:0.7617; eval:0.4949; lr:0.000500
epoch:30; metric:emoval; train:0.7707; eval:0.4965; lr:0.000500
epoch:31; metric:emoval; train:0.7832; eval:0.5007; lr:0.000500
epoch:32; metric:emoval; train:0.7941; eval:0.5090; lr:0.000500
epoch:33; metric:emoval; train:0.7787; eval:0.4876; lr:0.000500
epoch:34; metric:emoval; train:0.7875; eval:0.4938; lr:0.000500
epoch:35; metric:emoval; train:0.7752; eval:0.5061; lr:0.000500
epoch:36; metric:emoval; train:0.7899; eval:0.5053; lr:0.000500
epoch:37; metric:emoval; train:0.7903; eval:0.4989; lr:0.000500
epoch:38; metric:emoval; train:0.8005; eval:0.4939; lr:0.000500
epoch:39; metric:emoval; train:0.7958; eval:0.5057; lr:0.000500
epoch:40; metric:emoval; train:0.7838; eval:0.5025; lr:0.000500
epoch:41; metric:emoval; train:0.7943; eval:0.4924; lr:0.000500
epoch:42; metric:emoval; train:0.8140; eval:0.5121; lr:0.000500
epoch:43; metric:emoval; train:0.8030; eval:0.4882; lr:0.000500
epoch:44; metric:emoval; train:0.8064; eval:0.5087; lr:0.000250
epoch:45; metric:emoval; train:0.8078; eval:0.4978; lr:0.000250
epoch:46; metric:emoval; train:0.8041; eval:0.5117; lr:0.000250
epoch:47; metric:emoval; train:0.7981; eval:0.4954; lr:0.000250
epoch:48; metric:emoval; train:0.8204; eval:0.5061; lr:0.000250
epoch:49; metric:emoval; train:0.8144; eval:0.4814; lr:0.000250
epoch:50; metric:emoval; train:0.8112; eval:0.5080; lr:0.000250
epoch:51; metric:emoval; train:0.8041; eval:0.4898; lr:0.000250
epoch:52; metric:emoval; train:0.7996; eval:0.5125; lr:0.000250
epoch:53; metric:emoval; train:0.7939; eval:0.5055; lr:0.000250
epoch:54; metric:emoval; train:0.7893; eval:0.4957; lr:0.000250
epoch:55; metric:emoval; train:0.7783; eval:0.5048; lr:0.000250
epoch:56; metric:emoval; train:0.8096; eval:0.5049; lr:0.000250
epoch:57; metric:emoval; train:0.8040; eval:0.5003; lr:0.000250
epoch:58; metric:emoval; train:0.7944; eval:0.5118; lr:0.000250
Early stopping at epoch 58, best epoch: 28
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2-th folder, best_index: 27, duration: 11.964590549468994 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.6857; eval:-0.4742; lr:0.000500
epoch:2; metric:emoval; train:-0.4184; eval:-0.1500; lr:0.000500
epoch:3; metric:emoval; train:-0.1612; eval:-0.0073; lr:0.000500
epoch:4; metric:emoval; train:-0.0092; eval:0.1357; lr:0.000500
epoch:5; metric:emoval; train:0.0988; eval:0.2621; lr:0.000500
epoch:6; metric:emoval; train:0.1841; eval:0.3047; lr:0.000500
epoch:7; metric:emoval; train:0.2678; eval:0.3713; lr:0.000500
epoch:8; metric:emoval; train:0.3095; eval:0.3535; lr:0.000500
epoch:9; metric:emoval; train:0.3557; eval:0.4225; lr:0.000500
epoch:10; metric:emoval; train:0.4016; eval:0.4387; lr:0.000500
epoch:11; metric:emoval; train:0.4410; eval:0.4702; lr:0.000500
epoch:12; metric:emoval; train:0.4636; eval:0.4638; lr:0.000500
epoch:13; metric:emoval; train:0.4776; eval:0.5120; lr:0.000500
epoch:14; metric:emoval; train:0.5272; eval:0.4928; lr:0.000500
epoch:15; metric:emoval; train:0.5500; eval:0.5140; lr:0.000500
epoch:16; metric:emoval; train:0.5678; eval:0.5093; lr:0.000500
epoch:17; metric:emoval; train:0.5818; eval:0.5255; lr:0.000500
epoch:18; metric:emoval; train:0.5997; eval:0.5505; lr:0.000500
epoch:19; metric:emoval; train:0.6208; eval:0.5464; lr:0.000500
epoch:20; metric:emoval; train:0.6336; eval:0.5532; lr:0.000500
epoch:21; metric:emoval; train:0.6375; eval:0.5049; lr:0.000500
epoch:22; metric:emoval; train:0.6489; eval:0.5657; lr:0.000500
epoch:23; metric:emoval; train:0.6703; eval:0.5678; lr:0.000500
epoch:24; metric:emoval; train:0.6853; eval:0.5710; lr:0.000500
epoch:25; metric:emoval; train:0.7075; eval:0.5666; lr:0.000500
epoch:26; metric:emoval; train:0.7072; eval:0.5677; lr:0.000500
epoch:27; metric:emoval; train:0.7140; eval:0.5617; lr:0.000500
epoch:28; metric:emoval; train:0.7257; eval:0.5537; lr:0.000500
epoch:29; metric:emoval; train:0.7406; eval:0.5546; lr:0.000500
epoch:30; metric:emoval; train:0.7455; eval:0.5706; lr:0.000500
epoch:31; metric:emoval; train:0.7527; eval:0.5654; lr:0.000500
epoch:32; metric:emoval; train:0.7524; eval:0.5692; lr:0.000500
epoch:33; metric:emoval; train:0.7652; eval:0.5610; lr:0.000500
epoch:34; metric:emoval; train:0.7675; eval:0.5771; lr:0.000500
epoch:35; metric:emoval; train:0.7588; eval:0.5478; lr:0.000500
epoch:36; metric:emoval; train:0.7759; eval:0.5845; lr:0.000500
epoch:37; metric:emoval; train:0.7820; eval:0.5582; lr:0.000500
epoch:38; metric:emoval; train:0.7811; eval:0.5661; lr:0.000500
epoch:39; metric:emoval; train:0.7726; eval:0.5784; lr:0.000500
epoch:40; metric:emoval; train:0.8015; eval:0.5646; lr:0.000500
epoch:41; metric:emoval; train:0.7915; eval:0.5704; lr:0.000500
epoch:42; metric:emoval; train:0.7875; eval:0.5540; lr:0.000500
epoch:43; metric:emoval; train:0.7907; eval:0.5648; lr:0.000500
epoch:44; metric:emoval; train:0.7899; eval:0.5503; lr:0.000500
epoch:45; metric:emoval; train:0.7859; eval:0.5680; lr:0.000500
epoch:46; metric:emoval; train:0.8009; eval:0.5447; lr:0.000500
epoch:47; metric:emoval; train:0.7718; eval:0.5863; lr:0.000500
epoch:48; metric:emoval; train:0.8018; eval:0.5688; lr:0.000500
epoch:49; metric:emoval; train:0.8005; eval:0.5814; lr:0.000500
epoch:50; metric:emoval; train:0.7890; eval:0.5757; lr:0.000500
epoch:51; metric:emoval; train:0.8013; eval:0.5795; lr:0.000500
epoch:52; metric:emoval; train:0.7984; eval:0.5806; lr:0.000500
epoch:53; metric:emoval; train:0.7945; eval:0.5555; lr:0.000500
epoch:54; metric:emoval; train:0.8134; eval:0.5900; lr:0.000500
epoch:55; metric:emoval; train:0.8022; eval:0.5690; lr:0.000500
epoch:56; metric:emoval; train:0.7903; eval:0.5710; lr:0.000500
epoch:57; metric:emoval; train:0.7881; eval:0.5655; lr:0.000500
epoch:58; metric:emoval; train:0.7991; eval:0.5522; lr:0.000500
epoch:59; metric:emoval; train:0.7846; eval:0.5825; lr:0.000500
epoch:60; metric:emoval; train:0.7938; eval:0.5513; lr:0.000500
epoch:61; metric:emoval; train:0.7967; eval:0.5742; lr:0.000500
epoch:62; metric:emoval; train:0.7867; eval:0.5559; lr:0.000500
epoch:63; metric:emoval; train:0.7943; eval:0.5620; lr:0.000500
epoch:64; metric:emoval; train:0.7924; eval:0.5617; lr:0.000500
epoch:65; metric:emoval; train:0.7951; eval:0.5714; lr:0.000500
epoch:66; metric:emoval; train:0.7998; eval:0.5397; lr:0.000500
epoch:67; metric:emoval; train:0.7903; eval:0.5745; lr:0.000500
epoch:68; metric:emoval; train:0.7953; eval:0.5334; lr:0.000500
epoch:69; metric:emoval; train:0.7975; eval:0.5743; lr:0.000500
epoch:70; metric:emoval; train:0.8175; eval:0.5558; lr:0.000250
epoch:71; metric:emoval; train:0.8096; eval:0.5621; lr:0.000250
epoch:72; metric:emoval; train:0.8141; eval:0.5655; lr:0.000250
epoch:73; metric:emoval; train:0.8218; eval:0.5593; lr:0.000250
epoch:74; metric:emoval; train:0.8355; eval:0.5582; lr:0.000250
epoch:75; metric:emoval; train:0.8317; eval:0.5469; lr:0.000250
epoch:76; metric:emoval; train:0.8336; eval:0.5686; lr:0.000250
epoch:77; metric:emoval; train:0.8357; eval:0.5621; lr:0.000250
epoch:78; metric:emoval; train:0.8242; eval:0.5707; lr:0.000250
epoch:79; metric:emoval; train:0.8353; eval:0.5692; lr:0.000250
epoch:80; metric:emoval; train:0.8179; eval:0.5511; lr:0.000250
epoch:81; metric:emoval; train:0.8330; eval:0.5613; lr:0.000250
epoch:82; metric:emoval; train:0.8404; eval:0.5506; lr:0.000250
epoch:83; metric:emoval; train:0.8416; eval:0.5431; lr:0.000250
epoch:84; metric:emoval; train:0.8252; eval:0.5553; lr:0.000250
Early stopping at epoch 84, best epoch: 54
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3-th folder, best_index: 53, duration: 18.876964569091797 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.8657; eval:-0.4723; lr:0.000500
epoch:2; metric:emoval; train:-0.4769; eval:-0.2782; lr:0.000500
epoch:3; metric:emoval; train:-0.2954; eval:-0.1590; lr:0.000500
epoch:4; metric:emoval; train:-0.1635; eval:0.0206; lr:0.000500
epoch:5; metric:emoval; train:0.0050; eval:0.1357; lr:0.000500
epoch:6; metric:emoval; train:0.1011; eval:0.2040; lr:0.000500
epoch:7; metric:emoval; train:0.1879; eval:0.2688; lr:0.000500
epoch:8; metric:emoval; train:0.2570; eval:0.2965; lr:0.000500
epoch:9; metric:emoval; train:0.3150; eval:0.3682; lr:0.000500
epoch:10; metric:emoval; train:0.3831; eval:0.4106; lr:0.000500
epoch:11; metric:emoval; train:0.4100; eval:0.4333; lr:0.000500
epoch:12; metric:emoval; train:0.4509; eval:0.4326; lr:0.000500
epoch:13; metric:emoval; train:0.4710; eval:0.4610; lr:0.000500
epoch:14; metric:emoval; train:0.5197; eval:0.4500; lr:0.000500
epoch:15; metric:emoval; train:0.5334; eval:0.4775; lr:0.000500
epoch:16; metric:emoval; train:0.5507; eval:0.5076; lr:0.000500
epoch:17; metric:emoval; train:0.5737; eval:0.5155; lr:0.000500
epoch:18; metric:emoval; train:0.5880; eval:0.5293; lr:0.000500
epoch:19; metric:emoval; train:0.6054; eval:0.5392; lr:0.000500
epoch:20; metric:emoval; train:0.6289; eval:0.5272; lr:0.000500
epoch:21; metric:emoval; train:0.6328; eval:0.5427; lr:0.000500
epoch:22; metric:emoval; train:0.6514; eval:0.5587; lr:0.000500
epoch:23; metric:emoval; train:0.6692; eval:0.5655; lr:0.000500
epoch:24; metric:emoval; train:0.6696; eval:0.5353; lr:0.000500
epoch:25; metric:emoval; train:0.6932; eval:0.5164; lr:0.000500
epoch:26; metric:emoval; train:0.6988; eval:0.5487; lr:0.000500
epoch:27; metric:emoval; train:0.7141; eval:0.5632; lr:0.000500
epoch:28; metric:emoval; train:0.7298; eval:0.5434; lr:0.000500
epoch:29; metric:emoval; train:0.7368; eval:0.5536; lr:0.000500
epoch:30; metric:emoval; train:0.7446; eval:0.5476; lr:0.000500
epoch:31; metric:emoval; train:0.7711; eval:0.5510; lr:0.000500
epoch:32; metric:emoval; train:0.7616; eval:0.5397; lr:0.000500
epoch:33; metric:emoval; train:0.7683; eval:0.5640; lr:0.000500
epoch:34; metric:emoval; train:0.7772; eval:0.5323; lr:0.000500
epoch:35; metric:emoval; train:0.7732; eval:0.5661; lr:0.000500
epoch:36; metric:emoval; train:0.7853; eval:0.5667; lr:0.000500
epoch:37; metric:emoval; train:0.7807; eval:0.5608; lr:0.000500
epoch:38; metric:emoval; train:0.7869; eval:0.5573; lr:0.000500
epoch:39; metric:emoval; train:0.7793; eval:0.5193; lr:0.000500
epoch:40; metric:emoval; train:0.7870; eval:0.4789; lr:0.000500
epoch:41; metric:emoval; train:0.7753; eval:0.5331; lr:0.000500
epoch:42; metric:emoval; train:0.7822; eval:0.5408; lr:0.000500
epoch:43; metric:emoval; train:0.7738; eval:0.5472; lr:0.000500
epoch:44; metric:emoval; train:0.7800; eval:0.5460; lr:0.000500
epoch:45; metric:emoval; train:0.7748; eval:0.5636; lr:0.000500
epoch:46; metric:emoval; train:0.7900; eval:0.5385; lr:0.000500
epoch:47; metric:emoval; train:0.7731; eval:0.5525; lr:0.000500
epoch:48; metric:emoval; train:0.7882; eval:0.5702; lr:0.000500
epoch:49; metric:emoval; train:0.7893; eval:0.5394; lr:0.000500
epoch:50; metric:emoval; train:0.7783; eval:0.5470; lr:0.000500
epoch:51; metric:emoval; train:0.7930; eval:0.5412; lr:0.000500
epoch:52; metric:emoval; train:0.7783; eval:0.5616; lr:0.000500
epoch:53; metric:emoval; train:0.7898; eval:0.5527; lr:0.000500
epoch:54; metric:emoval; train:0.7916; eval:0.5812; lr:0.000500
epoch:55; metric:emoval; train:0.7951; eval:0.5572; lr:0.000500
epoch:56; metric:emoval; train:0.8004; eval:0.5490; lr:0.000500
epoch:57; metric:emoval; train:0.7877; eval:0.5609; lr:0.000500
epoch:58; metric:emoval; train:0.7863; eval:0.5483; lr:0.000500
epoch:59; metric:emoval; train:0.7922; eval:0.5537; lr:0.000500
epoch:60; metric:emoval; train:0.7833; eval:0.5598; lr:0.000500
epoch:61; metric:emoval; train:0.7820; eval:0.5780; lr:0.000500
epoch:62; metric:emoval; train:0.8076; eval:0.5514; lr:0.000500
epoch:63; metric:emoval; train:0.7924; eval:0.5633; lr:0.000500
epoch:64; metric:emoval; train:0.8035; eval:0.5506; lr:0.000500
epoch:65; metric:emoval; train:0.7794; eval:0.5611; lr:0.000500
epoch:66; metric:emoval; train:0.8048; eval:0.5744; lr:0.000500
epoch:67; metric:emoval; train:0.7859; eval:0.5696; lr:0.000500
epoch:68; metric:emoval; train:0.7896; eval:0.5491; lr:0.000500
epoch:69; metric:emoval; train:0.7967; eval:0.5532; lr:0.000500
epoch:70; metric:emoval; train:0.8067; eval:0.5633; lr:0.000250
epoch:71; metric:emoval; train:0.8108; eval:0.5551; lr:0.000250
epoch:72; metric:emoval; train:0.8096; eval:0.5694; lr:0.000250
epoch:73; metric:emoval; train:0.7930; eval:0.5524; lr:0.000250
epoch:74; metric:emoval; train:0.8065; eval:0.5460; lr:0.000250
epoch:75; metric:emoval; train:0.8211; eval:0.5600; lr:0.000250
epoch:76; metric:emoval; train:0.8229; eval:0.5569; lr:0.000250
epoch:77; metric:emoval; train:0.8272; eval:0.5485; lr:0.000250
epoch:78; metric:emoval; train:0.8322; eval:0.5562; lr:0.000250
epoch:79; metric:emoval; train:0.8337; eval:0.5586; lr:0.000250
epoch:80; metric:emoval; train:0.8264; eval:0.5453; lr:0.000250
epoch:81; metric:emoval; train:0.8360; eval:0.5530; lr:0.000250
epoch:82; metric:emoval; train:0.8344; eval:0.5538; lr:0.000250
epoch:83; metric:emoval; train:0.8269; eval:0.5558; lr:0.000250
epoch:84; metric:emoval; train:0.8284; eval:0.5441; lr:0.000250
Early stopping at epoch 84, best epoch: 54
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4-th folder, best_index: 53, duration: 20.69735550880432 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Step2: training (multiple epoches)
epoch:1; metric:emoval; train:-0.6368; eval:-0.4101; lr:0.000500
epoch:2; metric:emoval; train:-0.3761; eval:-0.1162; lr:0.000500
epoch:3; metric:emoval; train:-0.1187; eval:0.0320; lr:0.000500
epoch:4; metric:emoval; train:0.0456; eval:0.1833; lr:0.000500
epoch:5; metric:emoval; train:0.1435; eval:0.2434; lr:0.000500
epoch:6; metric:emoval; train:0.2599; eval:0.3126; lr:0.000500
epoch:7; metric:emoval; train:0.3208; eval:0.3816; lr:0.000500
epoch:8; metric:emoval; train:0.3518; eval:0.4276; lr:0.000500
epoch:9; metric:emoval; train:0.4241; eval:0.4229; lr:0.000500
epoch:10; metric:emoval; train:0.4592; eval:0.4349; lr:0.000500
epoch:11; metric:emoval; train:0.4861; eval:0.4574; lr:0.000500
epoch:12; metric:emoval; train:0.4964; eval:0.4967; lr:0.000500
epoch:13; metric:emoval; train:0.5323; eval:0.4914; lr:0.000500
epoch:14; metric:emoval; train:0.5661; eval:0.5192; lr:0.000500
epoch:15; metric:emoval; train:0.5842; eval:0.5199; lr:0.000500
epoch:16; metric:emoval; train:0.5861; eval:0.5258; lr:0.000500
epoch:17; metric:emoval; train:0.6076; eval:0.5402; lr:0.000500
epoch:18; metric:emoval; train:0.6467; eval:0.5298; lr:0.000500
epoch:19; metric:emoval; train:0.6566; eval:0.5517; lr:0.000500
epoch:20; metric:emoval; train:0.6530; eval:0.5370; lr:0.000500
epoch:21; metric:emoval; train:0.6809; eval:0.5610; lr:0.000500
epoch:22; metric:emoval; train:0.6965; eval:0.5390; lr:0.000500
epoch:23; metric:emoval; train:0.7072; eval:0.5508; lr:0.000500
epoch:24; metric:emoval; train:0.7225; eval:0.5497; lr:0.000500
epoch:25; metric:emoval; train:0.7064; eval:0.5517; lr:0.000500
epoch:26; metric:emoval; train:0.7274; eval:0.5408; lr:0.000500
epoch:27; metric:emoval; train:0.7470; eval:0.5393; lr:0.000500
epoch:28; metric:emoval; train:0.7628; eval:0.4956; lr:0.000500
epoch:29; metric:emoval; train:0.7599; eval:0.5061; lr:0.000500
epoch:30; metric:emoval; train:0.7658; eval:0.5160; lr:0.000500
epoch:31; metric:emoval; train:0.7761; eval:0.5463; lr:0.000500
epoch:32; metric:emoval; train:0.7714; eval:0.5244; lr:0.000500
epoch:33; metric:emoval; train:0.7808; eval:0.5217; lr:0.000500
epoch:34; metric:emoval; train:0.7783; eval:0.5277; lr:0.000500
epoch:35; metric:emoval; train:0.7982; eval:0.5381; lr:0.000500
epoch:36; metric:emoval; train:0.8023; eval:0.5334; lr:0.000500
epoch:37; metric:emoval; train:0.7938; eval:0.5579; lr:0.000250
epoch:38; metric:emoval; train:0.7982; eval:0.5305; lr:0.000250
epoch:39; metric:emoval; train:0.7993; eval:0.5640; lr:0.000250
epoch:40; metric:emoval; train:0.8042; eval:0.5430; lr:0.000250
epoch:41; metric:emoval; train:0.8146; eval:0.5568; lr:0.000250
epoch:42; metric:emoval; train:0.8056; eval:0.5460; lr:0.000250
epoch:43; metric:emoval; train:0.8050; eval:0.5429; lr:0.000250
epoch:44; metric:emoval; train:0.8023; eval:0.5264; lr:0.000250
epoch:45; metric:emoval; train:0.8123; eval:0.5471; lr:0.000250
epoch:46; metric:emoval; train:0.7995; eval:0.5306; lr:0.000250
epoch:47; metric:emoval; train:0.8038; eval:0.5341; lr:0.000250
epoch:48; metric:emoval; train:0.7998; eval:0.5432; lr:0.000250
epoch:49; metric:emoval; train:0.7969; eval:0.5249; lr:0.000250
epoch:50; metric:emoval; train:0.8017; eval:0.5102; lr:0.000250
epoch:51; metric:emoval; train:0.7940; eval:0.5432; lr:0.000250
epoch:52; metric:emoval; train:0.8170; eval:0.5593; lr:0.000250
epoch:53; metric:emoval; train:0.7928; eval:0.5382; lr:0.000250
epoch:54; metric:emoval; train:0.7810; eval:0.5413; lr:0.000250
epoch:55; metric:emoval; train:0.7993; eval:0.5153; lr:0.000125
epoch:56; metric:emoval; train:0.7998; eval:0.5114; lr:0.000125
epoch:57; metric:emoval; train:0.7922; eval:0.5283; lr:0.000125
epoch:58; metric:emoval; train:0.8025; eval:0.5236; lr:0.000125
epoch:59; metric:emoval; train:0.7842; eval:0.5246; lr:0.000125
epoch:60; metric:emoval; train:0.7851; eval:0.5305; lr:0.000125
epoch:61; metric:emoval; train:0.7771; eval:0.5196; lr:0.000125
epoch:62; metric:emoval; train:0.7860; eval:0.5116; lr:0.000125
epoch:63; metric:emoval; train:0.7897; eval:0.5377; lr:0.000125
epoch:64; metric:emoval; train:0.7792; eval:0.5187; lr:0.000125
epoch:65; metric:emoval; train:0.7904; eval:0.5352; lr:0.000125
epoch:66; metric:emoval; train:0.7997; eval:0.5052; lr:0.000125
epoch:67; metric:emoval; train:0.7987; eval:0.5262; lr:0.000125
epoch:68; metric:emoval; train:0.7978; eval:0.5162; lr:0.000125
epoch:69; metric:emoval; train:0.8037; eval:0.5251; lr:0.000125
Early stopping at epoch 69, best epoch: 39
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5-th folder, best_index: 38, duration: 14.502877950668335 >>>>>
====== Prediction and Saving =======
save results in ./saved-trimodal/result/cv_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust+utt+None_f1:0.7409_acc:0.7439_val:0.6595_1769656695.0140932.npz
save results in ./saved-trimodal/result/test1_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust+utt+None_f1:0.8098_acc:0.8102_val:0.6428_1769656695.0140932.npz
save results in ./saved-trimodal/result/test2_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust+utt+None_f1:0.7404_acc:0.7427_val:0.6529_1769656695.0140932.npz
save results in ./saved-trimodal/result/test3_features:Baichuan-13B-Base-UTT+chinese-hubert-large-UTT+clip-vit-large-patch14-UTT_dataset:MER2023_model:attention_robust+utt+None_f1:0.8758_acc:0.8813_val:78.4551_1769656695.0140932.npz
