import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import sys
import torch
import gradio as gr
import numpy as np
import cv2
import time
import threading
import queue
from collections import deque

# Add parent directory to path to import toolkit
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from feature_extractor import FeatureExtractor
from toolkit.models.attention_robust_v2 import AttentionRobustV2
from demo import DemoArgs, extract_audio_from_video

# --- Global Components ---
MODELS_LOADED = False
extractor = None
model = None
emotion_labels = ['Neutral', 'Angry', 'Happy', 'Sad', 'Worried', 'Surprise']
EMOTION_COLORS = {
    'Neutral': '#6B7280',
    'Angry': '#EF4444',
    'Happy': '#F59E0B',
    'Sad': '#3B82F6',
    'Worried': '#8B5CF6',
    'Surprise': '#10B981'
}
EMOTION_EMOJIS = {
    'Neutral': 'ğŸ˜',
    'Angry': 'ğŸ˜ ',
    'Happy': 'ğŸ˜Š',
    'Sad': 'ğŸ˜¢',
    'Worried': 'ğŸ˜Ÿ',
    'Surprise': 'ğŸ˜²'
}

# --- Custom CSS for Beautiful UI ---
CUSTOM_CSS = """
/* å…¨å±€æ ·å¼ */
.gradio-container {
    max-width: 1200px !important;
    margin: auto !important;
    font-family: 'Segoe UI', system-ui, -apple-system, sans-serif !important;
    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%) !important;
    min-height: 100vh;
    padding: 20px !important;
}

/* æ ‡é¢˜æ ·å¼ */
.main-title {
    text-align: center;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    font-size: 2.2rem !important;
    font-weight: 700 !important;
    margin-bottom: 0.3rem !important;
    letter-spacing: -0.5px;
}

.subtitle {
    text-align: center;
    color: #6B7280;
    font-size: 1rem;
    margin-bottom: 1rem;
}

/* å…³é”®ï¼šè®©å›¾ç‰‡å®Œæ•´æ˜¾ç¤ºä¸è¢«è£å‰ª */
.gr-image img {
    object-fit: contain !important;
    max-height: 100% !important;
    width: auto !important;
    margin: auto !important;
}

/* æƒ…ç»ªç»“æœé¢æ¿ */
.emotion-panel {
    background: linear-gradient(145deg, #ffffff 0%, #f8fafc 100%);
    border-radius: 16px;
    padding: 20px;
    border: 1px solid #e2e8f0;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05);
}

/* æŒ‰é’®æ ·å¼ */
.gr-button-primary {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
    border: none !important;
    font-weight: 600 !important;
    padding: 14px 28px !important;
    font-size: 1rem !important;
    border-radius: 12px !important;
    transition: all 0.3s ease !important;
    box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4) !important;
}

.gr-button-primary:hover {
    transform: translateY(-2px) !important;
    box-shadow: 0 8px 25px rgba(102, 126, 234, 0.5) !important;
}

.gr-button-secondary {
    background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%) !important;
    border: none !important;
    font-weight: 600 !important;
    border-radius: 12px !important;
}

/* Tabæ ·å¼ */
.tabs {
    border-radius: 16px !important;
    overflow: hidden;
}

.tab-nav {
    background: #f1f5f9 !important;
    padding: 8px !important;
    border-radius: 12px !important;
    margin-bottom: 16px !important;
}

.tab-nav button {
    font-weight: 600 !important;
    padding: 12px 24px !important;
    border-radius: 8px !important;
    transition: all 0.2s ease !important;
}

.tab-nav button.selected {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
    color: white !important;
    box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3) !important;
}

/* è¾“å…¥æ¡†æ ·å¼ */
.gr-textbox {
    border-radius: 10px !important;
    border: 2px solid #e2e8f0 !important;
    transition: border-color 0.2s ease !important;
}

.gr-textbox:focus-within {
    border-color: #667eea !important;
}

/* çŠ¶æ€æŒ‡ç¤ºå™¨ */
.status-box {
    background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%);
    border-radius: 10px;
    padding: 12px 16px;
    font-weight: 500;
    color: #3730a3;
}

/* æç¤ºå¡ç‰‡ */
.tip-card {
    background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);
    border-radius: 12px;
    padding: 16px;
    border-left: 4px solid #f59e0b;
}

.warning-card {
    background: linear-gradient(135deg, #fee2e2 0%, #fecaca 100%);
    border-radius: 12px;
    padding: 16px;
    border-left: 4px solid #ef4444;
}

/* å®æ—¶æŒ‡ç¤ºåŠ¨ç”» */
@keyframes pulse {
    0%, 100% { opacity: 1; transform: scale(1); }
    50% { opacity: 0.7; transform: scale(0.95); }
}

.live-badge {
    display: inline-flex;
    align-items: center;
    gap: 6px;
    background: #ef4444;
    color: white;
    padding: 4px 12px;
    border-radius: 20px;
    font-size: 0.85rem;
    font-weight: 600;
}

.live-badge::before {
    content: '';
    width: 8px;
    height: 8px;
    background: white;
    border-radius: 50%;
    animation: pulse 1.5s ease-in-out infinite;
}

/* å“åº”å¼è®¾è®¡ */
@media (max-width: 768px) {
    .main-title { font-size: 1.6rem !important; }
    .gradio-container { padding: 10px !important; }
    .main-content { padding: 16px; }
}

/* éšè—ä¸éœ€è¦çš„å…ƒç´  */
.gr-form { gap: 12px !important; }
footer { display: none !important; }
"""

def load_components():
    """åŠ è½½æ¨¡å‹ç»„ä»¶ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰"""
    global extractor, model, MODELS_LOADED
    if MODELS_LOADED:
        return True, "âœ… æ¨¡å‹å·²å°±ç»ª"

    try:
        print("Loading Extractor...")
        extractor = FeatureExtractor()
        
        print("Loading Model...")
        model_args = DemoArgs()
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = AttentionRobustV2(model_args)
        if device == 'cuda':
            model = model.cuda()
        model.eval()
        
        MODELS_LOADED = True
        return True, "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ"
    except Exception as e:
        return False, f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

def predict_emotion(features):
    """é¢„æµ‹æƒ…ç»ª"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    batch = {
        'audios': torch.tensor(features['audio']).float().to(device),
        'texts': torch.tensor(features['text']).float().to(device),
        'videos': torch.tensor(features['video']).float().to(device)
    }
    
    with torch.no_grad():
        _, emos_out, _, _ = model(batch)
        probs = torch.softmax(emos_out, dim=1).cpu().numpy()[0]
        
    return {label: float(prob) for label, prob in zip(emotion_labels, probs)}

def create_emotion_display(probs_dict):
    """åˆ›å»ºç¾è§‚çš„æƒ…ç»ªæ˜¾ç¤ºHTML"""
    if probs_dict is None:
        return "<div style='text-align:center;color:#9CA3AF;padding:40px;'>ç­‰å¾…åˆ†æ...</div>"
    
    top_emotion = max(probs_dict, key=probs_dict.get)
    top_prob = probs_dict[top_emotion]
    emoji = EMOTION_EMOJIS.get(top_emotion, 'ğŸ­')
    color = EMOTION_COLORS.get(top_emotion, '#6B7280')
    
    html = f"""
    <div style="text-align:center;padding:20px;">
        <div style="font-size:4rem;margin-bottom:10px;">{emoji}</div>
        <div style="font-size:1.8rem;font-weight:700;color:{color};">{top_emotion}</div>
        <div style="font-size:1.2rem;color:#6B7280;margin-bottom:20px;">ç½®ä¿¡åº¦: {top_prob:.1%}</div>
        <div style="text-align:left;max-width:300px;margin:auto;">
    """
    
    sorted_probs = sorted(probs_dict.items(), key=lambda x: x[1], reverse=True)
    for label, prob in sorted_probs:
        bar_color = EMOTION_COLORS.get(label, '#6B7280')
        emoji_small = EMOTION_EMOJIS.get(label, '')
        html += f"""
            <div style="margin:8px 0;">
                <div style="display:flex;justify-content:space-between;margin-bottom:4px;">
                    <span>{emoji_small} {label}</span>
                    <span style="color:#6B7280;">{prob:.1%}</span>
                </div>
                <div style="background:#E5E7EB;border-radius:4px;height:8px;overflow:hidden;">
                    <div style="background:{bar_color};height:100%;width:{prob*100}%;border-radius:4px;transition:width 0.3s;"></div>
                </div>
            </div>
        """
    
    html += "</div></div>"
    return html

def extract_frame_features(frame_rgb, audio_chunk, text_feat):
    """ä»å•å¸§å’ŒéŸ³é¢‘ç‰‡æ®µæå–ç‰¹å¾"""
    # Video feature from single frame
    inputs = extractor.video_processor(images=[frame_rgb], return_tensors="pt").to(extractor.device)
    with torch.no_grad():
        vid_out = extractor.video_model.get_image_features(**inputs)
    video_feat = vid_out.cpu().numpy()
    
    # Audio feature
    if audio_chunk is not None and len(audio_chunk) >= 1600:
        a_inputs = extractor.audio_processor(audio_chunk, sampling_rate=16000, return_tensors="pt").to(extractor.device)
        with torch.no_grad():
            a_out = extractor.audio_model(**a_inputs)
        audio_feat = a_out.last_hidden_state.mean(dim=1).cpu().numpy()
    else:
        audio_feat = np.zeros((1, 768))
    
    return {
        'text': text_feat,
        'audio': audio_feat,
        'video': video_feat
    }

# ==================== è§†é¢‘æ–‡ä»¶åˆ†æ ====================

def process_video_realtime(video_path, text_input, progress=gr.Progress()):
    """å®æ—¶å¤„ç†è§†é¢‘æ–‡ä»¶ - ä¼˜åŒ–åŒæ­¥"""
    if not video_path:
        yield None, "è¯·å…ˆä¸Šä¼ è§†é¢‘", None, create_emotion_display(None)
        return

    if not MODELS_LOADED:
        yield None, "æ­£åœ¨åŠ è½½æ¨¡å‹...", None, create_emotion_display(None)
        success, msg = load_components()
        if not success:
            yield None, msg, None, create_emotion_display(None)
            return

    try:
        # 1. é¢„å¤„ç†éŸ³é¢‘
        yield None, "æ­£åœ¨æå–éŸ³é¢‘...", None, create_emotion_display(None)
        audio_path = video_path + ".wav"
        extract_audio_from_video(video_path, audio_path)

        import librosa
        try:
            full_audio, sr = librosa.load(audio_path, sr=16000)
        except:
            import torchaudio
            wav, sr = torchaudio.load(audio_path)
            if sr != 16000:
                transform = torchaudio.transforms.Resample(sr, 16000)
                wav = transform(wav)
            full_audio = wav.mean(dim=0).numpy()
            sr = 16000

        if os.path.exists(audio_path):
            os.remove(audio_path)

        # 2. å‡†å¤‡æ–‡æœ¬ç‰¹å¾
        text_content = text_input if text_input else ""
        text_feat = extractor.extract_text_feature(text_content)

        # 3. æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        # è®¡ç®—æ˜¾ç¤ºå°ºå¯¸
        max_width = 720
        if width > max_width:
            scale = max_width / width
            display_size = (max_width, int(height * scale))
        else:
            display_size = (width, height)

        # åˆ†æé—´éš” (æ¯0.5ç§’åˆ†æä¸€æ¬¡)
        analysis_interval = 0.5
        # æ˜¾ç¤ºå¸§ç‡ (é™ä½åˆ°10fpsä»¥å‡å°‘å»¶è¿Ÿ)
        display_fps = 10
        frame_skip = max(1, int(fps / display_fps))

        current_probs = None
        last_analysis_time = 0
        frame_idx = 0

        total_duration = total_frames / fps
        start_time = time.time()

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_idx += 1
            current_time = frame_idx / fps

            # è·³å¸§æ˜¾ç¤ºä»¥ä¿æŒæµç•…
            if frame_idx % frame_skip != 0:
                continue

            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            display_frame = cv2.resize(frame_rgb, display_size)

            # åœ¨å¸§ä¸Šå åŠ æƒ…ç»ªä¿¡æ¯
            if current_probs:
                top_emo = max(current_probs, key=current_probs.get)
                conf = current_probs[top_emo]
                # åŠé€æ˜èƒŒæ™¯
                overlay = display_frame.copy()
                cv2.rectangle(overlay, (10, 10), (220, 70), (0, 0, 0), -1)
                display_frame = cv2.addWeighted(overlay, 0.6, display_frame, 0.4, 0)
                cv2.putText(display_frame, f"{top_emo}: {conf:.0%}",
                           (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)

            # è¿›åº¦ä¿¡æ¯
            time_str = time.strftime('%M:%S', time.gmtime(current_time))
            total_time_str = time.strftime('%M:%S', time.gmtime(total_duration))
            status_msg = f"æ’­æ”¾ä¸­: {time_str} / {total_time_str}"

            # æ˜¯å¦éœ€è¦åˆ†æ
            if current_time - last_analysis_time >= analysis_interval:
                start_sample = int(last_analysis_time * sr)
                end_sample = int(current_time * sr)
                end_sample = min(end_sample, len(full_audio))

                audio_chunk = full_audio[start_sample:end_sample] if start_sample < end_sample else None
                if audio_chunk is not None and len(audio_chunk) < 1600:
                    audio_chunk = np.pad(audio_chunk, (0, max(0, 1600 - len(audio_chunk))))

                features = extract_frame_features(frame_rgb, audio_chunk, text_feat)
                current_probs = predict_emotion(features)
                last_analysis_time = current_time

            # ç®€åŒ–çš„åŒæ­¥ï¼šåŸºäºå®é™…ç»è¿‡æ—¶é—´
            elapsed = time.time() - start_time
            target_time = current_time
            if elapsed < target_time:
                time.sleep(min(target_time - elapsed, 0.1))

            yield display_frame, status_msg, current_probs, create_emotion_display(current_probs)

        cap.release()

        final_msg = f"åˆ†æå®Œæˆ | æ€»æ—¶é•¿: {total_time_str}"
        yield display_frame, final_msg, current_probs, create_emotion_display(current_probs)

    except Exception as e:
        import traceback
        traceback.print_exc()
        yield None, f"é”™è¯¯: {str(e)}", None, create_emotion_display(None)

# ==================== æ‘„åƒå¤´åˆ†æ ====================

# å…¨å±€å˜é‡ç”¨äºæ‘„åƒå¤´å®æ—¶åˆ†æ
webcam_text_context = ""
last_webcam_analysis = 0
cached_text_feat = None

def process_webcam_stream(frame, text_input):
    """å®æ—¶å¤„ç†æ‘„åƒå¤´è§†é¢‘æµ"""
    global webcam_text_context, last_webcam_analysis, cached_text_feat

    if frame is None:
        return None, "ç­‰å¾…æ‘„åƒå¤´ç”»é¢...", create_emotion_display(None)

    if not MODELS_LOADED:
        success, msg = load_components()
        if not success:
            return frame, msg, create_emotion_display(None)

    try:
        current_time = time.time()

        # è½¬æ¢é¢œè‰²ç©ºé—´
        if len(frame.shape) == 3 and frame.shape[2] == 3:
            frame_rgb = frame if frame.dtype == np.uint8 else (frame * 255).astype(np.uint8)
        else:
            frame_rgb = frame

        # é™åˆ¶åˆ†æé¢‘ç‡ (æ¯0.5ç§’åˆ†æä¸€æ¬¡ï¼Œé¿å…å¡é¡¿)
        if current_time - last_webcam_analysis < 0.5:
            return frame_rgb, "å®æ—¶åˆ†æä¸­...", None  # è¿”å›Noneè¡¨ç¤ºä¸æ›´æ–°æƒ…ç»ªæ˜¾ç¤º

        last_webcam_analysis = current_time

        # æ–‡æœ¬ç‰¹å¾ï¼ˆç¼“å­˜ï¼Œåªåœ¨æ–‡æœ¬å˜åŒ–æ—¶é‡æ–°è®¡ç®—ï¼‰
        text_content = text_input if text_input else ""
        if text_content != webcam_text_context or cached_text_feat is None:
            webcam_text_context = text_content
            cached_text_feat = extractor.extract_text_feature(text_content)

        # æ‘„åƒå¤´æ²¡æœ‰éŸ³é¢‘ï¼Œä½¿ç”¨é›¶å‘é‡
        audio_feat = np.zeros((1, 768))

        # è§†é¢‘ç‰¹å¾
        inputs = extractor.video_processor(images=[frame_rgb], return_tensors="pt").to(extractor.device)
        with torch.no_grad():
            vid_out = extractor.video_model.get_image_features(**inputs)
        video_feat = vid_out.cpu().numpy()

        features = {
            'text': cached_text_feat,
            'audio': audio_feat,
            'video': video_feat
        }

        probs = predict_emotion(features)
        top_emo = max(probs, key=probs.get)

        # åœ¨ç”»é¢ä¸Šå åŠ æƒ…ç»ªä¿¡æ¯
        display_frame = frame_rgb.copy()
        h, w = display_frame.shape[:2]

        # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
        overlay = display_frame.copy()
        cv2.rectangle(overlay, (10, 10), (220, 70), (0, 0, 0), -1)
        display_frame = cv2.addWeighted(overlay, 0.6, display_frame, 0.4, 0)

        # ç»˜åˆ¶æƒ…ç»ªæ–‡å­—
        cv2.putText(display_frame, f"{top_emo}: {probs[top_emo]:.0%}",
                   (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)

        status = f"å®æ—¶åˆ†æä¸­ | {top_emo} ({probs[top_emo]:.0%})"
        return display_frame, status, create_emotion_display(probs)

    except Exception as e:
        import traceback
        traceback.print_exc()
        return frame, f"åˆ†æé”™è¯¯: {str(e)}", create_emotion_display(None)

# ==================== Gradioç•Œé¢ ====================

def create_interface():
    with gr.Blocks(css=CUSTOM_CSS, title="å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿ", theme=gr.themes.Soft()) as app:

        # æ ‡é¢˜åŒºåŸŸ
        gr.HTML("""
            <div style="text-align:center;padding:15px 0 10px 0;">
                <h1 class="main-title">å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿ</h1>
                <p class="subtitle">åŸºäº AttentionRobustV2 | è§†è§‰ + éŸ³é¢‘ + æ–‡æœ¬ å¤šæ¨¡æ€èåˆ</p>
            </div>
        """)

        # æ¨¡å‹çŠ¶æ€æ 
        with gr.Row():
            model_status = gr.Textbox(
                value="æ¨¡å‹æœªåŠ è½½ - é¦–æ¬¡åˆ†ææ—¶è‡ªåŠ¨åŠ è½½",
                label="",
                interactive=False,
                scale=5,
                elem_classes=["status-box"]
            )
            load_btn = gr.Button("é¢„åŠ è½½æ¨¡å‹", variant="secondary", scale=1, size="sm")

        gr.HTML("<div style='height:10px'></div>")

        # Tabé€‰é¡¹å¡
        with gr.Tabs() as tabs:

            # ========== Tab 1: è§†é¢‘æ–‡ä»¶åˆ†æ ==========
            with gr.TabItem("è§†é¢‘æ–‡ä»¶åˆ†æ", id=0):
                # ä¸Šæ–¹ï¼šä¸Šä¼ å’Œæ§åˆ¶
                with gr.Row():
                    video_input = gr.Video(
                        label="ä¸Šä¼ è§†é¢‘",
                        format="mp4",
                        height=100,
                        scale=2
                    )
                    text_input_video = gr.Textbox(
                        label="æ–‡æœ¬å†…å®¹ï¼ˆå¯é€‰ï¼‰",
                        placeholder="è¾“å…¥è§†é¢‘ä¸­çš„å¯¹è¯å†…å®¹...",
                        lines=2,
                        scale=2
                    )
                    with gr.Column(scale=1):
                        analyze_btn = gr.Button(
                            "å¼€å§‹åˆ†æ",
                            variant="primary",
                            size="lg"
                        )
                        video_status = gr.Textbox(
                            value="ç­‰å¾…ä¸Šä¼ è§†é¢‘",
                            label="",
                            interactive=False,
                            lines=1
                        )

                # ä¸‹æ–¹ï¼šè§†é¢‘ç”»é¢å’Œæƒ…ç»ªç»“æœå¹¶æ’
                with gr.Row():
                    video_display = gr.Image(
                        label="å®æ—¶åˆ†æç”»é¢",
                        type="numpy",
                        scale=3
                    )
                    with gr.Column(scale=2):
                        gr.HTML("<h3 style='margin:0 0 12px 0;color:#374151;'>æƒ…ç»ªåˆ†æç»“æœ</h3>")
                        emotion_html = gr.HTML(create_emotion_display(None))
                        emotion_output = gr.Label(
                            label="æƒ…ç»ªæ¦‚ç‡åˆ†å¸ƒ",
                            num_top_classes=6,
                            visible=False
                        )

            # ========== Tab 2: æ‘„åƒå¤´å®æ—¶åˆ†æ ==========
            with gr.TabItem("æ‘„åƒå¤´å®æ—¶åˆ†æ", id=1):
                # æç¤º
                gr.HTML("""
                    <div class="warning-card" style="margin-bottom:12px;">
                        <strong>æç¤ºï¼š</strong>æ‘„åƒå¤´éœ€è¦ HTTPS è¿æ¥ï¼Œè¯·ä½¿ç”¨ <code>--share</code> å¯åŠ¨ã€‚å¯ç”¨æ‘„åƒå¤´åè‡ªåŠ¨å¼€å§‹å®æ—¶åˆ†æã€‚
                    </div>
                """)

                # ä¸Šæ–¹ï¼šæ–‡æœ¬è¾“å…¥å’ŒçŠ¶æ€
                with gr.Row():
                    text_input_webcam = gr.Textbox(
                        label="æ–‡æœ¬å†…å®¹ï¼ˆå¯é€‰ï¼‰",
                        placeholder="è¾“å…¥å½“å‰æƒ…å¢ƒæè¿°...",
                        lines=1,
                        scale=3
                    )
                    webcam_status = gr.Textbox(
                        value="å¯ç”¨æ‘„åƒå¤´åè‡ªåŠ¨å¼€å§‹åˆ†æ",
                        label="çŠ¶æ€",
                        interactive=False,
                        lines=1,
                        scale=2
                    )

                # ä¸‹æ–¹ï¼šæ‘„åƒå¤´ç”»é¢å’Œæƒ…ç»ªç»“æœ
                with gr.Row():
                    with gr.Column(scale=3):
                        webcam_input = gr.Image(
                            label="æ‘„åƒå¤´è¾“å…¥ï¼ˆç‚¹å‡»å¯ç”¨ï¼‰",
                            source="webcam",
                            streaming=True,
                            type="numpy"
                        )
                        webcam_output = gr.Image(
                            label="åˆ†æç»“æœï¼ˆå¸¦æƒ…ç»ªæ ‡æ³¨ï¼‰",
                            type="numpy"
                        )

                    with gr.Column(scale=2):
                        gr.HTML("<h3 style='margin:0 0 12px 0;color:#374151;'>å®æ—¶æƒ…ç»ª</h3>")
                        webcam_emotion_html = gr.HTML(create_emotion_display(None))

                        gr.HTML("""
                            <div class="tip-card" style="margin-top:20px;">
                                <strong>ä½¿ç”¨è¯´æ˜ï¼š</strong><br>
                                1. ç‚¹å‡»æ‘„åƒå¤´åŒºåŸŸå¯ç”¨<br>
                                2. è‡ªåŠ¨å®æ—¶åˆ†ææƒ…ç»ª<br>
                                3. ç»“æœå åŠ åœ¨ä¸‹æ–¹ç”»é¢
                            </div>
                        """)

        # ä½¿ç”¨è¯´æ˜æŠ˜å åŒº
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.HTML("""
                <div style="display:grid;grid-template-columns:1fr 1fr;gap:20px;padding:10px;">
                    <div>
                        <h4 style="color:#667eea;margin-bottom:10px;">åŠŸèƒ½ä»‹ç»</h4>
                        <ul style="color:#4b5563;line-height:1.8;">
                            <li><strong>è§†é¢‘æ–‡ä»¶åˆ†æ</strong>ï¼šä¸Šä¼ è§†é¢‘ï¼Œå®æ—¶æ˜¾ç¤ºæƒ…ç»ªåˆ†æ</li>
                            <li><strong>æ‘„åƒå¤´å®æ—¶åˆ†æ</strong>ï¼šåƒç›´æ’­ä¸€æ ·å®æ—¶åˆ†ææƒ…ç»ª</li>
                        </ul>
                    </div>
                    <div>
                        <h4 style="color:#667eea;margin-bottom:10px;">æ”¯æŒçš„æƒ…ç»ª</h4>
                        <div style="display:grid;grid-template-columns:1fr 1fr;gap:5px;color:#4b5563;">
                            <span>Neutral ä¸­æ€§</span><span>Angry æ„¤æ€’</span>
                            <span>Happy å¼€å¿ƒ</span><span>Sad æ‚²ä¼¤</span>
                            <span>Worry æ‹…å¿§</span><span>Surprise æƒŠè®¶</span>
                        </div>
                    </div>
                </div>
            """)

        # ========== äº‹ä»¶ç»‘å®š ==========

        def preload_model():
            success, msg = load_components()
            return msg

        load_btn.click(
            fn=preload_model,
            outputs=model_status
        )

        analyze_btn.click(
            fn=process_video_realtime,
            inputs=[video_input, text_input_video],
            outputs=[video_display, video_status, emotion_output, emotion_html]
        )

        # æ‘„åƒå¤´å®æ—¶æµå¤„ç†
        webcam_input.stream(
            fn=process_webcam_stream,
            inputs=[webcam_input, text_input_webcam],
            outputs=[webcam_output, webcam_status, webcam_emotion_html]
        )

    return app


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--share', action='store_true', help='ç”Ÿæˆå…¬ç½‘HTTPSé“¾æ¥ï¼ˆæ‘„åƒå¤´åŠŸèƒ½éœ€è¦ï¼‰')
    parser.add_argument('--port', type=int, default=6006, help='æœåŠ¡ç«¯å£')
    args = parser.parse_args()

    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…ä»CDNåŠ è½½èµ„æºå¯¼è‡´å¡ä½
    import os
    os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"

    print("=" * 60)
    print("å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«ç³»ç»Ÿ")
    print("=" * 60)
    print(f"æœ¬åœ°åœ°å€: http://localhost:{args.port}")
    if args.share:
        print("æ­£åœ¨ç”Ÿæˆå…¬ç½‘HTTPSé“¾æ¥...")
    else:
        print("æç¤º: ä½¿ç”¨ --share å‚æ•°å¯ç”ŸæˆHTTPSå…¬ç½‘é“¾æ¥ï¼ˆæ‘„åƒå¤´åŠŸèƒ½éœ€è¦ï¼‰")
    print("=" * 60)

    app = create_interface()
    app.queue(concurrency_count=1).launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=args.share,
        show_error=True,
        favicon_path=None
    )
